{
  "hash": "ca5e43a42571e87b1d054c4dc8da3f48",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exploring Categorical Data\"\neditor: \n  markdown: \n    wrap: 80\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n## Introduction {#sec-cda}\n\nA variable is known as a *categorical variable* if each observation belongs to\none of a set of categories. Examples of categorical variables are: gender,\nreligion, race and type of residence.\n\nCategorical variables are typically modeled using discrete random variables,\nwhich are strictly defined in terms whether or not the support is countable. The\nalternative to categorical variables are quantitative variables, which are\ntypically modeled using continuous random variables.\n\nAnother method for distinguishing between quantitative and categorical variables\nis to ask if there is a meaningful distance between any two points in the data.\nIf such a distance is meaningful then we have quantitative data. For instance,\nit makes sense to compute the difference in systolic blood pressure between\nsubjects but it does not make sense to consider the mathematical operation\n(\"smoker\" - \"non-smoker\").\n\nIt is important to identify which type of data we have (quantitative or\ncategorical), since it affects the exploration techniques that we can apply.\n\nThere are two sub-types of categorical variables:\n\n-   A categorical variable is *ordinal* if the observations can be ordered, but\n    do not have specific quantitative values.\n-   A categorical variable is *nominal* if the observations can be classified\n    into categories, but the categories have no specific ordering.\n\nIn this topic, we shall discuss techniques for identifying the presence, and for\nmeasuring the strength, of the association between two categorical variables. We\nshall also demonstrate common visualisations used with categorical data.\n\n## Contingency Tables\n\n::: {#exm-chest-pain style=\"background-color: #D5D1D1; padding: 20px\"}\n### Chest Pain and Gender\n\n\\index{Chest pain!description}\n\nSuppose that 1073 NUH patients who were at high risk for cardiovascular disease\n(CVD) were randomly sampled. They were then queried on two things:\n\n1.  Had they experienced the onset of severe chest pain in the preceding 6\n    months? (yes/no)\n2.  What was their gender? (male/female)\n\nThe data would probably have been recorded in the following format (only first\nfew rows shown):\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| Patient|Gender |Pain    |\n|-------:|:------|:-------|\n|       1|female |no pain |\n|       2|male   |no pain |\n|       3|female |no pain |\n|       4|male   |no pain |\n|       5|female |no pain |\n|       6|male   |pain    |\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nHowever, it would probably be summarised and presented in this format, which is\nknown as a *contingency table*.\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|       | pain| no pain|\n|:------|----:|-------:|\n|male   |   46|     474|\n|female |   37|     516|\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nIn a contingency table, each observation from the dataset falls in exactly one\nof the cells. The sum of all entries in the cells equals the number of\nindependent observations in the dataset.\n:::\n\nAll the techniques we shall touch upon in this chapter are applicable to\ncontingency tables.\n\n## Visualisations\n\n### Bar charts\n\nA common method for visualisation cross combinations of categorical data is to\nuse a bar chart.\n\n::: {#exm-pol-assoc-3 style=\"background-color: #D5D1D1; padding: 20px\"}\n### Political Association and Gender Barchart\n\n\\index{Political association!bar chart}\n\nConsider data given in the table below where both variables are nominal. It\ncross-classifies poll respondents according to their gender and their political\nparty affiliation.\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|       | Dem| Ind| Rep|\n|:------|---:|---:|---:|\n|female | 762| 327| 468|\n|male   | 484| 239| 477|\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nHere is R code to make a barchart for this data.\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(lattice)\nbarchart(political_tab/rowSums(political_tab), \n         horizontal = FALSE, auto.key=TRUE)\n```\n\n::: {.cell-output-display}\n![](04-categorical_data_analysis_files/figure-pdf/r-pol-3-1.pdf){fig-align='center' fig-pos='H' width=65%}\n:::\n:::\n\n\n\n\n\n\n\n\nWe can see that the proportion of Republicans is higher for males than for\nfemales. The corresponding proportion of Democrats is lower. However, note that\nthe barchart does not reflect that the marginal count for males was much less\nthan that for females.\n:::\n\nLet us turn to making bar charts with pandas and Python.\n\n::: {#exm-claritin-2 style=\"background-color: #D5D1D1; padding: 20px\"}\n### Claritin and Nervousness Barchart\n\n\\index{Claritin!bar chart}\n\nClaritin is a drug for treating allergies. However, it has a side effect of\ninducing nervousness in patients. From a sample of 450 subjects, 188 of them\nwere randomly assigned to take Claritin, and the remaining were assigned to take\nthe placebo. The observed data was as follows:\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|         | nervous| not nervous|\n|:--------|-------:|-----------:|\n|claritin |       4|         184|\n|placebo  |       2|         260|\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nA barchart can be created in Python with the following code. This barchart is\nslightly different from the one in @exm-pol-assoc-3 because it is not stacked.\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\nclaritin_tab = np.array([[4, 184], [2, 260]])\nclaritin_prop = claritin_tab/claritin_tab.sum(axis=1).reshape((2,1))\n\nxx = pd.DataFrame(claritin_prop, \n                  columns=['nervous', 'not_nervous'], \n                  index=['claritin', 'placebo'])\n\nax = xx.plot(kind='bar', stacked=False, rot=1.0, figsize=(10,4))\nax.legend(loc='upper left');\n```\n\n::: {.cell-output-display}\n![](04-categorical_data_analysis_files/figure-pdf/py-claritin-2-1.pdf){fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\n\n\n\n\n\n\n\nThe proportion of patients who are not nervous is similar in both groups.\n:::\n\n### Mosaic plots\n\nUnlike a bar chart, a mosaic plot reflects the count in each cell (through the\narea), along with the proportions of interest. Let us inspect how we can make\nmosaic plots for the political association data earlier.\n\n:::: {#exm-pol-assoc-2 style=\"background-color: #D5D1D1; padding: 20px\"}\n### Political Association and Gender Mosaic Plot\n\n\\index{Political association!mosaic plot}\n\nThe colours in the output for the R code reflect the sizes of the standardised\nresiduals. We shall discuss this in more detail in @sec-rc-tables.\n\n::: panel-tabset\n#### R code\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmosaicplot(political_tab, shade=TRUE)\n```\n\n::: {.cell-output-display}\n![](04-categorical_data_analysis_files/figure-pdf/r-pol-4-3.pdf){fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\n\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom statsmodels.graphics.mosaicplot import mosaic\nimport matplotlib.pyplot as plt\n\npolitical_tab = np.asarray([[762,327,468], [484,239,477]])\nmosaic(political_tab, statistic=True, gap=0.05);\n```\n\n::: {.cell-output-display}\n![](04-categorical_data_analysis_files/figure-pdf/py-pol-4-1.pdf){fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\n\n\n\n\n\n\n:::\n::::\n\n### Conditional Density Plots\n\nWhen we have one categorical and one quantitative variable, the kind of plot we\nmake really depends on which is the response, and which is the explanatory\nvariable. If the response variable is the quantitative one, it makes sense to\ncreate boxplots or histograms. However, if the response variable is a the\ncategorical one, we should really be making something along these lines:\n\n::: {#exm-heart-1 style=\"background-color: #D5D1D1; padding: 20px\"}\n\\index{Heart failure!conditional density plot}\n\nThe dataset at [UCI\nrepository](https://archive.ics.uci.edu/dataset/519/heart+failure+clinical+records)\ncontains records on 299 patients who had heart failure. The data was collected\nduring the follow-up period; each patient had 13 clinical features recorded. The\nprimary variable of interest was whether they died or not. Suppose we wished to\nplot how this varied with age (a quantitative variable):\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata_path <- file.path(\"data\", \"heart+failure+clinical+records\", \n                       \"heart_failure_clinical_records_dataset.csv\")\nheart_failure <- read.csv(data_path)\nspineplot(as.factor(DEATH_EVENT) ~ age, data=heart_failure)\n```\n\n::: {.cell-output-display}\n![](04-categorical_data_analysis_files/figure-pdf/r-cd-1-3.pdf){fig-align='center' fig-pos='H' width=65%}\n:::\n:::\n\n\n\n\n\n\n\n\nIt reflects how the probability of an event varies with the quantitative\nexplanatory variable. A smoothed version of this is known as the conditional\ndensity plot:\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncdplot(as.factor(DEATH_EVENT) ~ age, data=heart_failure)\n```\n\n::: {.cell-output-display}\n![](04-categorical_data_analysis_files/figure-pdf/r-cd-2-1.pdf){fig-align='center' fig-pos='H' width=65%}\n:::\n:::\n\n\n\n\n\n\n\n\nFrom either plot, it is clear to see that there is an increased proportion of\ndeath during follow-up associated with older patients.\n:::\n\n## Tests for Independence\n\nIn the contingency table above, the two categorical variables are *Gender* and\n*Presence/Absence of Pain*. With contingency tables, the main inferential task\nusually relates to assessing the association between the two categorical\nvariables.\n\n::: {.callout-note title=\"Independent Categorical Variables\"}\nIf two categorical variables are **independent**, then the joint distribution of\nthe variables would be equal to the product of the marginals. If two variables\nare not independent, we say that they are **associated**.\n:::\n\nIn the remainder of this section, we are going to discuss and apply statistical\nhypothesis tests. Refer to @sec-sig-test-proc for a quick recap about hypothesis\ntesting.\n\n### $\\chi^2$-Test for Independence\n\nThe $\\chi^2$-test uses the definition above to assess if two variables in a\ncontingency table are associated. The null and alternative hypotheses are\n\n\\begin{eqnarray*}\nH_0 &:& \\text{The two variables are indepdendent.}  \\\\\nH_1 &:& \\text{The two variables are not indepdendent.}\n\\end{eqnarray*}\n\nUnder the null hypothesis, we can estimate the joint distribution from the\nobserved marginal counts. Based on this estimated joint distribution, we then\ncompute *expected* counts (which may not be integers) for each cell. The test\nstatistic essentially compares the deviation of *observed* cell counts from the\nexpected cell counts.\n\n::: {#exm-chest-pain-2 style=\"background-color: #D5D1D1; padding: 20px\"}\n### Chest Pain and Gender Expected Counts\n\n\\index{Chest pain!expected counts}\n\nContinuing from @exm-chest-pain, we can compute the estimated marginals using\nrow and column proportions\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n------------------------------------------------------------------------------ \nchest_tab (table)\n\nSummary: \nn: 1'073, rows: 2, columns: 2\n\nPearson's Chi-squared test (cont. adj):\n  X-squared = 1.4555, df = 1, p-value = 0.2276\nFisher's exact test p-value = 0.2089\n\n                                     \n                 pain   no pain   Sum\n                                     \nmale     freq      46       474   520\n         p.row   8.8%     91.2%     .\n         p.col  55.4%     47.9% 48.5%\n                                     \nfemale   freq      37       516   553\n         p.row   6.7%     93.3%     .\n         p.col  44.6%     52.1% 51.5%\n                                     \nSum      freq      83       990 1'073\n         p.row   7.7%     92.3%     .\n         p.col      .         .     .\n                                     \n\n----------\n' 95% conf. level\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nIn the output above, ignore the $p$-values for now.. We'll get to those in a\nminute. Let $X$ represent gender and $Y$ represent chest pain. Then from the\nestimated proportions in the \"Sum\" row and colums, we can read off the following\nestimated probabilities\n\n\\begin{eqnarray*}\n\\widehat{P}(X = \\text{male}) &=& 0.485  \\\\\n\\widehat{P}(Y = \\text{pain}) &=& 0.077\n\\end{eqnarray*}\n\nConsequently, *under* $H_0$, we would estimate $$\n\\widehat{P}(X = \\text{male},\\, Y= \\text{pain}) = 0.485 \\times 0.077 \\approx 0.04\n$$\n\nFrom a sample of size 1073, the expected count for this cell is then\n\n$$\n0.04 \\times 1073 = 42.92\n$$\n:::\n\nUsing the approach above, we can derive a general formula for the expected count\nin each cell: $$\n\\text{Expected count} = \\frac{\\text{Row total} \\times \\text{Column total}}{\\text{Total sample size}}\n$$\n\nThe formula for the $\\chi^2$-test statistic (with continuity correction) is: $$\n\\chi^2 = \\sum \\frac{(|\\text{expected} - \\text{observed} | - 0.50 )^2}{\\text{expected count}} \n$$ {#eq-chi-sq}\n\nThe sum is taken over every cell in the table. Hence in a $2\\times2$ table, as\nin @exm-chest-pain, there would be 4 terms in the summation.\n\n::::: {#exm-chest-pain-3 style=\"background-color: #D5D1D1; padding: 20px\"}\n### Chest Pain and Gender $\\chi^2$ Test\n\n\\index{Chest pain!chi-squared test}\n\nLet us see how we can apply and interpret the $\\chi^2$-test for the data in\n@exm-chest-pain.\n\n::: panel-tabset\n#### R code\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- matrix(c(46, 37, 474, 516), nrow=2)\ndimnames(x) <- list(c(\"male\", \"female\"), c(\"pain\", \"no pain\"))\nchest_tab <- as.table(x)\n\nchisq_output <- chisq.test(chest_tab)\nchisq_output\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  chest_tab\nX-squared = 1.4555, df = 1, p-value = 0.2276\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy import stats\n\nchest_array = np.array([[46, 474], [37, 516]])\n\nchisq_output = stats.chi2_contingency(chest_array)\n\nprint(f\"The p-value is {chisq_output.pvalue:.3f}.\")\n## The p-value is 0.228.\nprint(f\"The test-statistic value is {chisq_output.statistic:.3f}.\")\n## The test-statistic value is 1.456.\n```\n:::\n\n\n\n\n\n\n\n:::\n\nSince the $p$-value is 0.2276, we would not reject the null hypothesis at\nsignificance level 5%. We do not have sufficient evidence to conclude that the\nvariables are not independent.\n\nTo extract the expected cell counts, we can use the following code:\n\n::: panel-tabset\n#### R code\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq_output$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           pain  no pain\nmale   40.22367 479.7763\nfemale 42.77633 510.2237\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nchisq_output.expected_freq\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[ 40.22367195, 479.77632805],\n       [ 42.77632805, 510.22367195]])\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n:::\n\nThe test statistic compares the above table to the *observed* table, earlier in\n@exm-chest-pain.\n:::::\n\n::: callout-important\nIt is only suitable to use the $\\chi^2$-test when all *expected cell counts* are\nlarger than 5.\n:::\n\n### Fisher's Exact Test\n\nWhen the condition above is not met, we turn to Fisher's Exact Test. The null\nand alternative hypothesis are the same, but the test statistic is not derived\nin the same way.\n\nIf the marginal totals are fixed, and the two variables are independent, it can\nbe shown that the individual cell counts arise from the hypergeometric\ndistribution. The hypergeometric distribution is defined as follows.\n\nSuppose we have an urn with $m$ black balls and $n$ red balls. From this urn, we\ndraw a random sample (without replacement) of size $k$. If we let $W$ be the\nnumber of red balls drawn, then $W$ follows a hypergeometric distribution. The\npmf is:\n\n$$\nP(W=w) = \\frac{\\binom{n}{w} \\binom{m}{k-w}}{\\binom{n+m}{k}}\n$$\n\n::: callout-note\nWhat is the support of $W$?\n:::\n\nTo transfer this to the context of $2 \\times 2$ tables, suppose we have fix the\nmarginal counts of the table, and consider the count in the top-left corner to\nbe a random variable following a hypergeometric distribution, with $r_{1\\cdot}$\nblack balls and $r_{2 \\cdot}$ red balls. Consider drawing a sample of size\n$c_{1 \\cdot}$ from these $r_{1\\cdot} + r_{2 \\cdot}$ balls.\n\n|                                   |              |                             |\n|-----------------------------------|--------------|-----------------------------|\n| $W$                               | \\-           | $r_{1 \\cdot}$ (black balls) |\n| \\-                                | \\-           | $r_{2\\cdot}$ (red balls)    |\n| $c_{1 \\cdot}$ (sample size drawn) | $c_{2\\cdot}$ |                             |\n\nNote that, assuming the marginal counts are fixed, knowledge of one of the four \nentries in the table is sufficient to compute all the counts in the table.\n\nThe test statistic is the observed count, $w$, in this cell. The $p$-value is calculated\nas\n\n$$\nP(W \\le w)\n$$\nIn practice, instead of summing over all values, the $p$-value is obtained by\nsimulating tables with the same marginals as the observed dataset, and estimating \nthe above probability.\n\nUsing Fisher's test sidesteps the need for a large sample size (which is\nrequired for the $\\chi^2$ approximation to hold); hence the \"Exact\" in the name\nof the test.\n\n:::: {#exm-claritin-1 style=\"background-color: #D5D1D1; padding: 20px\"}\n### Claritin and Nervousness\n\\index{Claritin!Fisher's exact test}\n\nHere is the code to perform the Fisher Exact Test for the Claritin data.\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|         | nervous| not nervous|\n|:--------|-------:|-----------:|\n|claritin |       4|         184|\n|placebo  |       2|         260|\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n::: panel-tabset\n#### R code\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <-  matrix(c(4, 2, 184, 260), nrow=2)\ndimnames(y) <- list(c(\"claritin\", \"placebo\"), c(\"nervous\", \"not nervous\"))\nclaritin_tab <- as.table(y)\n\nfisher.test(claritin_tab)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  claritin_tab\np-value = 0.2412\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.399349 31.473382\nsample estimates:\nodds ratio \n  2.819568 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclaritin_tab = np.array([[4, 184], [2, 260]])\n\nfe_output = stats.fisher_exact(claritin_tab)\n\nprint(f\"The p-value for the test is {fe_output.pvalue:.4f}.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe p-value for the test is 0.2412.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n:::\n\nAs the $p$-value is 0.2412, we again do not have sufficient evidence to reject\nthe null hypothesis and conclude that there is a significant association.\n\nBy the way, we can check (in R) to see that the $\\chi^2$-test would not have\nbeen appropriate:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(claritin_tab)$expected\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in chisq.test(claritin_tab): Chi-squared approximation may be incorrect\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          nervous not nervous\nclaritin 2.506667    185.4933\nplacebo  3.493333    258.5067\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n::::\n\n### $\\chi^2$-Test for $r \\times c$ Tables {#sec-rc-tables}\n\nSo far, we have considered the situation of two categorical variables where each\none has only two outcomes (2x2 table). However, it is common that we want to\ncheck the association between two nominal variables where one of them or both\nhave more than 2 outcomes. Consider data given in the table below where both\nvariables are nominal.\n\n::: {#exm-pol-assoc-1 style=\"background-color: #D5D1D1; padding: 20px\"}\n### Political Association and Gender\n\n\\index{Political association!chi-squared test}\n\nLet us return to the political association data, which we plotted in\n@exm-pol-assoc-3. The R code for applying the test is identical to before\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(political_tab)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  political_tab\nX-squared = 30.07, df = 2, p-value = 2.954e-07\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nIn this case, there is strong evidence to reject $H_0$. At 5% level, we would\nreject the null hypothesis and conclude there is an association between gender\nand political affiliation.\n:::\n\nIn general, we might have $r$ rows and $c$ columns. The null and alternative\nhypotheses are identical to the 2x2 case, and the test statistic is computed in\nthe same way. However, under the null hypothesis, the test statistic follows a\n$\\chi^2$ distribution with $(r-1)(c-1)$ degrees of freedom.\n\nThe $\\chi^2$-test is based on a model of independence - the expected counts are\nderived under this assumption. As such, it is possible to derive residuals and\nstudy them, to see where the data deviates from this model.\n\nWe define the *standardised residuals* to be\n\n$$\nr_{ij} = \\frac{n_{ij} - \\mu_{ij}}{\\sqrt{\\mu_{ij} (1 - p_{i+})(1 -p_{+j} )}}\n$$ where\n\n-   $n_{ij}$ is the observed cell count in row $i$ and column $j$ (cell $ij$).\n-   $\\mu_{ij}$ is the *expected* cell count in row $i$ and column $j$\n-   $p_{i+}$ is the marginal probability of row $i$\n-   $p_{+j}$ is the marginal probability of column $j$.\n\nThe residuals can be obtained from the test output. Under $H_0$, the residuals\nshould be close to a standard Normal distribution. If the residual for a\nparticular cell is very large (or small), we suspect that lack of fit (to the\nindependence model) arises from that cell.\n\nFor the political association table, the standardised residuals (from R) are:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(political_tab)$stdres\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Dem        Ind        Rep\nfemale  4.5020535  0.6994517 -5.3159455\nmale   -4.5020535 -0.6994517  5.3159455\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n## Measures of Association\n\nThis sections covers bivariate measures of association for contingency tables.\n\n### Odds Ratio\n\nThe most generally applicable measure of association, for 2x2 tables, is the\nOdds Ratio (OR). Suppose we have $X$ and $Y$ to be Bernoulli random variables\nwith (population) success probabilities $p_1$ and $p_2$.\n\nWe define the odds of success for $X$ to be $$\n\\frac{p_1}{1-p_1}\n$$ Similarly, the odds of success for random variable $Y$ is\n$\\frac{p_2}{1-p_2}$.\n\nIn order to measure the strength of their association, we use the *odds ratio*:\n$$\n\\frac{p_1/ (1-p_1)}{p_2/(1-p_2)}\n$$\n\nThe odds ratio can take on any value from 0 to $\\infty$.\n\n-   A value of 1 indicates no association between $X$ and $Y$. If $X$ and $Y$\n    were independent, this is what we would observe.\n-   Deviations from 1 indicate stronger association between the variables.\n-   Note that deviations from 1 are not symmetric. For a given pair of\n    variables, an association of 0.25 or 4 is the same - it is just a matter of\n    which variable we put in the numerator odds.\n\nDue to the above asymmetry, we often use the log-odds-ratio instead: $$\n\\log \\frac{p_1/ (1-p_1)}{p_2/(1-p_2)} \n$$\n\n-   Log-odds-ratios can take values from $-\\infty$ to $\\infty$.\n-   A value of 0 indicates no association between $X$ and $Y$.\n-   Deviations from 0 indicate stronger association between the variables, and\n    deviations are now symmetric; a log-odds-ratio of -0.2 indicates the same\n    *strength* as 0.2, just the opposite direction.\n\nTo obtain a confidence interval for the odds-ratio, we work with the log-odds\nratio and then exponentiate the resulting interval. Here are the steps for a\n$2\\times 2$:\n\n1.  The sample data in a 2x2 table can be labelled as\n    $n_{11}, n_{12}, n_{21}, n_{22}$.\n2.  The *sample* odds ratio is $$\n    \\widehat{OR} = \\frac{n_{11} \\times n_{22}}{n_{12} \\times n_{21}}\n    $$\n3.  For a large sample size, it can be shown that $\\log \\widehat{OR}$ follows a\n    Normal distribution. Hence a 95% confidence interval can be obtained through\n    $$\n    \\log \\frac{n_{11} \\times n_{22}}{n_{12} \\times n_{21}} \\pm z_{0.025} \n    \\times ASE(\\log \\widehat{OR})\n    $$\n\nwhere\n\n-   the ASE (Asymptotic Standard Error) of the estimator is $$\n    \\sqrt{\\frac{1}{n_{11}} + \\frac{1}{n_{12}} + \\frac{1}{n_{21}} + \\frac{1}{n_{22}}} \n    $$\n\n:::: {#exm-chest-pain-4 style=\"background-color: #D5D1D1; padding: 20px\"}\n### Chest Pain and Gender Odds Ratio\n\n\\index{Chest pain!odds ratio}\n\nLet us compute the confidence interval for the odds ratio in the chest pain and\ngender example from earlier.\n\n::: panel-tabset\n#### R code\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DescTools)\nOddsRatio(chest_tab,conf.level = .95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nodds ratio     lwr.ci     upr.ci \n 1.3534040  0.8626023  2.1234612 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport statsmodels.api as sm\nchest_tab2 = sm.stats.Table2x2(chest_array)\n\nprint(chest_tab2.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Estimate   SE   LCB    UCB  p-value\n--------------------------------------------------\nOdds ratio        1.353        0.863 2.123   0.188\nLog odds ratio    0.303 0.230 -0.148 0.753   0.188\nRisk ratio        1.322        0.872 2.004   0.188\nLog risk ratio    0.279 0.212 -0.137 0.695   0.188\n--------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n:::\n::::\n\n### For Ordinal Variables\n\nWhen both variables are ordinal, it is often useful to compute the strength (or\nlack) of any monotone trend association. It allows us to assess if\n\n> As the level of $X$ increases, responses on $Y$ tend to increase toward higher\n> levels, or responses on $Y$ tend to decrease towards lower levels.\n\nFor instance, perhaps job satisfaction tends to increase as income does. In this\nsection, we shall discuss a measure for ordinal variables, analogous to\nPearson's correlation for quantitative variables, that describes the degree to\nwhich the relationship is monotone. It is based on the idea of a concordant or\ndiscordant pair of subjects.\n\n::: {#def-con-dis-1 style=\"background-color: #D5D1D1; padding: 20px\"}\n-   A **pair of subjects** is *concordant* if the subject ranked higher on $X$\n    also ranks higher on $Y$.\n-   A **pair** is *discordant* if the subject ranking higher on $X$ ranks lower\n    on $Y$.\n-   A **pair** is *tied* if the subjects have the same classification on $X$\n    and/or $Y$.\n:::\n\nIf we let\n\n-   $C$: number of concordant pairs in a dataset, and\n-   $D$: number of discordant pairs in a dataset.\n\nThen if $C$ is much larger than $D$, we would have reason to believe that there\nis a strong positive association between the two variables. Here are two\nmeasures of association based on $C$ and $D$:\n\n1.  Goodman-Kruskal $\\gamma$ is computed as $$\n    \\gamma = \\frac{C - D}{C + D}\n    $$\n2.  Kendall $\\tau_b$ is $$\n    \\tau_b = \\frac{C - D}{A}\n    $$ where $A$ is a normalising constant that results in a measure that works\n    better with ties, and is less sensitive than $\\gamma$ to the cut-points\n    defining the categories. $\\gamma$ has the advantage that it is more easily\n    interpretable.\n\nFor both measures, values close to 0 indicate a very weak trend, while values\nclose to 1 (or -1) indicate a strong positive (negative) association.\n\n:::: {#exm-job-income-1 style=\"background-color: #D5D1D1; padding: 20px\"}\n### Job Satisfaction by Income\n\n\\index{Job satisfaction!ordinal association}\n\nConsider the following table, obtained from @agresti2012categorical. The\noriginal data come from a nationwide survey conducted in the US in 1996.\n\n::: panel-tabset\n#### R code\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- matrix(c(1, 3, 10, 6,\n              2, 3, 10, 7,\n              1, 6, 14, 12,\n              0, 1,  9, 11), ncol=4, byrow=TRUE)\ndimnames(x) <- list(c(\"<15,000\", \"15,000-25,000\", \"25,000-40,000\", \">40,000\"), \n                    c(\"Very Dissat.\", \"Little Dissat.\", \"Mod. Sat.\", \n                      \"Very Sat.\"))\nus_svy_tab <- as.table(x)\n\noutput <- Desc(x, plotit = FALSE, verbose = 3)\noutput[[1]]$assocs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                       estimate  lwr.ci  upr.ci\nContingency Coeff.       0.2419       -       -\nCramer V                 0.1439  0.0000  0.1693\nKendall Tau-b            0.1524 -0.0083  0.3130\nGoodman Kruskal Gamma    0.2211 -0.0085  0.4507\nStuart Tau-c             0.1395 -0.0082  0.2871\nSomers D C|R             0.1417 -0.0080  0.2915\nSomers D R|C             0.1638 -0.0116  0.3392\nPearson Correlation      0.1772 -0.0241  0.3647\nSpearman Correlation     0.1769 -0.0245  0.3645\nLambda C|R               0.0377  0.0000  0.2000\nLambda R|C               0.0159  0.0000  0.0693\nLambda sym               0.0259  0.0000  0.1056\nUncertainty Coeff. C|R   0.0311 -0.0076  0.0699\nUncertainty Coeff. R|C   0.0258 -0.0069  0.0585\nUncertainty Coeff. sym   0.0282 -0.0072  0.0637\nMutual Information       0.0508       -       -\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy import stats\n\nus_svy_tab = np.array([[1, 3, 10, 6], \n                      [2, 3, 10, 7],\n                      [1, 6, 14, 12],\n                      [0, 1,  9, 11]])\n\ndim1 = us_svy_tab.shape\nx = []; y=[]\nfor i in range(0, dim1[0]):\n    for j in range(0, dim1[1]):\n        for k in range(0, us_svy_tab[i,j]):\n            x.append(i)\n            y.append(j)\n\nkt_output = stats.kendalltau(x, y)\nprint(f\"The estimate of tau-b is {kt_output.statistic:.4f}.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe estimate of tau-b is 0.1524.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n:::\n\nThe output shows that both $\\gamma = 0.22$ and $\\tau_b =0.15$ are close to\nsignificant. The lower confidence limit is close to being positive.\n::::\n\n## Further readings\n\nIn general, I have found that R packages seem to have a lot more measures of\nassociation for categorical variables. In Python, the measures are spread out\nacross packages.\n\nAbove, we have only scratched the surface of what is available. If you are keen,\ndo read up on\n\n1.  Somer's D (for association between nominal and ordinal)\n2.  Mutual Information (for association between all types of pairs of\n    categorical variables)\n3.  Polychoric correlation (for association between two ordinal variables)\n\nAlso, take note of how log odds ratios, $\\tau_b$ and $\\gamma$ work - they range\nbetween -1 to 1 (in general), and values close to 0 reflect weak association.\nValues of $a$ and $-a$ indicate the same *strength*, but different direction of\nassociation. This allows the same intuition that Pearson's correlation does.\nWhen you are presented with new metrics, try to understand them by asking\nsimilar questions about them.\n\n## References\n\n### Website References\n\n1.  Documentation pages from statsmodels:\n    -   [Mosaic\n        plots](https://www.statsmodels.org/stable/generated/statsmodels.graphics.mosaicplot.mosaic.html#statsmodels.graphics.mosaicplot.mosaic)\n    -   [Contingency\n        tables](https://www.statsmodels.org/stable/contingency_tables.html#module-statsmodels.stats.contingency_tables)\n2.  Documentation pages from scipy:\n    -   [Fisher test with\n        scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.fisher_exact.html)\n    -   [$\\chi^2$-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html)\n3.  [Heart failure\n    data](https://archive.ics.uci.edu/dataset/519/heart+failure+clinical+records)\n4.  [More information on Kendall $\\tau_b$\n    statistic](https://online.stat.psu.edu/stat509/lesson/18/18.3)\n5.  The $\\chi^2$ test we studied is a *test of independence*. It is a variant of\n    the $\\chi^2$ goodness-of-fit test, which is used to assess if data come from\n    a particular distribution. It's just in our case, the presumed distribution\n    is one with independence between the groups. Read more about the\n    [goodness-of-fit test\n    here](https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Introductory_Statistics_(Shafer_and_Zhang)/11%3A_Chi-Square_Tests_and_F-Tests/11.02%3A_Chi-Square_One-Sample_Goodness-of-Fit_Tests).\n",
    "supporting": [
      "04-categorical_data_analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}