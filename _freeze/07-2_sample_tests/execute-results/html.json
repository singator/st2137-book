{
  "hash": "799b421aa53f31c6dbfa7e15100aeef2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Two-sample Hypothesis Tests\"\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n## Introduction\n\nIn this topic, we introduce the routines for a common class of hypothesis tests:\nthe scenario of comparing the location parameter of two groups. This technique\nis commonly used in A/B testing to assess if an intervention has resulted in a\nsignificant difference between two groups.\n\nHypothesis tests are routinely abused in many ways by investigators. Such tests\ntypically require strong assumptions to hold, and can result in false positives\nor negatives. As such, personally, I prefer to use confidence intervals to make\nan assessment of the significance of a result. However, in this topic, we\nintroduce how the $p$-values can be obtained for these hypothesis tests.\n\n## Procedure for Significance Tests {#sec-sig-test-proc}\n\nAs a recap, here is the general approach for conducting a hypothesis test:\n\n### Step 1: Assumptions\n\nIn this step, we verify that the assumptions required for the test are\nvalid. In some tests, this step is carried out at the end of the others, but it\nis always essential to perform. Some tests are very sensitive to the assumptions\n- this is the main reason that the class of robust statistics was invented.\n\n### Step 2: State the hypotheses and significance level\n\nThe purpose of hypothesis testing is to make an inferential statement about the\npopulation from which the data arose. This inferential statement is what we\nrefer to as the hypothesis regarding the population.\n\n::: {.callout-note}\nA hypothesis is a statement about *population*, usually claiming that a parameter\ntakes a particular numerical value or falls in a certain range of values.\n:::\n\nThe hypotheses will be stated as a pair: The first hypothesis is the null\nhypothesis $H_0$ and the second is the alternative hypothesis $H_1$. Both\nstatements will involve the **population parameter** (not the data summary) of\ninterest. For example, if we have a sample of observations from two groups $A$\nand $B$, and we wish to assess if the mean of the populations is different, the\nhypotheses would be\n\n\\begin{eqnarray*}\nH_0: & \\mu_A = \\mu_B \\\\\nH_1: & \\mu_A \\ne \\mu_B \n\\end{eqnarray*}\n\n$H_0$ is usually a statement that indicates \"no difference\", and $H_1$ is\n*usually* the complement of $H_0$. \n\nAt this stage, it is also crucial to state the significance level of the test.\nThe significance level corresponds to the Type I error of the test - the\nprobability of rejecting $H_0$ when in fact it was true. This level is usually\ndenoted as $\\alpha$, and is usually taken to be 5%, but there is no reason to\nadopt this blindly. Think of the choice of 5% as corresponding to accepting an\nerror rate of 1 in 20 - that's how it was originally decided upon by Fisher.\nWhere possible, the significance level should be chosen to be appropriate for\nthe problem at hand.\n\n::: {.callout-warning}\nIt is important to state the significance level at this stage, because if it is\nchosen *after inspecting the data*, the test is no longer valid. This is because,\nafter knowing the $p$-value, one could always choose the significance level such \nthat it yields the *desired* decision (reject or not).\n:::\n\nIt is also possible to test *one-sided* alternatives. In such scenarios, the \nhypotheses would be of the form:\n\n\\begin{eqnarray*}\nH_0: & \\mu_A = \\mu_B \\\\\nH_1: & \\mu_A > \\mu_B \n\\end{eqnarray*}\n\nSuch a test is known as a one-tailed test.\n\n### Step 3: Compute the test statistic\n\nThe test statistic is usually a measure of how far the observed data deviates \nfrom the scenario defined by $H_0$. Usually, the larger it is, the more evidence \nwe have against $H_0$.\n\nThe construction of a hypothesis test involves the derivation of the exact or\napproximate distribution of the test statistic under $H_0$. Deviations from the \nassumption could render this distribution incorrect.\n\n### Step 4: Compute the $p$-value\n\nThe $p$-value quantifies the chance of observing such a test statistic, or one\nthat is more extreme in the direction of $H_1$, under $H_0$. The distribution of\nthe test statistic under $H_0$ is used to compute this value between 0 and 1. A\nvalue closer to 0 indicates stronger evidence against $H_0$.\n\nIn the case of a one-tailed test, try to understand the behaviour of the test \nstatistic, and identify the *signal* that would yield more evidence supporting\nthe alternative hypothesis.\n\n### Step 5: State your conclusion\n\nThis is the binary decision stage. If the $p$-value is less than the stated \nsignificance level, we conclude that we reject $H_0$. Otherwise, we say that \nwe do not reject $H_0$. It is conventional to use this terminology (instead of \nsaying \"accept $H_1$\") since our $p$-value is obtained with respect to $H_0$.\n\n## Confidence Intervals\n\nConfidence intervals are an alternative method of inference for population parameters.\nInstead of yielding a binary reject/do-not-reject result, they return a confidence \ninterval that contains the plausible values for the population parameter. Many \nconfidence intervals are derived by inverting hypothesis tests, and almost all\nconfidence intervals are of the form \n\n$$\n\\text{Sample estimate}\\; \\pm \\; \\text{margin of error} \n$$\n\nFor instance, if we observe $x_1, \\ldots, x_n$ from a Normal distribution, and\nwish to estimate the mean of the distribution, the 95% confidence interval based\non the the $t$ distribution is\n\n$$\n\\bar{x} \\pm t_{0.025, n-1} \\times \\frac{s}{\\sqrt{n}}\n$$\nwhere \n\n* $s$ is the sample standard deviation, and \n* $t_{0.025, n-1}$ is the 0.025-quantile from the $t$ distribution with $n-1$ degrees\nof freedom.\n\nThe formulas for many confidence intervals rely on asymptotic Normality of the \nestimator. However, this is an assumption that can be overcome with the technique \nof bootstrapping. We shall touch on this in the final topic of our course, \nin @sec-bootstrapping.\n\nBootstrapping can also be used to sidestep the distributional assumptions in hypothesis\ntests, but I still much prefer confidence intervals to tests because they yield an \ninterval; they provide much more information than a binary outcome.\n\n## Parametric Tests\n\nParametric tests are hypothesis tests that assume some form of distribution for\nthe sample (or population) to follow. An example of such a test is the $t$-test, which\nassumes that the data originate from a Normal distribution. \n\nConversely, nonparametric tests are hypothesis tests that do not assume any form\nof distribution for the sample. It seems we should always use non-parametric\ntests since distributional assumptions would not be violated, right?\nUnfortunately, since nonparametric tests are so general, they do not have a high\ndiscriminative ability - we say that they have low power. In other words, if a\ndataset truly comes from a Normal distribution, using the $t$-test would be able\nto detect smaller differences between the groups better than a non-parametric\ntest.\n\nIn this section, we cover parametric tests for comparing the difference in mean \nbetween **two** groups. \n\n### Independent Samples Test\n\nIn an independent samples $t$-test, observations in one group yield\n*no information* about the observations in the other group. Independent samples can\narise in a few ways:\n\n* In an experimental study, study units could be assigned randomly to different\ntreatments, thus forming the two groups.\n* In an observational study, we could draw a random sample from the population, and\nthen record an explanatory categorical variable on each unit, such as the gender\nor senior-citizen status.\n* In an observational study, we could draw a random sample from a group (say\nsmokers), and then a random sample from another group (say non-smokers). This would\nresult in a situation where the independent 2-sample $t$-test is appropriate.\n\n#### Formal Set-up\n\nFormally speaking, this is how the independent 2-sample t-test works:\n\nSuppose that $X_1,X_2,\\ldots,X_{n_1}$ are independent observations from group 1,\nand $Y_1, \\ldots Y_{n_2}$ are independent observations from group 2. It is assumed \nthat \n\n\\begin{eqnarray}\nX_i &\\sim& N(\\mu_1,\\, \\sigma^2),\\; i=1,\\ldots,n_1 \\\\\nY_j &\\sim& N(\\mu_2,\\, \\sigma^2),\\; j=1,\\ldots,n_2\n\\end{eqnarray}\n\nThe null and alternative hypotheses would be \n\n\\begin{eqnarray*}\nH_0: & \\mu_1 = \\mu_2 \\\\\nH_1: & \\mu_1 \\ne \\mu_2 \\\\\n\\end{eqnarray*}\n\nThe test statistic for this test is:\n\n$$\nT_1 = \\frac{(\\bar{X} - \\bar{Y}) - 0 }{s_p\\sqrt{1/n_1 + 1/n_2} }\n$$\nwhere \n$$\ns^2_p = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1) s_2^2}{n_1 + n_2 -2 }\n$$\n\nUnder $H_0$, the test statistic $T_1 \\sim t_{n_1 + n_2 -2}$. When we use a software\nto apply the test above, it will typically also return a confidence interval,\ncomputed as\n\n$$\n(\\bar{X} - \\bar{Y}) \\pm t_{n_1 + n_2 -2, 1 - \\alpha/2} \\times s_p\\sqrt{1/n_1 + 1/n_2}\n$$\n\nThe set-up above corresponds to the case where the variance within each group is\nassumed to be the same. We use information from both groups to estimate the \ncommon variance. If we find evidence in the data to the contrary, we used \nthe *unpooled* variance in the denominator of $T_1$:\n\n$$\nT_{1,unpooled} = \\frac{(\\bar{X} - \\bar{Y}) - 0 }{\\sqrt{s^2_1/n_1 + s^2_2/n_2} }\n$$\n\nwhere $s^2_1$ and $s^2_2$ are the sample variance from groups 1 and 2. The test\nstatistic still follows a $t$ distribution, but the degrees of freedom are \napproximated. This approximation is known as the Satterthwaite approximation.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-abalone-1}\n\n### Abalone Measurements\n\\index{Abalone!2-sample t-test}\n\nThe dataset on abalone measurements from the [UCI machine learning\nrepository](https://archive.ics.uci.edu/dataset/1/abalone) contains measurements\nof physical characteristics, along with the gender status. We derive a sample of\n50 measurements of male and female abalone records for use here. Our goal is \nto study if there is a significant difference between the viscera weight[^07-viscera] between\nmales and females. The derived dataset can be found on Canvas.\n\n[^07-viscera]: Viscera is the gut weight after bleeding out the abalone (in grams).\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nabl <- read.csv(\"data/abalone_sub.csv\")\nx <- abl$viscera[abl$gender == \"M\"]\ny <- abl$viscera[abl$gender == \"F\"]\n\nt.test(x, y, var.equal=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  x and y\nt = 0.91008, df = 98, p-value = 0.365\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.02336287  0.06294287\nsample estimates:\nmean of x mean of y \n  0.30220   0.28241 \n```\n\n\n:::\n:::\n\n\n\n\n#### Python code\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\n\nabl = pd.read_csv(\"data/abalone_sub.csv\")\n#abl.head()\n#abalone_df.describe()\n\nx = abl.viscera[abl.gender == \"M\"]\ny = abl.viscera[abl.gender == \"F\"]\n\nt_out = stats.ttest_ind(x, y)\nci_95 = t_out.confidence_interval()\n\nprint(f\"\"\"\n* The p-value for the test is {t_out.pvalue:.3f}. \n* The actual value of the test statistic is {t_out.statistic:.3f}.\n* The upper and lower limits of the CI are ({ci_95[0]:.3f}, {ci_95[1]:.3f}).\n\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n* The p-value for the test is 0.365. \n* The actual value of the test statistic is 0.910.\n* The upper and lower limits of the CI are (-0.023, 0.063).\n```\n\n\n:::\n:::\n\n\n\n\n#### SAS Output \n\nWhen we run the independent samples t-test on SAS, we should observe the output\nin @fig-sas-abalone-001.\n\n::: {layout-ncol=1}\n![SAS: Independent samples t-test, Abalone data](figs/sas_abalone_ind_ttest-001a.png){#fig-sas-abalone-001 fig-align=\"center\" width=65%}\n:::\n\n:::\n\nThe assumptions of the test include Normality and equal variance. Let us assess\nthe equality of variance first. While there are many hypothesis tests\nspecifically for assessing if variances are equal (e.g. Levene, Bartlett), in\nour class, I advocate a simple rule of thumb. If the larger s.d is more than\ntwice the smaller one, than we should not use the equal variance form of the\ntest. This rule of thumb is widely used in practice (see @sec-web-ref-07).\n\n::: {.panel-tabset}\n\n\n#### R code \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naggregate(viscera ~ gender, data=abl, sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  gender   viscera\n1      F 0.1087070\n2      M 0.1087461\n```\n\n\n:::\n:::\n\n\n\n\n#### Python code\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nabl.groupby('gender').describe()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       viscera                                                            \n         count     mean       std    min       25%    50%       75%    max\ngender                                                                    \nF         50.0  0.28241  0.108707  0.095  0.201250  0.275  0.365125  0.575\nM         50.0  0.30220  0.108746  0.040  0.253125  0.310  0.348750  0.638\n```\n\n\n:::\n:::\n\n\n\n:::\n\nTo assess the Normality assumption, we make also histograms and qq-plots\n(see @fig-r-abalone-normality-1, @fig-r-abalone-normality-2, and  \n@fig-r-abalone-normality-3).\n\n::: {.callout-note}\nOnly the R code is shown here.\n:::\n\n\n\n\n::: {.cell layout=\"[[1],[1,1]]\" layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(lattice)\nhistogram(~viscera | gender, data=abl, type=\"count\")\nqqnorm(y, main=\"Female Abalones\");  qqline(y)\nqqnorm(x, main=\"Male Abalones\");  qqline(x)\n```\n\n::: {.cell-output-display}\n![R: Histograms for males and females](07-2_sample_tests_files/figure-html/fig-r-abalone-normality-1.png){#fig-r-abalone-normality-1 fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![R: QQ-plot for females](07-2_sample_tests_files/figure-html/fig-r-abalone-normality-2.png){#fig-r-abalone-normality-2 fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![R: QQ-plot for males](07-2_sample_tests_files/figure-html/fig-r-abalone-normality-3.png){#fig-r-abalone-normality-3 fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\nWe would conclude that there is no significant difference between the mean \nviscera weight of males and females.\n:::\n\n<br>\n\nApart from qq-plots, it is worthwhile to touch on additional methods that are \nused to assess how much a dataset deviates from Normality. \n\n### More on Assessing Normality\n\n#### Skewness \n\nThe Normal distribution is symmetric about it's mean. Hence if we observe\nasymmetry in our histogram, we might suspect deviation from Normality. To\nquantify this asymmetry, we use *skewness*. One method of estimating the\nskewness of a distribution from data is:\n\n$$\ng_1 = \\frac{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^3}{[\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2 ]^{3/2}}\n$$\nThis is the method-of-moments estimator for the distribution skewness parameter. A value\nclose to 0 indicates low skewness (i.e. high symmetry). Positive values correspond to \nright-skew and negative values to left-skew.\n\n#### Kurtosis \n\nKurtosis measures the thickness of the tails of a distribution. Positive\nkurtosis implies that the tails are \"fatter\" than those of a Normal. Negative\nvalues indicate that the tails are \"thinner\" than those of a Normal.\n\nThe method of moments estimator is \n\n$$\ng_2 = \\frac{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^4}{[\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2 ]^2} - 3\n$$\n\n#### Hypothesis tests for Normality\n\nThe Shapiro-Wilk test and the Kolmogorov-Smirnov test are formal hypothesis\ntests with the following hypotheses:\n\n\\begin{eqnarray*}\nH_0: & \\text{ Data follows a Normal distribution} \\\\\nH_1: & \\text{ Data does not follow a Normal distribution} \n\\end{eqnarray*}\n\nYou can read more about them in the references, but take note that applying\nmultiple tests leads to a higher Type I error. Moreover, a large small sample\nsize will almost always reject $H_0$ because small deviations are being classed\nas significant. I advocate a more graphical approach in assessing Normality,\nespecially since the solution to Non-normality (the bootstrap) is readily\naccessible today.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-abalone-norm}\n\n### Abalone Measurements\n\\index{Abalone!Normality checks}\n\nLet us apply the above computations to the abalone measurements.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DescTools)\naggregate(viscera ~ gender, data=abl, Skew, method=1)\n##   gender   viscera\n## 1      F 0.4060918\n## 2      M 0.2482997\n\naggregate(viscera ~ gender, data=abl, Kurt, method=1)\n##   gender    viscera\n## 1      F -0.2431501\n## 2      M  1.1660593\n\n# Shapiro-Wilk Test only for males:\nshapiro.test(x)\n## \n## \tShapiro-Wilk normality test\n## \n## data:  x\n## W = 0.96779, p-value = 0.1878\n```\n:::\n\n\n\n\n#### Python code\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nabl.groupby(\"gender\").skew()\n##          viscera\n## gender          \n## F       0.418761\n## M       0.256046\n\nfor i,df in abl.groupby('gender'):\n    print(f\"{df.gender.iloc[0]}: {df.viscera.kurt():.4f}\")\n## F: -0.1390\n## M: 1.4220\n    \nstats.shapiro(x)\n## ShapiroResult(statistic=np.float64(0.9677872659314101), pvalue=np.float64(0.1878490793650714))\n```\n:::\n\n\n\n\n#### SAS output \n\n![SAS: Normality tests](figs/sas_abalone_normality_tests_a.png){#fig-sas-norm-001 width=65%}\n\n:::\n\nAlthough the skewness seems large for the female group, the Normality tests \ndo not reject the null hypothesis (see @fig-sas-norm-001).\n\n:::\n\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n\n### Paired Sample Test\n\nThe data in a paired sample test also arises from two groups, but the two groups \nare not independent. A very common scenario that gives rise to this test is when \nthe same subject receives both treatments. His/her measurement under each treatment \ngives rise to a measurement in each group. However, the measurements are no longer \nindependent.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-paired-1}\n\n### Reaction time of drivers\n\nConsider a study on 32 drivers sampled from a driving school. Each driver is \nput in a simulation of a driving situation, where a target flashes red and  green\nat random periods. Whenever the driver sees red, he/she has to press a brake button.\n\nFor each driver, the study is carried out twice - at one of the repetitions, the\nindividual carries on a phone conversation while at the other, the driver\nlistens to the radio.\n\nEach measurement falls under one of two groups - \"phone\" or \"radio\", but the measurements\nfor driver $i$ are clearly related. Some people might just have a slower/faster baseline \nreaction time!\n\nThis is a situation where a paired sample test is appropriate, not an independent \nsample test.\n:::\n\n#### Formal Set-up\n\nSuppose that we observe $X_1, \\ldots , X_n$ independent observations from group\n1 and $Y_1, \\ldots, Y_n$ independent observations from group 2. However the pair \n$(X_i, Y_i)$ are correlated. It is assumed that\n\n\\begin{eqnarray}\nX_i &\\sim& N(\\mu_1,\\, \\sigma_1^2),\\; i=1,\\ldots,n \\\\\nY_j &\\sim& N(\\mu_2,\\, \\sigma_2^2),\\; j=1,\\ldots,n\n\\end{eqnarray}\n\nWe let $D_i = X_i - Y_i$ for $i=1, \\ldots, n$. It follows that \n$$\nD_i \\sim N(\\mu_1 - \\mu_2,\\; \\sigma^2_1 + \\sigma^2_2 - 2 cov(X_i, Y_i))\n$$\nThe null and alternative hypotheses are stated in terms of the distribution of\n$D_i$:\n\n\\begin{eqnarray*}\nH_0: & \\mu_D = 0 \\\\\nH_1: & \\mu_D \\ne 0\n\\end{eqnarray*}\n\nThe test statistic for this test is:\n\n$$\nT_2 = \\frac{\\bar{D} - 0 }{s / \\sqrt{n} }\n$$\nwhere \n$$\ns^2 = \\frac{\\sum_{i=1}^n (D_i - \\bar{D})^2}{(n - 1)}\n$$\n\nUnder $H-0$, the test statistic $T_2 \\sim t_{n - 1}$. When we use a software\nto apply the test above, it will typically also return a confidence interval,\ncomputed as\n\n$$\n\\bar{D} \\pm t_{n - 1, 1 - \\alpha/2} \\times s / \\sqrt{n}\n$$\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-hr-1}\n\n### Heart Rate Before/After Treadmill\n\\index{Treadmill!paired t-test}\n\nThe following dataset comes from the textbook [@rosner2015fundamentals], where \nan individual recorded his heart rate before using a treadmill (baseline) and \n5 minutes after use, for 12 days in 2006. \n\n::: {.panel-tabset}\n\n#### R code \n\nTo run the paired test, we use the same function `t.test` as earlier, but we set the \nargument `paired`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhr_df <- read.csv(\"data/health_promo_hr.csv\")\nbefore <- hr_df$baseline\nafter <- hr_df$after5\nt.test(before, after, paired=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  before and after\nt = -21.714, df = 11, p-value = 2.209e-10\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -16.77286 -13.68548\nsample estimates:\nmean difference \n      -15.22917 \n```\n\n\n:::\n:::\n\n\n\n\n#### Python code\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nhr_df = pd.read_csv(\"data/health_promo_hr.csv\")\n#hr_df.head()\n\npaired_out = stats.ttest_rel(hr_df.baseline, hr_df.after5)\nprint(f\"\"\"\nTest statistic: {paired_out.statistic:.3f}.\np-val: {paired_out.pvalue:.3f}.\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTest statistic: -21.714.\np-val: 0.000.\n```\n\n\n:::\n:::\n\n\n\n\n#### SAS Output\n\n![SAS: HR paired t-test](figs/sas_hr_paired_ttest_a.png){width=65%}\n\n\n::: \n\nIt is imperative to also make the checks for Normality. If you were to make\nthem, you would realise that the sample size is rather small - it is difficult\nto make the case for Normality here.\n\nA couple of interesting plots from SAS are shown in @fig-sas-paired-prof and \n@fig-sas-agreement.\n\n::: {layout-ncol=2}\n\n![SAS: Paired profiles](figs/sas_hr_paired_profiles.png){#fig-sas-paired-prof}\n\n![SAS: Agreement plot](figs/sas_hr_agreement.png){#fig-sas-agreement}\n\n:::\n\nWhen we inspect the paired plot, we are looking for a similar gradient for each line,\nor at least similar in sign. If instead, we observed a combination of positive \nand negative gradients, then we would be less confident that there is a difference \nin means between the groups.\n\nFor the agreement plot, if we were to observe the points scattered around the\nline $y=x$, then we would be more inclined to believe that the mean difference\nis indeed 0.\n\n:::\n\n## Non-parametric Tests\n\nIf the distributional assumptions of the $t$-test are not met, we can take action \nin several ways. Historically, one method was to transform the data (if it was \nskewed) to make the histogram symmetric and thus closer to a Normal. But this was \nnot ideal - it did not help in cases where the data was symmetric but had fatter \ntails than the Normal. As time progressed, statisticians invented tools to overcome\nthe distributional assumptions. One sub-field was robust statistics, which keep \nthe assumptions to a minimum, e.g. only requiring the underlying distribution to be \nsymmetric. Another sub-field was the area of non-parametric statistics, where \nalmost **no** distributional assumptions are made about the data.\n\nIn this section, we cover the non-parametric analogues for independent and paired\n2-sample tests.\n\n### Independent Samples Test {#sec-indep-nonpar}\n\nThe non-parametric analogue of the independent 2-sample test is the Wilcoxon Rank\nSum (WRS) test (equivalent to the Mann-Whitney test). \n\n#### Formal Set-up\n\nSuppose that we observe $X_1, \\ldots , X_{n_1}$ independent observations from group\n1 (with distribution $F$) and $Y_1, \\ldots, Y_{n_2}$ independent observations from\ngroup 2 (with distribution $G$). \n\nThe hypotheses associated with this test are:\n\n\\begin{eqnarray*}\nH_0 &: & F = G \\\\\nH_1 &: & F(x) = G(x - \\Delta), \\Delta \\neq 0\n\\end{eqnarray*}\n\nIn other words, the alternative hypothesis is that the distribution of group 1\nis a location shift of the distribution of group 2.\n\nThe WRS test begins by pooling the $n_1 + n_2$ data points and ranking them. The \nsmallest observation is awarded rank 1, and the largest observation is awarded \nrank $n_1 + n_2$, assuming there are no tied values. If there are tied values, \nthe observations with the same value receive an averaged rank.\n\nCompute $R_1$, the sum of the ranks in group 1. If this sum is large, it means \nthat most of the values in group 1 were larger than those in group 2. Note that \nthe average rank in the combined sample is \n$$\n\\frac{n_1 + n_2 + 1}{2}\n$$\n\nUnder $H_0$, the expected rank sum of group 1 is \n$$\nE(R_1) = n_1 \\times  \\frac{n_1 + n_2 + 1}{2}\n$$\nThe test statistic is a comparison of $R_1$ with the above expected value:\n\n$$\nW_1 = \\begin{cases}\n\\frac{\\left|R_1 - \\frac{n_1(n_1+n_2+1)}{2} \\right| - \\frac{1}{2}}{\\sqrt{n_1n_2(n_1 + n_2 +1)/12}}, & R_1 \\ne \\frac{n_1 (n_1 + n_2 + 1)}{2} \\text{ and no ties} \\\\\n\\frac{\\left|R_1 - \\frac{n_1(n_1+n_2+1)}{2} \\right| - \\frac{1}{2}}{\\sqrt{n_1n_2 \\left( n_1 + n_2 +1 - \\frac{\\sum_{i=1}^g t_i (t_i^2-1)}{(n_1+n_2)(n_1+n_2-1)} \\right) /12}}, & R_1 \\ne \\frac{n_1 (n_1 + n_2 + 1)}{2} \\text{ and ties present} \\\\\n0, & R_1 = \\frac{n_1 (n_1 + n_2 + 1)}{2} \n\\end{cases}\n$$\n\nwhere $g$ refers to the number of groups with ties, and $t_i$ refers to the\nnumber of tied values in each group.\n\nThe test above should only be used if both $n_1$ and $n_2$ are at least 10, and\nif the observations (not the ranks) come from an underlying continuous\ndistribution. If these assumptions hold, then the test statistic $W_1$ follows a\n$N(0,1)$ distribution.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-abalone-2}\n\n### Abalone Measurements\n\\index{Abalone!non-parametric two-sample test}\n\n::: {.panel-tabset}\n\n#### R code \n\nWe can perform the WRS test in R:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  x and y\nW = 1415.5, p-value = 0.2553\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\n\n\n#### Python code\n\nAs mentioned, the Mann-Whitney test will return the same $p$-value as the \nWRS test.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwrs_out = stats.mannwhitneyu(x, y)\n\nprint(f\"\"\"Test statistic: {wrs_out.statistic:.3f}.\np-val: {wrs_out.pvalue:.3f}.\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest statistic: 1415.500.\np-val: 0.255.\n```\n\n\n:::\n:::\n\n\n\n\n#### SAS Output\n\nThe SAS output can be seen in @fig-sas-wrs-001.\n\n![SAS: Wilcoxon rank sum output](figs/sas_abalone_wrs_a.png){#fig-sas-wrs-001 width=65%}\n\n:::\n\nSince we know the number of observations in each group to be more than 10, the\napproximation holds. Comparing with @exm-abalone-1, observe that we have a\nsimilar conclusion.\n\nNotice that the test statistic appears different in SAS. However, it is simply \na matter of a location shift of the test statistic. R and Python subtract the \nsmallest possible sum of ranks from group 1, but SAS does not. In our example, \nthe group size is 50. Hence we can recover the SAS test statistic from the R\ntest statistic with\n\n$$\n1415.5 + \\frac{50(50+1)}{2} = 2690.5\n$$\n\nThe $p$-value is identical in all three software.\n\n:::\n\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n\n### Paired Samples Test {#sec-pair-nonpar}\n\nThe analogue of the paired sample $t$-test is known as the Wilcoxon Sign Test \n(WST).\n\n#### Formal Set-up\n\nAgain, suppose that we observe $X_1, \\ldots , X_n$ observations from group 1 and\n$Y_1, \\ldots, Y_n$ observations from group 2. Groups 1 and 2 are paired (or\ncorrelated) in some way.\n\nOnce again, we compute $D_i = X_i - Y_i$. The null hypothesis is that\n\n\\begin{eqnarray*}\nH_0 &: & \\text{median of $D_i$ is 0.} \\\\\nH_1 &: & \\text{median of $D_i$ is not 0.}\n\\end{eqnarray*}\n\nWe begin by ranking the $|D_i|$. Ignoring pairs for which $D_i = 0$, we rank the \nremaining observations from 1 for the pair with the smallest absolute value, \nup to $n$ for the pair with the largest absolute value (assuming no ties).\n\nWe then compute $R_2$, the sum of ranks for the positive $D_i$. If this sum is \nlarge, we expect that the pairs with $X_i > Y_i$ have a larger difference (in \nabsolute values) than those with $X_i < Y_i$. Under $H_0$, it can be shown that \n\n$$\nE(R_2) = m(m+1)/4\n$$\nwhere $m$ is the number of of non-zero differences.\n\nThus the test statistic is a comparison of $R_2$ with the above expected value:\n\n$$\nW_2 = \\begin{cases}\n\\frac{\\left|R_2 - \\frac{n(n + 1)}{4} \\right| - \\frac{1}{2}}{\\sqrt{n (n+1)(2n + 1)/24}}, & R_2 \\ne \\frac{n(n + 1)}{4} \\text{ and no ties} \\\\\n\\frac{\\left|R_2 - \\frac{n(n+1)}{4} \\right| - \\frac{1}{2}}{\\sqrt{n (n+1)(2n+1)/(24 - \n\\sum_{i=1}^g (t^3_i - t_i) / 48 )}}, & R_2 \\ne \\frac{n (n + 1)}{4} \\text{ and ties present} \\\\\n0, & R_2 = \\frac{n (n + 1)}{4} \n\\end{cases}\n$$\nwhere $g$ denotes the number of tied groups, and $t_i$ refers to the number of \ndifferences with the same absolute value in the $i$-th tied group.\n\nIf the number of non-zero $D_i$'s is at least 16, then the test statistic $W_2$ \nfollows a $N(0,1)$ distribution approximately. \n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-hr-2}\n\n### Heart Rate Before/After Treadmill\n\\index{Treadmill!non-parametric paired test}\n\n::: {.panel-tabset}\n\nHere is how we can perform the non-parametric test in the various software.\n\n#### R code\n\nWe can perform the Wilcoxon Signed Rank test in R:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(before, after, paired = TRUE, exact = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon signed rank test with continuity correction\n\ndata:  before and after\nV = 0, p-value = 0.002507\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\n\n\n#### Python code\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwsr_out = stats.wilcoxon(hr_df.baseline, hr_df.after5,  \n                         correction=True, method='approx')\nprint(f\"\"\"Test statistic: {wsr_out.statistic:.3f}.\np-val: {wsr_out.pvalue:.3f}.\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest statistic: 0.000.\np-val: 0.003.\n```\n\n\n:::\n:::\n\n\n\n\n#### SAS Output \n\n![SAS: Signed rank test output](figs/sas_hr_wilcox_a.png){width=65%}\n\n:::\n\nIn this problem, we do not have 16 non-zero $D_i$'s. Hence, we should in fact \nbe using the \"exact\" version of the test (R and Python). However, the exact version \nof the test cannot be used when there are ties. \n\nSAS does indeed use the \"exact\" version of the test, so that accounts for the\ndifference in $p$-values. The test statistic in SAS is computed by subtracting\n$E(R_2)$, but in R and Python this subtraction is not done. To get from test\nstatistic in R (0) to the one in SAS (-39):\n\n$$\n0 - \\frac{12(12 + 1)}{4} = -39\n$$\n\nIn cases like these, we can turn to the bootstrap, or we can use a permutation test. We \nshall revisit these in our final topic @sec-simulation.\n\n:::\n\n## Summary\n\nWhile the test statistics are shown in detail, we do not need the full details\nfor our class. I should also point out that the definitions of the test\nstatistic in @sec-indep-nonpar and @sec-pair-nonpar were both taken from\n@rosner2015fundamentals. However, take note that different software have\nslightly different implementations, for instance in how they deal with ties. As\nalways, my advice is to read the documentation as much as possible, and then \nto use the output of the tests as a guide to your decision-making (instead of \ntreating them as the absolute truth).\n\n## References\n\n### Website References {#sec-web-ref-07}\n\n1. [UCI full abalone dataset](https://archive.ics.uci.edu/dataset/1/abalone): The \n   dataset in the examples above, starting from @exm-abalone-1, consists of samples \n   from the full dataset.\n2. Inference recap from Penn State:\n   * [Hypothesis testing recap](https://online.stat.psu.edu/statprogram/reviews/statistical-concepts/hypothesis-testing) \n   * [Confidence intervals recap](https://online.stat.psu.edu/statprogram/reviews/statistical-concepts/confidence-intervals)\n4. [Tests for Normality](https://online.stat.psu.edu/stat501/lesson/7/7.5) More information on the Kolmogorov-Smirnov and Shapiro-Wilks Tests for Normality.\n5. [Overview of $t$-tests](https://www.bmj.com/about-bmj/resources-readers/publications/statistics-square-one/7-t-tests) This page includes the rule of thumb about deciding when to use the \n   equal variance test, and when to use the unequal variances version.\n6. [SAS vs. R/Python](https://stats.stackexchange.com/questions/65875/different-ways-to-calculate-the-test-statistic-for-the-wilcoxon-rank-sum-test) This link provides an explanation why the WRS test statistic for \n  SAS is different from R in @exm-abalone-2.\n\n## Exercises\n\n1.  List the different methods of assessing Normality.\n2.  Compute skewness for all variables in the concrete data set.\n3.  Compute $R_1$, the test statistic for the Wilcoxon Rank Sum Test, using R \n    and Python.\n4.  Compute kurtosis for the `chem` data from chapter 5. Remove the outliers \n    and recompute.\n5.  Perform the 2-sample t-test for `G3` scores, comparing scores for `Medu=1`\n    to `Medu=4`.",
    "supporting": [
      "07-2_sample_tests_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}