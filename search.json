[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Computing and Programming",
    "section": "",
    "text": "Preface\n\n\nOne more robot learns to feel,\n\n\nSomething more than a machine..\n\n\nThe lyrics above are from the song “One More Robot/Sympathy 3000-21”, written by The Flaming Lips in 2002. When I started preparing to teach this course in mid-2023, AI had been around for a year or two already. I was stuck for a long time wondering if this course is still relevant; I played the song quite a few times..\nHere’s what I mean: The course is meant to introduce statistics majors (and minors) to three computing languages: R, Python and SAS. But.. why does anyone need to know this when we can just ask Gemini?\nIf you are taking this course, should you pretend that AI does not exist? Obviously not! If you really need help, don’t hesitate to use AI. But if you do use AI, I would hope that you take some time to dissect it’s solution to aid in more long-term understanding and recall.\nI believe there is a need for data analysts to know how to code from scratch, if only because of the value of the process. By coding from nought, and by digging into data ourselves, we sharpen our minds.\nIn “How to Solve It”, Polya recommends that, upon finding one solution to a problem, we ought to go back and find ways to generalise the solution or to optimise it. Challenging ourselves in these small ways will improve us as data analysts. I hope this course provides you opportunities to do so.\nI believe there is great value in struggling with a problem. That’s what university was meant to be about - to allow you to experiment, make mistakes and find your inclinations without too much damage. I sincerely hope you find joy in working with data, R, Python and SAS during this course!\nIf you are a fellow instructor and you find something useful in this textbook, please do let me know at vik.gopal@nus.edu.sg. The book, along with slides and scripts/notebooks can all be found in the github repository. If you need more details about anything, do feel free to write as well.\nSo long, and thanks for reading!\nVik\nhttps://blog.nus.edu.sg/stavg",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro_to_R.html",
    "href": "01-intro_to_R.html",
    "title": "1  Introduction to R",
    "section": "",
    "text": "1.1 Introduction\nR evolved from the S language, which was first developed by Rick Becker, John Chambers and Allan Wilks. S was created with the following goals in mind:\nLater on, S evolved into S-PLUS, which became very costly. Ross Ihaka and Robert Gentleman from the University of Auckland, decided to write a stripped-down version of S, which was R. Five years later, version 1.0.0 of R was released on 29 Feb 2000. As of Dec 2024, the latest version of R is 4.4.2 It is maintained by the (R Core Team 2024).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#introduction",
    "href": "01-intro_to_R.html#introduction",
    "title": "1  Introduction to R",
    "section": "",
    "text": "S was conceived as a powerful tool for statistical modelling. It enabled you to specify and fit statistical models to your data, assess the goodness of fit and then display parameter estimates, standard errors and predicted values derived from the model. It provided the means to define and manipulate data. The intent was to provide the user with maximum control over the model-fitting process.\nS was to be used for data exploration (tabulating, sorting data, drawing plots to look for trends, etc.)\nIt was to be used as a sophisticated calculator to evaluate complex arithmetic expressions, and as a very flexible and general object-oriented programming language to perform more extensive data manipulation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#installing-r-and-rstudio",
    "href": "01-intro_to_R.html#installing-r-and-rstudio",
    "title": "1  Introduction to R",
    "section": "1.2 Installing R and Rstudio",
    "text": "1.2 Installing R and Rstudio\nTo download R, go to CRAN, the Comprehensive R Archive Network, download the version for your operating system and install it.\nA new major version is released once a year, and there are 2 - 3 minor releases each year. Upgrading is painful, but it gets worse if you wait to upgrade.\n\n\n\n\n\n\nImportant\n\n\n\nFor our class, please ensure that you have version 4.4.1 or later. Functions in older versions work differently, so you might face problems or differences with some of the codes in the notes.\n\n\n\n\n\n\n\n\nFigure 1.1: Base R\n\n\n\nAfter installing, you can start using R straightaway. However, the basic GUI is not very user-friendly, as you can see from Figure 1.1. Instead of using the basic GUI for R, we are going to use RStudio. RStudio is an Integrated Development Environment (IDE) for R. It provides several features that base R does not, including:\n\nA history of previous plots made.\nThe ability to browse the objects in our workspace more easily.\n\nThe installation file for RStudio can be obtained from this URL. It is updated a couple of times a year. Make sure you have at least version 2024.09.x for our course.\n\n\n\n\n\n\nFigure 1.2: Rstudio interface\n\n\n\nHere’s a quick orientation of the panels in Rstudio, with reference to Figure 1.2.\n\nPanel 1 is the .\n\nThis is where you type R commands.\nThe output from these commands or functions will also be seen here.\nUse the \\(\\uparrow\\) key to scroll through previously entered commands.\n\nPanel 2 contains the History and Environment tabs.\n\nThe History tab displays all commands that have been previously entered in the current session.\nThese commands can be sent directly to the source code panel or the console panel.\nThe Environment tab in this panel has a list of items that have been created in the current session.\n\nPanel 3 contains the Files, Plots and Help tabs.\n\nThe Files tab contains a directory structure that allows one to choose and open files in the source code editor.\nThrough the Plots tab, one can access all plots that have been created in the current session.\nThe Help tab displays the documentation for R functions.\n\nPanel 4 contains the source code editor.\n\nThis where you edit R scripts.\nYou can hit Ctrl-Enter to execute a command (in the console panel) while your cursor is in the source code panel.\nYou can also highlight code and the editor and execute it directly in the console panel.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#basic-data-structures-in-r",
    "href": "01-intro_to_R.html#basic-data-structures-in-r",
    "title": "1  Introduction to R",
    "section": "1.3 Basic Data Structures in R",
    "text": "1.3 Basic Data Structures in R\nProbably the four most frequently used data structures in R are the following:\n\nVector: A set of elements of the same mode (logical; numeric; character; factor).\nMatrix: A set of elements appearing in rows and columns, where the elements are of the same mode.\nDataframe: This is similar to the matrix object in that it is 2-dimensional. However, columns in a dataframe can have different modes. Rows typically contain different observations from your study or measurements from your experiment. The columns contain the values of different variables which may be of different modes.\nList: A list is a generalization of a vector – it represents a collection of data objects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#creating-basic-objects-in-r",
    "href": "01-intro_to_R.html#creating-basic-objects-in-r",
    "title": "1  Introduction to R",
    "section": "1.4 Creating Basic Objects in R",
    "text": "1.4 Creating Basic Objects in R\n\nVectors\nTo create a vector in R, the simplest way is to use the combine function c().\n\n#creating a vector of numbers:\nnumeric_vec &lt;- c(2,4,6,8,10)\nnumeric_vec\n\n[1]  2  4  6  8 10\n\n# creating a vector of strings/characters:\nstring_vec &lt;-c(\"weight\", \"height\", \"gender\")\nstring_vec\n\n[1] \"weight\" \"height\" \"gender\"\n\n# creating a Boolean vector (T/F):\nlogical_vec &lt;- c(TRUE, TRUE, FALSE)\nlogical_vec\n\n[1]  TRUE  TRUE FALSE\n\n# creating factors:\nfactors_vec &lt;- factor(c(\"male\", \"male\", \"female\"))\nfactors_vec\n\n[1] male   male   female\nLevels: female male\n\n\nFactors are slightly different from strings. In R, they are used to represent categorical variables in linear models.\nWhen we need to create a vector that defines groups (of Females followed by Males, for instance), we can turn to a convenient function called rep(). This function replicates elements of vectors and lists. The syntax is as follows: rep(a, b) will replicate the item a, b times. Here are some examples:\n\nr1 &lt;- rep(2,3)\nr1\n\n[1] 2 2 2\n\nr2 &lt;- rep(c(1,2),3)\nr2\n\n[1] 1 2 1 2 1 2\n\nr3 &lt;- rep(c(6,3),c(2,4))\nr3\n\n[1] 6 6 3 3 3 3\n\nr4 &lt;- rep(string_vec, 2)\nr4\n\n[1] \"weight\" \"height\" \"gender\" \"weight\" \"height\" \"gender\"\n\n\nOn other occasions, we may need to create an index vector, along the rows of a dataset. The seq() function is useful for this purpose. It creates a sequence of numbers that are evenly spread out.\n\nseq(from=2, to=10, by=2)\n\n[1]  2  4  6  8 10\n\nseq(from=2, to=10, length = 5)\n\n[1]  2  4  6  8 10\n\nseq(2, 5, 0.8)\n\n[1] 2.0 2.8 3.6 4.4\n\nseq(2, 5, 0.8) * 2\n\n[1] 4.0 5.6 7.2 8.8\n\n\nThe final example above, where the sequence vector of length 4 is multiplied by a scalar 2, is an example of the recycling rule in R – the shorter vector is recycled to match the length of the longer one. This rule applies in all built-in R functions. Try to use this rule to your advantage when using R.\nIf you only need to create a vector of integers that increase by 1, you do not even need seq(). The : colon operator will handle the task.\n\ns1 &lt;- 2:5\ns1\n\n[1] 2 3 4 5\n\n\n\n\nMatrices\nThus far, we have been creating vectors. Matrices are higher dimensional objects. To create a matrix, we use the matrix() function. The syntax is as follows: matrix(v,r,c) will take the values from vector v and create a matrix with r rows and c columns. R is column-major, which means that, by default, the matrix is filled column-by-column, not row-by-row.\n\nv &lt;- c(1:6)\nm1 &lt;- matrix(v, nrow=2, ncol=3)\nm1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nm2 &lt;- matrix(v, nrow=2, ncol=3, byrow=TRUE)\nm2\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\n\nNew rows (or columns) can be added to an existing matrix using the command rbind() (resp. cbind()).\n\na &lt;- c(1,2,3,4)\nb &lt;- c(5,6,7,8)\nab_row &lt;- rbind(a,b)\nab_row\n\n  [,1] [,2] [,3] [,4]\na    1    2    3    4\nb    5    6    7    8\n\nab_col &lt;- cbind(ab_row, c(9,10))\nab_col\n\n  [,1] [,2] [,3] [,4] [,5]\na    1    2    3    4    9\nb    5    6    7    8   10\n\n\n\n\nDataframes\nNow let’s turn to dataframes, which are the most common object we are going to use for storing data in R. A dataframe is a tabular object, like a matrix, but the columns can be of different types; some can be numeric and some can be character, for instance. Think of a dataframe as an object with rows and columns:\n\nThe rows contain different observations or measurements;\nThe columns contain the values of different variables.\n\nAs a general guideline, we should try to store our data in a format where a single variable is not spread across columns.\n\nExample 1.1 (Tidy Data) Consider an experiment where there are three treatments (control, pre-heated and pre-chilled), and two measurements per treatment. The response variable is store in the following dataframe:\n\n\n\n\n\nControl\nPre_heated\nPre_chilled\n\n\n\n\n6.1\n6.3\n7.1\n\n\n5.9\n6.2\n8.2\n\n\n\n\n\nThe above format is probably convenient for recording data. However, the response variable has been spread across three columns. The tidy version of the dataset is:\n\n\n\n\n\nResponse\nTreatment\n\n\n\n\n6.1\ncontrol\n\n\n5.9\ncontrol\n\n\n6.3\npre_heated\n\n\n6.2\npre_heated\n\n\n7.1\npre_chilled\n\n\n8.2\npre_chilled\n\n\n\n\n\nThe second version is more amenable to computing conditional summaries, making plots and for modeling in R.\n\nDataframes can be created from matrices, and they can also be created from individual vectors. The function as.data.frame() converts a matrix into a dataframe, with generic column names assigned.\n\ndf1 &lt;- as.data.frame(m1)\ndf1\n\n  V1 V2 V3\n1  1  3  5\n2  2  4  6\n\n\nIf we intend to pack individual vectors into a dataframe, we use the function data.frame(). We can also specify custom column names when we call this function.\n\na &lt;- c(11,12)\nb &lt;- c(13,14)\ndf2 &lt;- data.frame(col1 = a, col2 = b)\ndf2\n\n  col1 col2\n1   11   13\n2   12   14\n\n\n\n\nLists\nFinally, we turn to lists. You can think of a list in R as a very general basket of objects. The objects do not have to be of the same type or length. The objects can be lists themselves. Lists are created using the list( ) function; elements within a list are accessed using the $ notation, or by using the names of the elements in the list.\n\nls1 &lt;- list(A=seq(1, 5, by=2), B=seq(1, 5, length=4))\nls1\n\n$A\n[1] 1 3 5\n\n$B\n[1] 1.000000 2.333333 3.666667 5.000000\n\nls1[[2]]\n\n[1] 1.000000 2.333333 3.666667 5.000000\n\nls1[[\"B\"]]\n\n[1] 1.000000 2.333333 3.666667 5.000000\n\nls1$A\n\n[1] 1 3 5\n\n\n\nExample 1.2 (Extracting p-values) The iris dataset is a very famous dataset that comes with R. It contains measurements on the flowers of three different species. Let us conduct a 2-sample \\(t\\)-test, and extract the \\(p\\)-value.\n\nsetosa &lt;- iris$Sepal.Length[iris$Species == \"setosa\"]\nvirginica &lt;- iris$Sepal.Length[iris$Species == \"virginica\"]\n\nt_test_out &lt;- t.test(setosa, virginica)\nstr(t_test_out) \n\nList of 10\n $ statistic  : Named num -15.4\n  ..- attr(*, \"names\")= chr \"t\"\n $ parameter  : Named num 76.5\n  ..- attr(*, \"names\")= chr \"df\"\n $ p.value    : num 3.97e-25\n $ conf.int   : num [1:2] -1.79 -1.38\n  ..- attr(*, \"conf.level\")= num 0.95\n $ estimate   : Named num [1:2] 5.01 6.59\n  ..- attr(*, \"names\")= chr [1:2] \"mean of x\" \"mean of y\"\n $ null.value : Named num 0\n  ..- attr(*, \"names\")= chr \"difference in means\"\n $ stderr     : num 0.103\n $ alternative: chr \"two.sided\"\n $ method     : chr \"Welch Two Sample t-test\"\n $ data.name  : chr \"setosa and virginica\"\n - attr(*, \"class\")= chr \"htest\"\n\n\nstr( ) prints the structure of an R object. From the output above, we can tell that the output object is a list with 10 elements. The particular element we need to extract is p.value.\n\nt_test_out$p.value\n\n[1] 3.966867e-25",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#sec-01-read-data",
    "href": "01-intro_to_R.html#sec-01-read-data",
    "title": "1  Introduction to R",
    "section": "1.5 Reading Data into R",
    "text": "1.5 Reading Data into R\nIt is uncommon that we will be creating dataframes by hand, as we have been doing so far. It is more likely that we will be reading in a dataset from a file in order to perform analysis on it. Thus, at this point, let’s sidetrack a little and discuss how we can read data into R as a dataframe.\nThe two most common functions for this purpose are read.table() and read.csv(). The former is used when our data is contained in a text file, with spaces or tabs separating columns. The latter function is for reading in files with comma-separated-values. If our data is stored as a text file, it is always a good idea to open it and inspect it before getting R to read it in. Text files can be opened with any text editor; csv files can also be opened by Microsoft Excel. When we do so, we should look out for a few things:\n\nAre there column names in the first row, or does the data actually begin in line 1?\nIs it spaces or commas that separate columns?\nAre there trailing values in the last few lines of the file?\n\nThe file crab.txt contains measurements on crabs. If you open up the file outside of R, you should observe that the first line of the file contains column names. By the way, head() is a convenient function for inspecting the first few rows of a dataframe. There is a similar function tail() for inspecting the last few rows.\n\ndata1 &lt;- read.table(\"data/crab.txt\")\nhead(data1)\n\n     V1    V2    V3     V4     V5\n1 color spine width satell weight\n2     3     3  28.3      8  3.050\n3     4     3  22.5      0  1.550\n4     2     1  26.0      9  2.300\n5     4     3  24.8      0  2.100\n6     4     3  26.0      4  2.600\n\n\nThe data has not been read in correctly. To fix this, we need to inform R that the first row functions as the column names/headings.\n\ndata1 &lt;- read.table(\"data/crab.txt\", header=TRUE)\nhead(data1)\n\n  color spine width satell weight\n1     3     3  28.3      8   3.05\n2     4     3  22.5      0   1.55\n3     2     1  26.0      9   2.30\n4     4     3  24.8      0   2.10\n5     4     3  26.0      4   2.60\n6     3     3  23.8      0   2.10\n\n\nIf the first line of the data file does not contain the names of the variables, we can create a vector beforehand to store and then use the names.\n\nvarnames &lt;- c(\"Subject\", \"Gender\", \"CA1\", \"CA2\", \"HW\")\ndata2 &lt;- read.table(\"data/ex_1.txt\", header = FALSE,  \n                    col.names = varnames)\ndata2\n\n  Subject Gender CA1 CA2 HW\n1      10      M  80  84  A\n2       7      M  85  89  A\n3       4      F  90  86  B\n4      20      M  82  85  B\n5      25      F  94  94  A\n6      14      F  88  84  C\n\n\nThe use of read.csv() is very similar, but it is applicable when the fields within each line of the input file are separated by commas instead of tabs or spaces.\n\ndata3 &lt;- read.csv(\"data/ex_1_comma.txt\",  header = FALSE)\ndata3\n\n  V1 V2 V3 V4 V5\n1 10  M 80 84  A\n2  7  M 85 89  A\n3  4  F 90 86  B\n4 20  M 82 85  B\n5 25  F 94 94  A\n6 14  F 88 84  C",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#accessing-parts-of-dataframes",
    "href": "01-intro_to_R.html#accessing-parts-of-dataframes",
    "title": "1  Introduction to R",
    "section": "1.6 Accessing Parts of Dataframes",
    "text": "1.6 Accessing Parts of Dataframes\nWe now turn to the task of accessing a subset of rows and/or columns of a dataframe. The notation uses rectangular brackets, along with a comma inside these brackets to distinguish the row and column specifiers.\nTo access all rows from a particular set of columns, we leave the row column specification empty.\n\ndata3[, 2:4]\n\n  V2 V3 V4\n1  M 80 84\n2  M 85 89\n3  F 90 86\n4  M 82 85\n5  F 94 94\n6  F 88 84\n\n\nTo retrieve a subset of rows, we use the space before the comma.\n\ndata3[1:3, ]\n\n  V1 V2 V3 V4 V5\n1 10  M 80 84  A\n2  7  M 85 89  A\n3  4  F 90 86  B\n\n\nIndividual columns can be retrieved from a dataframe (as a vector) using the $ operator. These columns can then be used to retrieve only the rows that satisfy certain conditions. In order to achieve this task, we turn to logical vectors. The following code returns only the rows corresponding to Gender equal to ``M’’.\n\ndata2[data2$Gender == \"M\", ]\n\n  Subject Gender CA1 CA2 HW\n1      10      M  80  84  A\n2       7      M  85  89  A\n4      20      M  82  85  B\n\n\nLogical vectors contain TRUE/FALSE values in their components. These vectors can be combined with logical operations to yield only the rows that satisfy all conditions. The & operator is the AND operator. Below, we return all rows where Gender is equal to ``M’’ and CA2 is greater than 85.\n\ndata2[data2$Gender == \"M\" & data2$CA2 &gt; 85, ]\n\n  Subject Gender CA1 CA2 HW\n2       7      M  85  89  A\n\n\nFor your reference , the table below contains all the logical operators in R.\n\n\n\nOperator\nDescription\n\n\n\n\n&lt;\nless than\n\n\n&gt;\ngreater than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\nx | y\n(vectorised) OR\n\n\nx & y\n(vectorised) AND\n\n\n\nThe $ operator is both a getter and a setter, which means we can also use it to add new columns to the dataframe. The following command creates a new column named id, that contains a running sequence of integers beginning from 1; the new column essentially contains row numbers.\n\ndata2$id &lt;- 1:NROW(data2)\n\nBefore we leave this section on dataframes, we shall touch on how we can rearrange the dataframe according to particular columns in either ascending or descending order.\n\ndata2[order(data2$CA1), ]\n\n  Subject Gender CA1 CA2 HW id\n1      10      M  80  84  A  1\n4      20      M  82  85  B  4\n2       7      M  85  89  A  2\n6      14      F  88  84  C  6\n3       4      F  90  86  B  3\n5      25      F  94  94  A  5\n\n# arranges in reverse order:\ndata2[rev(order(data2$CA1)), ] \n\n  Subject Gender CA1 CA2 HW id\n5      25      F  94  94  A  5\n3       4      F  90  86  B  3\n6      14      F  88  84  C  6\n2       7      M  85  89  A  2\n4      20      M  82  85  B  4\n1      10      M  80  84  A  1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#sec-r-loops",
    "href": "01-intro_to_R.html#sec-r-loops",
    "title": "1  Introduction to R",
    "section": "1.7 Loops in R",
    "text": "1.7 Loops in R\nwhile loops execute a set of instructions as long as a particular condition holds true. On the other hand, for loops iterate over a set of items, executing a set of instructions each time.\n\nWhile LoopsFor Loops\n\n\nThe syntax for a while loop is as follows. The condition is checked at the beginning of each iteration. As long as it is TRUE, the set of R expressions within the curly braces will be executed.\n\nwhile( &lt;logical condition&gt; ) {\n  R expressions\n  ...\n}\n\nHere is an example of a while loop that increments a value until it reaches 10.\n\nx &lt;- 0 \nS &lt;- 0\nwhile(x&lt;=10) {\n  S &lt;- S + x  \n  x &lt;- x + 1\n}\nS\n\n[1] 55\n\n\n\n\nThe general syntax for a for loop is as follows:\n\nfor(&lt;index&gt; in &lt;set&gt; ) {\n  R expressions\n  ...\n}\n\nThe “set” can be a sequence generated by the colon operator, or it can be any vector. Here is the same code from the while loop. Notice that x does not have to be initialised.\n\nS &lt;- 0\nfor(x in 1:10){\n  S &lt;- S + x\n}\nS\n\n[1] 55\n\n\n\n\n\nAs a second example, consider these lines of R code, which prints out all squares of integers from 1 to 5. The cat( ) function concatenates a given sequence of strings and prints them to the console. We shall see more of it in Section 1.8.\n\nx &lt;- 0            \ntest &lt;- TRUE \n\nwhile(test) {\n  x &lt;- x+1 \n  test &lt;- x&lt;6\n  cat(x^2, test, \"\\n\") \n}\n\n1 TRUE \n4 TRUE \n9 TRUE \n16 TRUE \n25 TRUE \n36 FALSE",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#sec-redir-r-output",
    "href": "01-intro_to_R.html#sec-redir-r-output",
    "title": "1  Introduction to R",
    "section": "1.8 Redirecting R Output",
    "text": "1.8 Redirecting R Output\nThe cat() function can be used to print informative statements as our loop is running. This can be very helpful in debugging our code. It works by simply joining any strings given to it, and then printing them out to the console. The argument \"\\n\" instructs R to print a newline character after the strings.\n\ncat(\"The current number is\", x^2, \"\\n\") \n\nThe current number is 36 \n\n\nWhen we are running a job in the background, we may want the output to print to a file so that we can inspect it later at our convenience. That is where the sink() function comes in.\n\nsink(\"data/datasink_ex1.txt\")      # turn the sink on\nx &lt;- 0            \ntest &lt;- TRUE \n\nwhile(test) {\n  x &lt;- x+1 \n  test &lt;- isTRUE(x&lt;6)  \n  cat(x^2, test, \"\\n\")             # This will be written to the file.\n}\n\n1 TRUE \n4 TRUE \n9 TRUE \n16 TRUE \n25 TRUE \n36 FALSE \n\nsink()                             # turn the sink off \n\nWhen we have finished working with a dataframe, we may want save it to a file. For this purpose, we can use the following code. Once executed, the dataframe data2 will be written to a csv file named ex_1_with_IQ.csv in the data/ directory.\n\nwrite.csv(data2, \"data/ex_1_with_IQ.csv\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#user-defined-functions-in-r",
    "href": "01-intro_to_R.html#user-defined-functions-in-r",
    "title": "1  Introduction to R",
    "section": "1.9 User-Defined Functions in R",
    "text": "1.9 User-Defined Functions in R\nWe have already seen several useful functions in R, e.g. read.csv and head. Here is a list of other commonly used functions.\n\n\n\nFunction\nDescription\n\n\n\n\nmax(x)\nMaximum value of x\n\n\nmin(x)\nMinimum value of x\n\n\nsum(x)\nTotal of all the values in x\n\n\nmean(x)\nArithmetic average values in x\n\n\nmedian(x)\nMedian value of x\n\n\nrange(x)\nVector of length 2: min(x), max(x)\n\n\nvar(x)\nSample variance of x\n\n\ncor(x, y)\nCorrelation between vectors x and y\n\n\nsort(x)\nSorted version of x\n\n\n\nR is a fully-fledged programming language, so it is also possible to write our own functions in R. To define a function for later use in R, the syntax is as follows:\n\nfn_name &lt;- function(arguments) {\n  R expressions\n  ...\n  Returned object\n\n}\n\nThe final line of the function definition will determine what gets returned when the function is executed. Here is an example of a function that computes the circumference of a circle of given radius.\n\ncircumference &lt;- function(r) {\n  2*pi*r\n}\ncircumference(1.3)\n\n[1] 8.168141",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#miscellaneous",
    "href": "01-intro_to_R.html#miscellaneous",
    "title": "1  Introduction to R",
    "section": "1.10 Miscellaneous",
    "text": "1.10 Miscellaneous\n\nGetting Help\nAll functions in R are documented. When you need to find out more about the arguments of a function, or if you need examples of code that is guaranteed to work, then do look up the help page (even before turning to stackexchange). The help pages within R are accessible even if you are offline.\nTo access the help page of a particular function, use the following command:\n\n?mean\n\nIf you are not sure about the name of the function, you can use the following fuzzy search operator to return a page with a list of matching functions:\n\n??mean",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#installing-packages",
    "href": "01-intro_to_R.html#installing-packages",
    "title": "1  Introduction to R",
    "section": "1.11 Installing Packages",
    "text": "1.11 Installing Packages\nR is an open-source software. Many researchers and inventors of new statistical methodologies contribute to the software through packages1. At last check (Dec 2024), there are 21749 such packages. The packages can be perused by name, through this link.\nTo install one of these packages, you can use install.packages(). For instance, this command will install stringr (a package for string manipulations) and all its dependencies on your machine.\n\ninstall.packages(\"stringr\")\n\nOnce the installation is complete, we still need to load the package whenever we wish to use the functions within it. This is done with the library() function:\n\nlibrary(stringr)\n\nTo access a list of all available functions from a package, use:\n\nhelp(package=\"stringr\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#further-readings",
    "href": "01-intro_to_R.html#further-readings",
    "title": "1  Introduction to R",
    "section": "1.12 Further Readings",
    "text": "1.12 Further Readings\nIn our course, we will only be using basic R syntax, functions and plots. You may have heard of the tidyverse set of packages, which are a suite of packages that implement a particular paradigm of data manipulation and plotting. You can read and learn more about that approach by taking DSA2101, or by learning from Wickham, Çetinkaya-Rundel, and Grolemund (2023).\nThe DataCamp courses cover a little more on R e.g. use of apply family of functions. These will be included in our course, so please pay close attention in the DataCamp course.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#heads-up-on-differences-with-python",
    "href": "01-intro_to_R.html#heads-up-on-differences-with-python",
    "title": "1  Introduction to R",
    "section": "1.13 Heads-Up on Differences with Python",
    "text": "1.13 Heads-Up on Differences with Python\nIf you are coming from a Python background, please remember the following key differences:\n\nThe colon operator in R is not a slice operator (like in Python)\nA list in R is similar to Python in that it is a generic collection, but accessing the elements is done with a $ notation.\nThe assignment operator in R is &lt;-, but in Python it is =.\nTo create vectors in R, you need to use c( ).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#references",
    "href": "01-intro_to_R.html#references",
    "title": "1  Introduction to R",
    "section": "1.14 References",
    "text": "1.14 References\n\nWebsite References\n\nIris data: More information on this classic dataset.\nInstalling R\nInstalling Rstudio\n\n\n\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media, Inc.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-intro_to_R.html#footnotes",
    "href": "01-intro_to_R.html#footnotes",
    "title": "1  Introduction to R",
    "section": "",
    "text": "packages are simply collections of functions.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "02-intro_to_python.html",
    "href": "02-intro_to_python.html",
    "title": "2  Introduction to Python",
    "section": "",
    "text": "2.1 Introduction\nPython is a general-purpose programming language. It is a higher-level language than C, C++ and Java in the sense that a Python program does not have to be compiled before execution.\nIt was originally conceived back in the 1980s by Guido van Rossum at Centrum Wiskunde & Informatica (CWI) in the Netherlands. The language is named after a BBC TV show (Guido’s favorite program) “Monty Python’s Flying Circus”.\nPython reached version 1.0 in January 1994. Python 2.0 was released on October 16, 2000. Python 3.0, which is backwards-incompatible with earlier versions, was released on 3 December 2008.\nPython is a very flexible language; it is simple to learn yet is fast enough to be used in production. Over the past ten years, more and more comprehensive data science toolkits (e.g. scikit-learn, NTLK, tensorflow, keras) have been written in Python and are now the standard frameworks for those models.\nJust like R, Python is an open-source software. It is free to use and extend.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "02-intro_to_python.html#installing-python-and-jupyter-lab",
    "href": "02-intro_to_python.html#installing-python-and-jupyter-lab",
    "title": "2  Introduction to Python",
    "section": "2.2 Installing Python and Jupyter Lab",
    "text": "2.2 Installing Python and Jupyter Lab\nTo install Python, navigate to the official Python download page to obtain the appropriate installer for your operating system.\n\n\n\n\n\n\nImportant\n\n\n\nFor our class, please ensure that you are using at least Python 3.12.\n\n\nThe next step is to create a virtual environment for this course. Virtual environments are specific to Python. They allow you to retain multiple versions of Python, and of packages, on the same computer. Go through the videos on Canvas relevant to your operating system to create a virtual environment and install Jupyter Lab on your machine.\nJupyter notebooks are great for interactive work with Python, but more advanced users may prefer a full-fledged IDE. If you are an advanced user, and are comfortable with an IDE of your own choice (e.g. Spyder or VSCode), feel free to continue using that to run the codes for this course.\n\n\n\n\n\n\nImportant\n\n\n\nEven if you are using Anaconda/Spyder/VSCode, you still need to create a virtual environment.\n\n\nJupyter notebooks consist of cells, which can be of three main types:\n\ncode cells,\noutput cells, and\nmarkdown cells.\n\n\n\n\n\n\n\nFigure 2.1: Jupyter Lab\n\n\n\nIn Figure 2.1, the red box labelled 1 is a markdown cell. It can be used to contain descriptions or summary of the code. The cells in the box labelled 2 are code cells. To run the codes from our notes, you can copy and paste the codes into a new cell, and then execute them with Ctrl-Enter.\nTry out this Easter egg that comes with any Python installation:\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\nMore information on using Jupyter notebooks can be obtained from this link.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "02-intro_to_python.html#basic-data-structures-in-python",
    "href": "02-intro_to_python.html#basic-data-structures-in-python",
    "title": "2  Introduction to Python",
    "section": "2.3 Basic Data Structures in Python",
    "text": "2.3 Basic Data Structures in Python\nThe main objects in native1 Python that contain data are\n\nLists, which are defined with [ ]. Lists are mutable.\nTuples, which are defined with ( ). Tuples are immutable.\nDictionaries, which are defined with { }. Dictionaries have keys and items. They are also mutable.\n\nVery soon, we shall see that for statistics, the more common objects we shall deal with are dataframes (from pandas) and arrays (from numpy). However, the latter two require add-on packages; the three object classes listed above are baked into Python.\nBy the way, this is what mean by (im)mutable:\n\nx = [1, 3, 5, 7, 8, 9, 10]\n\n# The following is OK, because \"x\" is a list, and hence mutable\nx[3] = 17     \nprint(x)  \n\n[1, 3, 5, 17, 8, 9, 10]\n\n\n\n# The following will return an error, because x_tuple is a tuple, and hence \n# immutable.\nx_tuple = (1, 3, 5, 6, 8, 9, 10)\nx_tuple[3] = 17 \n\n\n\n\n\n\n\nNote\n\n\n\nNote that we do not need the c( ) function, like we did in R. This is a common mistake I make when switching between the two languages.\n\n\nHere is how we create lists, tuples and dictionaries.\n\nx_list = [1, 2, 3]\nx_tuple = (1, 2, 3)\nx_dict = {'a': 1, 'b': 2, 'c': 3} # access with x_dict['a']",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "02-intro_to_python.html#slice-operator-in-python",
    "href": "02-intro_to_python.html#slice-operator-in-python",
    "title": "2  Introduction to Python",
    "section": "2.4 Slice Operator in Python",
    "text": "2.4 Slice Operator in Python\nOne important point to take note is that, contrary to R, Python begins indexing of objects starting with 0. Second, the slice operator in Python is a little more powerful than in R. It can be used to extract regular sequences from a list, tuple or string easily.\nIn general, the syntax is &lt;list-like object&gt;[a:b], where a and b are integers. Such a call would return the elements at indices a, a+1 until b-1. Take note that the end point index is not included.\n\nchar_list = ['P', 'y', 't', 'h', 'o', 'n']\nchar_list[0]           # returns first element\n\n'P'\n\nchar_list[-1]          # returns last element\n\n'n'\n\nlen(char_list)         # returns number of elements in list-like object.\n\n6\n\nchar_list[::2]         # from first to last, every 2 apart.\n\n['P', 't', 'o']\n\n\nThis indexing syntax is used in the additional packages we use as well, so it is good to know about it. Figure 2.2 displays a pictorial representation of how positive and negative indexes work together.\n\n\n\n\n\n\nFigure 2.2: Positive and negative indices",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "02-intro_to_python.html#numpy-arrays",
    "href": "02-intro_to_python.html#numpy-arrays",
    "title": "2  Introduction to Python",
    "section": "2.5 Numpy Arrays",
    "text": "2.5 Numpy Arrays\nJust like R, Python has several contributed packages that are essential for statistics and data analysis. These include numpy and pandas. These appropriate versions of these packages would have been installed if you had used the requirements file when setting up Python.\n\nimport numpy as np\n\narray1 = np.array([1, 2, 3, 4, 5])\narray2 = np.array([6, 7, 8, 9, 10])\nmatrix1 = np.array([array1, array2])\nprint(matrix1)\n\n[[ 1  2  3  4  5]\n [ 6  7  8  9 10]]\n\n\nThe slice operator can then be used in each dimension of the matrix to subset it.\n\nmatrix1[0, 0::3]       # returns first row, columns 1 and 4\n\narray([1, 4])\n\nmatrix1[1, 1:3]        # returns second row, columns 2 and 3\n\narray([7, 8])\n\n\nThe numpy arrays are objects in Python, with several methods associated with them. For instance, here are a couple and how we can use them:\n\n# To obtain the dimensions of an array:\nmatrix1.shape\n\n(2, 5)\n\n# To transpose a 2-D array\nmatrix1.T\n\narray([[ 1,  6],\n       [ 2,  7],\n       [ 3,  8],\n       [ 4,  9],\n       [ 5, 10]])\n\n\nHere is a table with some common operations that you can apply on a numpy array. The objects referred to in the second column are from the earlier lines of code.\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nshape\nReturns dimensions, e.g. matrix1.shape\n\n\nT\nTransposes the array, e.g. matrix1.T\n\n\nmean\nComputes col- or row-wise means, e.g. matrix1.mean(axis=0) or matrix1.mean(axis=1)\n\n\nsum\nComputes col- or row-wise means, e.g. matrix1.sum(axis=0) or matrix1.sum(axis=1)\n\n\nargmax\nReturn the index corresponding to the max within the specified dimension, e.g. matrix1.argmax(axis=0) for the position with the max within each column.\n\n\nreshape\nTo change the dimensions, e.g. array1.reshape((5,1)) converts the array into a 5x1 matrix\n\n\n\nTo combine arrays, we use the functions vstack and hstack. These are analogous to rbind and cbind in R.\n\nnp.vstack([matrix1, array1])\n\narray([[ 1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10],\n       [ 1,  2,  3,  4,  5]])\n\nnp.hstack([array1.reshape((5,1)), \n           array2.reshape((5,1)),\n           matrix1.T])\n\narray([[ 1,  6,  1,  6],\n       [ 2,  7,  2,  7],\n       [ 3,  8,  3,  8],\n       [ 4,  9,  4,  9],\n       [ 5, 10,  5, 10]])",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "02-intro_to_python.html#pandas-dataframes",
    "href": "02-intro_to_python.html#pandas-dataframes",
    "title": "2  Introduction to Python",
    "section": "2.6 Pandas DataFrames",
    "text": "2.6 Pandas DataFrames\nThe next important add-on package that we shall work with is pandas. It provides a DataFrame class of objects for working with tabular data, just like data.frame within R. However, there are some syntactic differences with R that we shall get to soon. The following command creates a simple pandas\n\nimport pandas as pd\n\ndata = {'X': [1,2,3,4,5,6], 'Y': [6,5,4,3,2,1]}\ndf = pd.DataFrame(data, columns =['X', 'Y'])\nprint(df)\n\n   X  Y\n0  1  6\n1  2  5\n2  3  4\n3  4  3\n4  5  2\n5  6  1\n\n\nWe will get into the syntax for accessing subsets of the dataframe soon, but for now, here is how we can extract a single column from the dataframe. The resulting object is a pandas Series, which is a lot like a 1-D array, and can be indexed like one as well.\n\ncol_x = df.X\ncol_x[0:3]\n\n0    1\n1    2\n2    3\nName: X, dtype: int64\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe built-in objects in Python are lists, tuples and dictionaries. Lists and tuples can contain elements of different types, e.g. strings and integers in a single object. However, they have no dimensions, so to speak of. Numpy arrays can be high-dimensional structures. In addition, the elements have to be homogeneous. For instance, in a 2x2x2 numeric numpy array, every one of the 8 elements has to be a float point number. Pandas DataFrames are tabular objects, where, within a column, each element has to be of the same type.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "02-intro_to_python.html#reading-data-into-python",
    "href": "02-intro_to_python.html#reading-data-into-python",
    "title": "2  Introduction to Python",
    "section": "2.7 Reading Data into Python",
    "text": "2.7 Reading Data into Python\nLet us begin with the same file that we began with in the topic on R: crab.txt. In Section 1.5, we observed that this file contained headings, and that the columns were separated by spaces. The pandas function to read in such text files is read_table(). It has numerous optional arguments, but in this case we just need these two:\n\ndata1 = pd.read_table('data/crab.txt', header=0, sep=\"\\\\s+\")\ndata1.head()\n\n   color  spine  width  satell  weight\n0      3      3   28.3       8    3.05\n1      4      3   22.5       0    1.55\n2      2      1   26.0       9    2.30\n3      4      3   24.8       0    2.10\n4      4      3   26.0       4    2.60\n\n\nDo take note of the differences with R - the input to the header argument corresponds to the line number containing the column names. Secondly, the head() function is a method belonging to the DataFrame object.\nWhen the file does not contain column names, we can supply them (as a list or numpy array) when we read the data in. Here is an example:\n\nvarnames = [\"Subject\", \"Gender\", \"CA1\", \"CA2\", \"HW\"]\ndata2 = pd.read_table('data/ex_1.txt', header=None, \n                      names=varnames, sep=\"\\\\s+\")\ndata2\n\n   Subject Gender  CA1  CA2 HW\n0       10      M   80   84  A\n1        7      M   85   89  A\n2        4      F   90   86  B\n3       20      M   82   85  B\n4       25      F   94   94  A\n5       14      F   88   84  C",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "02-intro_to_python.html#subsetting-dataframes-with-pandas",
    "href": "02-intro_to_python.html#subsetting-dataframes-with-pandas",
    "title": "2  Introduction to Python",
    "section": "2.8 Subsetting DataFrames with Pandas",
    "text": "2.8 Subsetting DataFrames with Pandas\nDataFrames in pandas are indexed for efficient searching and retrieval. When subsetting them, we have to add either .loc or .iloc and use it with square brackets.\nThe .loc notation is used when we wish to index rows and columns according to their names. The general syntax is &lt;DataFrame&gt;.loc[ , ]. A slice operator can be used for each row subset and column subset to be retrieved.\n\n# retrieve rows 0,1,2 and columns from color to width\ndata1.loc[0:2, 'color':'width']\n\n   color  spine  width\n0      3      3   28.3\n1      4      3   22.5\n2      2      1   26.0\n\n# retrieve every second row starting from row 0 until row 5, and all columns\ndata1.loc[0:5:2, ]\n\n   color  spine  width  satell  weight\n0      3      3   28.3       8    3.05\n2      2      1   26.0       9    2.30\n4      4      3   26.0       4    2.60\n\n\nThe .iloc notation is used when we wish to index rows and columns using integer values. The general syntax is similar; try this and observe the difference with .loc.\n\ndata1.iloc[0:2, 0:2]\n\n   color  spine\n0      3      3\n1      4      3\n\n\nIf you notice, the .iloc notation respects the rules of the in-built slice operator, in the sense that the end point is not included in the output. On the other hand, the .loc notation includes the end point.\nIn data analysis, a common requirement is to subset a dataframe according to values in columns. Just like in R, this is achieved with logical values.\n\ndata2[data2.Gender == \"M\"]\n\n   Subject Gender  CA1  CA2 HW\n0       10      M   80   84  A\n1        7      M   85   89  A\n3       20      M   82   85  B\n\ndata2[(data2.Gender == \"M\") & (data2.CA2 &gt; 85)]\n\n   Subject Gender  CA1  CA2 HW\n1        7      M   85   89  A",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "02-intro_to_python.html#loops-in-python",
    "href": "02-intro_to_python.html#loops-in-python",
    "title": "2  Introduction to Python",
    "section": "2.9 Loops in Python",
    "text": "2.9 Loops in Python\nIt is extremely efficient to execute “for” loops in Python. Many objects in Python are iterators, which means they can be iterated over. Lists, tuples and dictionaries can all be iterated over very easily.\nBefore getting down to examples, take note that Python does not use curly braces to denote code blocks. Instead, these are defined by the number of indentations in a line.\n\nfor i in x[:2]:\n  print(f\"The current element is {i}.\")\n\nThe current element is 1.\nThe current element is 3.\n\n\nNotice how we do not need to set up any running index; the object is just iterated over directly. The argument to the print() function is an f-string. It is the recommended way to create string literals that can vary according to arguments.\nHere is another example of iteration, this time using dictionaries which have key-value pairs. In this case, we iterate over the keys.\n\ndict1 = {'holmes': 'male', 'watson': 'male', 'mycroft': 'male', \n         'hudson': 'female', 'moriarty': 'male', 'adler': 'female'}\n# dict1['hudson']\n\nfor x in dict1.keys():\n    print(f\"The gender of {x} is {dict1[x]}\")\n\nThe gender of holmes is male\nThe gender of watson is male\nThe gender of mycroft is male\nThe gender of hudson is female\nThe gender of moriarty is male\nThe gender of adler is female\n\n\nIn Section 1.7, we wrote a block of code that incremented an integer until the square was greater than 36. Here is the Python version of that code:\n\nx = 0          \ntest = True \n\nwhile test:\n    x += 1  \n    test = x &lt; 6\n    print(f\"{x**2}, {test}\")\n\n1, True\n4, True\n9, True\n16, True\n25, True\n36, False\n\n\nIt is also straightforward to write a for-loop to perform the above, since we know when the break-point of the loop will be. The np.arange() function generates evenly spaced integers.\n\nfor y in np.arange(6):\n    print(f\"The square of {y} is {y**2}.\")\n\nThe square of 0 is 0.\nThe square of 1 is 1.\nThe square of 2 is 4.\nThe square of 3 is 9.\nThe square of 4 is 16.\nThe square of 5 is 25.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "02-intro_to_python.html#user-defined-functions",
    "href": "02-intro_to_python.html#user-defined-functions",
    "title": "2  Introduction to Python",
    "section": "2.10 User Defined Functions",
    "text": "2.10 User Defined Functions\nThe syntax for creating a new function in Python is as follows:\n\ndef fn_name(arguments):\n  Python statements\n  ...\n  Returned object\n\nHere is the same function as earlier, computing the circumference of a circle with a given radius.\n\nimport math\n\ndef circumference(radius):\n    return 2 * math.pi * radius \n\ncircumference(1.3)\n\n8.168140899333462",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "02-intro_to_python.html#miscellaneous",
    "href": "02-intro_to_python.html#miscellaneous",
    "title": "2  Introduction to Python",
    "section": "2.11 Miscellaneous",
    "text": "2.11 Miscellaneous\n\nPackage installation\nSo far, we have used numpy and pandas, but we shall need to call upon a few other add-on packages we proceed in the course. These include statsmodels, scipy and matplotlib.\n\n\nGetting help\nMost functions in Python are well-documented. In order to access this documentation from within a Jupyter notebook, use the ? operator. For more details, including the source code, use the ?? operator. For instance, for more details of the pd.read_csv() function, you can execute this command:\n\npd.read_csv?\n\nThe internet is full of examples and how-to’s for Python; help is typically just a Google search or a chatGPT query away. However, it is always better to learn from the ground up instead of through snippets for specific tasks. Please look through the websites in Section 2.13.1 below.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "02-intro_to_python.html#major-differences-with-r",
    "href": "02-intro_to_python.html#major-differences-with-r",
    "title": "2  Introduction to Python",
    "section": "2.12 Major Differences with R",
    "text": "2.12 Major Differences with R\nBefore we leave this topic, take note of some very obvious differences with R:\n\nThe assignment operator in R is &lt;-; for Python it is =.\nWhen creating vectors in R, you will need c( ), but in Python, this is not the case.\nR implements it’s object oriented mechanism in a different manner from Python. For instance, when plotting with R, you would call plot(&lt;object&gt;) but in Python, you would call &lt;object&gt;.plot(). In Python, the methods belong to the class, but not in R.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "02-intro_to_python.html#references",
    "href": "02-intro_to_python.html#references",
    "title": "2  Introduction to Python",
    "section": "2.13 References",
    "text": "2.13 References\n\nWebsite References\n\nBeginner’s guide to Numpy: This is from the official numpy documentation website.\n10 minutes to Pandas: This is a quickstart to pandas, from the official website. You can find more tutorials on this page too.\nPython official documentation: This is from the official Python page. It contains a tutorial, an overview of all built-in packages, and several howto’s, including on regular expression. A very good website to learn from.\nPython download\nJupyter Lab help",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "02-intro_to_python.html#footnotes",
    "href": "02-intro_to_python.html#footnotes",
    "title": "2  Introduction to Python",
    "section": "",
    "text": "i.e., Python without any packages imported.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "03-summarising_data.html",
    "href": "03-summarising_data.html",
    "title": "3  Exploring Quantitative Data",
    "section": "",
    "text": "3.1 Introduction\nThis is the first topic where we are going to see how to perform statistical analysis using two software: R and Python. Every language has it’s strengths and weaknesses, so instead of attempting to exactly replicate the same chart/output in each software, we shall try to understand their respective approaches better.\nOur end goal is to analyse data - toward that end, we should be versatile and adaptable. Focus on learning how to be fast and productive in whichever environment you are told to work in. If you have a choice, then be aware of what each framework can do best, so that you can choose the right one.\nAlthough it is a simplistic view, the following diagram, Figure 3.1, provides a useful taxonomy of the types of columns we might have in our dataset. Having at least an initial inkling of the type of data matters, because it helps decide what type of summary to generate or what type of plot to make. Quantitative data are sometimes also referred to as numerical data.\nThere are two main ways of summarising data: numerically and graphically. This topic will cover:\nTechniques for categorical variables will be covered in Section 4.1.\nBefore proceeding, let us introduce one of the datasets that we’ll be using in this textbook. The dataset comes from the UCI Machine Learning Repository, which is a very useful place to get datasets for practice.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring Quantitative Data</span>"
    ]
  },
  {
    "objectID": "03-summarising_data.html#introduction",
    "href": "03-summarising_data.html#introduction",
    "title": "3  Exploring Quantitative Data",
    "section": "",
    "text": "Figure 3.1: Data types\n\n\n\n\n\nNumerical summaries for univariate quantitative variables.\nNumerical summary for association between two quantitative variables.\nUseful graphs for univariate and bivariate quantitative variables.\n\n\n\n\nExample 3.1 (Student Performance: Dataset Introduction) \nThe particular dataset can be downloaded from this page. Once you unzip the file, you will find two csv files in the student/ folder:\n\nstudent-mat.csv (performance in Mathematics)\nstudent-por.csv (performance in Portugese)\n\nEach dataset was collected using school reports and questionnaires. Each row corresponds to a student. The columns are attributes, including student grades, demographic information, and other social and school-related information. Each file corresponds to the students’ performance in one of the two subjects. For more information, you can refer to Cortez and Silva (2008).\n\n\n\n\n\n\n\n\n\n#\nFeature\nDescription (Type)\nDetails\n\n\n\n\n1\nschool\nstudent’s school (binary)\n“GP” - Gabriel Pereira, “MS” - Mousinho da Silveira\n\n\n2\nsex\nstudent’s sex (binary)\n“F” - female, “M” - male\n\n\n3\nage\nstudent’s age (numeric)\nfrom 15 to 22\n\n\n4\naddress\nstudent’s home address type (binary)\n“U” - urban, “R” - rural\n\n\n5\nfamsize\nfamily size (binary)\n“LE3” - less or equal to 3, “GT3” - greater than 3\n\n\n6\nPstatus\nparent’s cohabitation status (binary)\n“T” - living together, “A” - apart\n\n\n7\nMedu\nmother’s education (numeric)\n0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education, 4 - higher education\n\n\n8\nFedu\nfather’s education (numeric)\n0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education, 4 - higher education\n\n\n9\nMjob\nmother’s job (nominal)\nteacher, health, civil services, at_home, other\n\n\n10\nFjob\nfather’s job (nominal)\nteacher, health, civil services, at_home, other\n\n\n11\nreason\nreason to choose this school (nominal)\nclose to home, school reputation, course preference, other\n\n\n12\nguardian\nstudent’s guardian (nominal)\nmother, father, other\n\n\n13\ntraveltime\nhome to school travel time (numeric)\n1 - &lt;15 min, 2 - 15 to 30 min, 3 - 30 min to 1 hour, 4 - &gt;1 hour\n\n\n14\nstudytime\nweekly study time (numeric)\n1 - &lt;2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, 4 - &gt;10 hours\n\n\n15\nfailures\nnumber of past class failures (numeric)\nn if 1&lt;=n&lt;3, else 4\n\n\n16\nschoolsup\nextra educational support (binary)\nyes or no\n\n\n17\nfamsup\nfamily educational support (binary)\nyes or no\n\n\n18\npaid\nextra paid classes within the course subject (Math or Portuguese) (binary)\nyes or no\n\n\n19\nactivities\nextra-curricular activities (binary)\nyes or no\n\n\n20\nnursery\nattended nursery school (binary)\nyes or no\n\n\n21\nhigher\nwants to take higher education (binary)\nyes or no\n\n\n22\ninternet\nInternet access at home (binary)\nyes or no\n\n\n23\nromantic\nwith a romantic relationship (binary)\nyes or no\n\n\n24\nfamrel\nquality of family relationships (numeric)\nfrom 1 - very bad to 5 - excellent\n\n\n25\nfreetime\nfree time after school (numeric)\nfrom 1 - very low to 5 - very high\n\n\n26\ngoout\ngoing out with friends (numeric)\nfrom 1 - very low to 5 - very high\n\n\n27\nDalc\nworkday alcohol consumption (numeric)\nfrom 1 - very low to 5 - very high\n\n\n28\nWalc\nweekend alcohol consumption (numeric)\nfrom 1 - very low to 5 - very high\n\n\n29\nhealth\ncurrent health status (numeric)\nfrom 1 - very bad to 5 - very good\n\n\n30\nabsences\nnumber of school absences (numeric)\nfrom 0 to 93\n\n\n\nIn the explanatory variables above, notice that also severable variables have been stored in the dataset file using numbers, they are in fact categorical variables. Examples of these are variables 24 to 29. It seems fair to treat the three output columns as numeric though. G3 is the main output variable.\n\n\n\n\n\n\n\n\n\n#\nFeature\nDescription (Type)\nDetails\n\n\n\n\n31\nG1\nfirst period grade (numeric)\nfrom 0 to 20\n\n\n32\nG2\nsecond period grade (numeric)\nfrom 0 to 20\n\n\n33\nG3\nfinal grade (numeric, output target)\nfrom 0 to 20",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring Quantitative Data</span>"
    ]
  },
  {
    "objectID": "03-summarising_data.html#numerical-summaries",
    "href": "03-summarising_data.html#numerical-summaries",
    "title": "3  Exploring Quantitative Data",
    "section": "3.2 Numerical Summaries",
    "text": "3.2 Numerical Summaries\nNumerical summaries include:\n\nBasic information about the data, e.g. number of observations and missing values.\nMeasures of central tendency, e.g. mean, median\nMeasures of spread, e.g. standard deviation, IQR (interquartile range), range.\n\n\nExample 3.2 (Student Performance: Numerical Summaries) \nLet us read in the dataset and generate numerical summaries of the output variable of interest (G3).\n\nR codePython code\n\n\n\nstud_perf &lt;- read.table(\"data/student/student-mat.csv\", sep=\";\", \n                        header=TRUE)\nsummary(stud_perf$G3)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    8.00   11.00   10.42   14.00   20.00 \n\nsum(is.na(stud_perf$G3))\n\n[1] 0\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nstud_perf  = pd.read_csv(\"data/student/student-mat.csv\", delimiter=\";\")\nstud_perf.G3.describe()\n\ncount    395.000000\nmean      10.415190\nstd        4.581443\nmin        0.000000\n25%        8.000000\n50%       11.000000\n75%       14.000000\nmax       20.000000\nName: G3, dtype: float64\n\n#stud_perf.G3.info()\n\n\n\n\nFrom the output, we can understand that we have 395 observations, ranging from 0 to 20. I would surmise that the data is more or less symmetric in the middle (distance from 3rd-quartile to median is identical to distance from median to 1st-quartile). There are no missing values in the data.\nHowever, summaries of a single variable are rarely useful since we do not have a basis for comparison. In this dataset, we are interested in how the grade varies with one or some of the other variables. Let’s begin with Mother’s education.\n\nR codePython code\n\n\n\nround(aggregate(G3 ~ Medu, data=stud_perf, FUN=summary), 2)\n\n  Medu G3.Min. G3.1st Qu. G3.Median G3.Mean G3.3rd Qu. G3.Max.\n1    0    9.00      12.00     15.00   13.00      15.00   15.00\n2    1    0.00       7.50     10.00    8.68      11.00   16.00\n3    2    0.00       8.00     11.00    9.73      13.00   19.00\n4    3    0.00       8.00     10.00   10.30      13.00   19.00\n5    4    0.00       9.50     12.00   11.76      15.00   20.00\n\ntable(stud_perf$Medu)\n\n\n  0   1   2   3   4 \n  3  59 103  99 131 \n\n\n\n\n\nstud_perf[['Medu', 'G3']].groupby('Medu').describe()\n\n         G3                                                  \n      count       mean       std  min   25%   50%   75%   max\nMedu                                                         \n0       3.0  13.000000  3.464102  9.0  12.0  15.0  15.0  15.0\n1      59.0   8.677966  4.364594  0.0   7.5  10.0  11.0  16.0\n2     103.0   9.728155  4.636163  0.0   8.0  11.0  13.0  19.0\n3      99.0  10.303030  4.623486  0.0   8.0  10.0  13.0  19.0\n4     131.0  11.763359  4.267646  0.0   9.5  12.0  15.0  20.0\n\n\n\n\n\nNow we begin to understand the context of G3 a little better. As the education level of the mother increases, the mean does increase. The middle 50-percent of the grade does seem to increase as well. The exception is the case where the mother has no education, but we can see that there are only 3 observations in that category so we should read too much into it.\n\nHere are some things to note about numerical summaries:\n\nIf the mean and the median are close to each other, it indicates that the distribution of the data is close to symmetric.\nThe mean is sensitive outliers but the median is not. We shall see more about this in Section 5.1.\nWhen the mean is much larger than the median, it suggests that there could be a few very large observations. It has resulted in a right-skewed distribution. Conversely, if the mean is much smaller than the median, we probably have a left-skewed distribution.\n\nWhile numerical summaries provide us with some basic information about the data, they also leave out a lot. Even for experts, it is possible to have a wrong mental idea about the data from the numerical summaries alone. For instance, all three histograms in Figure 3.2 have a mean of 0 and standard deviation of 1!\n\n\n\n\n\n\nFigure 3.2: Three datasets with mean 0 and s.d. 1\n\n\n\nThat’s why we have to turn to graphics as well, in order to summarise our data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring Quantitative Data</span>"
    ]
  },
  {
    "objectID": "03-summarising_data.html#graphical-summaries",
    "href": "03-summarising_data.html#graphical-summaries",
    "title": "3  Exploring Quantitative Data",
    "section": "3.3 Graphical Summaries",
    "text": "3.3 Graphical Summaries\n\nHistograms\nThe first chart type we shall consider is a histogram. A histogram is a graph that uses bars to portray the frequencies or relative frequencies of the possible outcomes for a quantitative variable.\nWhen we create a histogram, here are some things that we look for:\n\nWhat is the overall pattern?: Do the data cluster together, or is there a gap such that one or more observations deviate from the rest?\nDo the data have a single mound or peak? If yes, then we have what is known as a unimodal distribution. Data with two ‘peaks’ are referred to as bimodal, and data with many peaks are referred to as multimodal. See Figure 3.4.\nIs the distribution symmetric or skewed? See Figure 3.5.\nAre there any suspected outliers? See Figure 3.3.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.3: Histogram with outliers\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.4: Bimodal histogram\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.5: Skewed histograms\n\n\n\n\n\n\n\n\nExample 3.3 (Student Performance: Histograms) \nNow let us return to the student performance dataset.\n\nR codePython code\n\n\n\nhist(stud_perf$G3, main=\"G3 histogram\")\n\n\n\n\n\n\n\n\n\n\n\nfig = stud_perf.G3.hist(grid=False)\nfig.set_title('G3 histogram');\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDo you notice anything different with the two histograms?\n\n\nIn general, we now have a little more information to accompany the 5 number summaries. It appears that the distribution is not a basic unimodal one. There is a large spike of about 40 students who scored very low scores.\nAs we mentioned earlier, it is not useful to inspect a histogram in a silo. Hence we shall condition on Mother’s education once more, to create separate histograms for each group. Remember that this is a case where the response variable G3 is quantitative, and the explanatory variable Medu is ordinal.\n\nR codePython code\n\n\n\nlibrary(lattice)\nhistogram(~G3 | Medu, data=stud_perf, type=\"density\",\n          as.table=TRUE)\n\n\n\n\n\n\n\n\n\n\n\nstud_perf.G3.hist(by=stud_perf.Medu, figsize=(15,10), density=True, \n                  layout=(2,3));\n\n\n\n\n\n\n\n\n\n\n\nAlthough the heights of the panels in the two versions are not identical, we can, by looking at either one, see that the proportion of 0-scores reduces as the mother’s education increases. Perhaps this is reading too much into the dataset, but there seem to be more scores on the higher end for highest educated mothers.\n\n\n\nDensity Plots\nHistograms are not perfect - when using them, we have to experiment with the bin size since this could mask details about the data. It is also easy to get distracted by the blockiness of histograms. An alternative to histograms is the kernel density plot. Essentially, this is obtained by smoothing the heights of the rectangles in a histogram.\nSuppose we have observed an i.i.d sample \\(x_1, x_2, \\ldots, x_n\\) from a continuous pdf \\(f(\\cdot)\\). Then the kernel density estimate at \\(x\\) is given by\n\\[\n\\hat{f}(x)  = \\frac{1}{nh} \\sum_{i=1}^n K \\left( \\frac{x - x_i}{h} \\right)\n\\] where\n\n\\(K\\) is a density function. A typical choice is the standard normal. The kernel places greater weights on nearby points (to \\(x\\)).\n\\(h\\) is a bandwidth, which determines which of the nearest points are used. The effect is similar to the number of bins in a histogram.\n\n\nExample 3.4 (Student Performance: Density Estimates) \n\nR codePython code\n\n\n\ndensityplot(~G3, groups=Medu, data=stud_perf, auto.key = TRUE, bw=1.5)\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nf, axs = plt.subplots(2, 3, squeeze=False, figsize=(15,6))\nout2 = stud_perf.groupby(\"Medu\")\nfor y,df0 in enumerate(out2):\n    tmp = plt.subplot(2, 3, y+1)\n    df0[1].G3.plot(kind='kde')\n    tmp.set_title(df0[0])\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, with density plots it is also possible to overlay them for closer comparison. This is not possible with histograms without some transparency in the rectangle colours.\n\n\n\nBoxplots\nA boxplot provides a skeletal representation of a distribution. Boxplots are very well suited for comparing multiple groups.\nHere are the steps for drawing a boxplot:\n\nDetermine \\(Q_1\\), \\(Q_2\\) and \\(Q_3\\). The box is made from \\(Q_1\\) and \\(Q_3\\). The median is drawn as a line or a dot within the box.\nDetermine the max-whisker reach: \\(Q_3 + 1.5 \\times IQR\\); the min-whisker reach by \\(Q_1 − 1.5 \\times IQR\\).\nAny data point that is out of the range from the min to max whisker reach is classified as a potential outlier.\nExcluding the potential outliers, the maximum point determines the upper whisker and the minimum point determines the lower whisker of a boxplot.\n\nA boxplot helps us to identify the median, lower and upper quantiles and outlier(s) (see Figure 3.6).\n\n\n\n\n\n\nFigure 3.6: Boxplot construction\n\n\n\n\nExample 3.5 (Student Performance: Boxplots) \nInstead of using mother’s education once again, we use the number of times a student goes out (goout) as the explanatory variable this time. From the boxplot outputs, it appears there is no strong strictly increasing/decreasing trend associated with G3. Instead, although the differences between the categories are not large, it seems as though there is an “optimal” number of times that students could go out. Too little and too much leads to lower median G3 scores.\n\nR codePython code\n\n\n\nbwplot(G3 ~ goout, horizontal = FALSE, data=stud_perf)\n\n\n\n\n\n\n\n\n\n\n\nstud_perf.plot.box(column='G3', by='goout')\n\nG3    Axes(0.125,0.11;0.775x0.77)\ndtype: object\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQQ-plots\nFinally, we turn to QQ-plots. A Quantile-Quantile plot is a graphical diagnostic tool for assessing if a dataset follows a particular distribution. Most of the time we would be interested in comparing against a Normal distribution.\nA QQ-plot plots the standardized sample quantiles against the theoretical quantiles of a N(0; 1) distribution. If they fall on a straight line, then we would say that there is evidence that the data came from a normal distribution.\nEspecially for unimodal datasets, the points in the middle will typically fall close to the line. The value of a QQ-plot is in judging if the tails of the data are fatter or thinner than the tails of the Normal.\n\n\n\n\n\n\n\n\n\n\n\n(a) Thinner than Normal\n\n\n\n\n\n\n\n\n\n\n\n(b) Fatter than Normal\n\n\n\n\n\n\n\nFigure 3.7: QQ-plots\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlease be careful! Some software/packages will switch the axes (i.e. plot the sample quantiles on the x-axis instead of the y-axis, unlike Figure 3.7). Please observe and interpret accordingly.\n\n\n\nExample 3.6 (Concrete Slump: Dataset Introduction) \nConcrete is a highly complex material. The slump flow of concrete is not only determined by the water content, but that is also influenced by other concrete ingredients. The UCI page for this dataset is here. The reference for this article is Yeh (2007).\nThe data set includes 103 data points. There are 7 input variables, and 3 output variables in the data set. These are the input columns in the data, all in units of kg per \\(m^3\\) concrete:\n\n\n\n\n\n\n\n\n#\nFeature\nDetails\n\n\n\n\n1\nCement\n\n\n\n2\nSlag\n\n\n\n3\nFly ash\n\n\n\n4\nWater\n\n\n\n5\nSP\nA super plasticizer to improve consistency.\n\n\n6\nCoarse Aggr.\n\n\n\n7\nFine Aggr.\n\n\n\n\nThere are three output variables in the dataset. You can read more about Slump and Flow from this wikipedia page.\n\n\n\n#\nFeature\nUnits\n\n\n\n\n1\nSLUMP\ncm\n\n\n2\nFLOW\ncm\n\n\n3\n28-day Compressive Strength\nMPa\n\n\n\n\nR codePython code\n\n\nTo read in the data in R:\n\nconcrete &lt;- read.csv(\"data/concrete+slump+test/slump_test.data\")\nnames(concrete)[c(1,11)] &lt;- c(\"id\", \"Comp.Strength\")\n\n\n\n\nconcrete = pd.read_csv(\"data/concrete+slump+test/slump_test.data\")\nconcrete.rename(columns={'No':'id', \n                         'Compressive Strength (28-day)(Mpa)':'Comp_Strength'},  \n                inplace=True)\n\n\n\n\n\nLet us consider the Comp.Strength output variable. The histogram overlay in Figure 3.8 suggests some skewness and fatter tails than the Normal.\n\n\n\n\n\n\n\n\nFigure 3.8: Comparing data with reference Normal (blue)\n\n\n\n\n\n\nExample 3.7 (Concrete: QQ-plots) \nThe next chart is a QQ-plot, for assessing deviations from Normality.\n\nR codePython code\n\n\n\nqqnorm(concrete$Comp.Strength)\nqqline(concrete$Comp.Strength)\n\n\n\n\n\n\n\n\n\n\n\nfrom scipy import stats\nimport statsmodels.api as sm\nsm.qqplot(concrete.Comp_Strength, line=\"q\");\n\n\n\n\n\n\n\n\n\n\n\nThe deviation of the tails does not seem to be that large, judging from the QQ-plot.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring Quantitative Data</span>"
    ]
  },
  {
    "objectID": "03-summarising_data.html#correlation",
    "href": "03-summarising_data.html#correlation",
    "title": "3  Exploring Quantitative Data",
    "section": "3.4 Correlation",
    "text": "3.4 Correlation\nWhen we are studying two quantitative variables, the most common numerical summary to quantify the relationship between them is the correlation coefficient. Suppose that \\(x_1, x_2, \\ldots, x_n\\) and \\(y_1, \\ldots, y_n\\) are two variables from a set of \\(n\\) objects or people. The sample correlation between these two variables is computed as:\n\\[\nr = \\frac{1}{n-1} \\sum_{i=1}^n \\frac{(x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y}\n\\] where \\(s_x\\) and \\(s_y\\) are the sample standard deviations. \\(r\\) is an estimate of the correlation between random variables \\(X\\) and \\(Y\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.9: Linear Relation\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.10: Non-linear relation\n\n\n\n\n\n\nA few things to note about the value \\(r\\), which is also referred to as the Pearson correlation:\n\n\\(r\\) is always between -1 and 1.\nA positive value for \\(r\\) indicates a positive association and a negative value for \\(r\\) indicates a negative association.\nTwo variables have the same correlation, no matter which one is coded as \\(X\\) and which one is coded as \\(Y\\).\n\nFigure 3.9 and Figure 3.10 contain sample plots of data and their corresponding \\(r\\) values. Notice how \\(r\\) does not reflect strong non-linear relationships.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring Quantitative Data</span>"
    ]
  },
  {
    "objectID": "03-summarising_data.html#scatterplot-matrices",
    "href": "03-summarising_data.html#scatterplot-matrices",
    "title": "3  Exploring Quantitative Data",
    "section": "3.5 Scatterplot Matrices",
    "text": "3.5 Scatterplot Matrices\n\n\nExample 3.8 (Concrete: Scatterplots) When we have multiple quantitative variables in a dataset, it is common to create a matrix of scatterplots. This allows for simultaneous inspection of bivariate relationships.\n\nR codePython code\n\n\n\ncol_to_use &lt;- c(\"Cement\", \"Slag\", \"Comp.Strength\", \"Water\", \"SLUMP.cm.\",\n                \"FLOW.cm.\")\npairs(concrete[, col_to_use], panel = panel.smooth)\n\n\n\n\n\n\n\n\n\n\n\npd.plotting.scatter_matrix(concrete[['Cement', 'Slag', 'Comp_Strength', 'Water', \n                                     'SLUMP(cm)', 'FLOW(cm)']], \n                           figsize=(12,12));\n\n\n\n\n\n\n\n\n\n\n\nWe can see that a few of the variables have several 0 values - cement, slag, slump and flow. Water appears to have some relation with slump and with flow.\nThe scatterplots allow a visual understanding of the patterns, but it is usually also good to compute the correlation of all pairs of variables.\n\nR codePython code\n\n\n\nlibrary(psych)\ncorPlot(cor(concrete[, col_to_use]), cex=0.8, show.legend = FALSE)\n\n\n\n\n\n\n\n\n\n\n\ncorr = concrete[['Cement', 'Slag', 'Comp_Strength', 'Water', \n                 'SLUMP(cm)', 'FLOW(cm)']].corr()\ncorr.style.background_gradient(cmap='coolwarm_r')\n\n\n\n\n\n\n \nCement\nSlag\nComp_Strength\nWater\nSLUMP(cm)\nFLOW(cm)\n\n\n\n\nCement\n1.000000\n-0.243553\n0.445725\n0.221091\n0.145913\n0.186461\n\n\nSlag\n-0.243553\n1.000000\n-0.331588\n-0.026775\n-0.284037\n-0.327231\n\n\nComp_Strength\n0.445725\n-0.331588\n1.000000\n-0.254235\n-0.223358\n-0.124029\n\n\nWater\n0.221091\n-0.026775\n-0.254235\n1.000000\n0.466568\n0.632026\n\n\nSLUMP(cm)\n0.145913\n-0.284037\n-0.223358\n0.466568\n1.000000\n0.906135\n\n\nFLOW(cm)\n0.186461\n-0.327231\n-0.124029\n0.632026\n0.906135\n1.000000\n\n\n\n\n\n\n\n\nThe plots you see above are known as heatmaps. They enable us to pick out groups of variables that are similar to one another. As you can see from the blue block in the lower right corner, Water, SLUMP.cm and FLOW.cm are very similar to one another.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring Quantitative Data</span>"
    ]
  },
  {
    "objectID": "03-summarising_data.html#references",
    "href": "03-summarising_data.html#references",
    "title": "3  Exploring Quantitative Data",
    "section": "3.6 References",
    "text": "3.6 References\n\nWebsite References\n\nUCI Machine Learning Repository\nStudent Performance Dataset.\nKernel density estimation\n\n\n\n\n\nCortez, Paulo, and Alice Maria Gonçalves Silva. 2008. “Using Data Mining to Predict Secondary School Student Performance.”\n\n\nYeh, I-Cheng. 2007. “Modeling Slump Flow of Concrete Using Second-Order Regressions and Artificial Neural Networks.” Cement and Concrete Composites 29 (6): 474–80.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring Quantitative Data</span>"
    ]
  },
  {
    "objectID": "04-categorical_data_analysis.html",
    "href": "04-categorical_data_analysis.html",
    "title": "4  Exploring Categorical Data",
    "section": "",
    "text": "4.1 Introduction\nA variable is known as a categorical variable if each observation belongs to one of a set of categories. Examples of categorical variables are: gender, religion, race and type of residence.\nCategorical variables are typically modeled using discrete random variables, which are strictly defined in terms whether or not the support is countable. The alternative to categorical variables are quantitative variables, which are typically modeled using continuous random variables.\nAnother method for distinguishing between quantitative and categorical variables is to ask if there is a meaningful distance between any two points in the data. If such a distance is meaningful then we have quantitative data. For instance, it makes sense to compute the difference in systolic blood pressure between subjects but it does not make sense to consider the mathematical operation (“smoker” - “non-smoker”).\nIt is important to identify which type of data we have (quantitative or categorical), since it affects the exploration techniques that we can apply.\nThere are two sub-types of categorical variables:\nIn this topic, we shall discuss techniques for identifying the presence, and for measuring the strength, of the association between two categorical variables. We shall also demonstrate common visualisations used with categorical data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploring Categorical Data</span>"
    ]
  },
  {
    "objectID": "04-categorical_data_analysis.html#sec-cda",
    "href": "04-categorical_data_analysis.html#sec-cda",
    "title": "4  Exploring Categorical Data",
    "section": "",
    "text": "A categorical variable is ordinal if the observations can be ordered, but do not have specific quantitative values.\nA categorical variable is nominal if the observations can be classified into categories, but the categories have no specific ordering.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploring Categorical Data</span>"
    ]
  },
  {
    "objectID": "04-categorical_data_analysis.html#contingency-tables",
    "href": "04-categorical_data_analysis.html#contingency-tables",
    "title": "4  Exploring Categorical Data",
    "section": "4.2 Contingency Tables",
    "text": "4.2 Contingency Tables\n\nExample 4.1 (Chest Pain and Gender) \nSuppose that 1073 NUH patients who were at high risk for cardiovascular disease (CVD) were randomly sampled. They were then queried on two things:\n\nHad they experienced the onset of severe chest pain in the preceding 6 months? (yes/no)\nWhat was their gender? (male/female)\n\nThe data would probably have been recorded in the following format (only first few rows shown):\n\n\n\n\n\nPatient\nGender\nPain\n\n\n\n\n1\nfemale\nno pain\n\n\n2\nmale\nno pain\n\n\n3\nfemale\nno pain\n\n\n4\nmale\nno pain\n\n\n5\nfemale\nno pain\n\n\n6\nmale\npain\n\n\n\n\n\nHowever, it would probably be summarised and presented in this format, which is known as a contingency table.\n\n\n\n\n\n\npain\nno pain\n\n\n\n\nmale\n46\n474\n\n\nfemale\n37\n516\n\n\n\n\n\nIn a contingency table, each observation from the dataset falls in exactly one of the cells. The sum of all entries in the cells equals the number of independent observations in the dataset.\n\nAll the techniques we shall touch upon in this chapter are applicable to contingency tables.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploring Categorical Data</span>"
    ]
  },
  {
    "objectID": "04-categorical_data_analysis.html#visualisations",
    "href": "04-categorical_data_analysis.html#visualisations",
    "title": "4  Exploring Categorical Data",
    "section": "4.3 Visualisations",
    "text": "4.3 Visualisations\n\nBar charts\nA common method for visualisation cross combinations of categorical data is to use a bar chart.\n\nExample 4.2 (Political Association and Gender Barchart) \nConsider data given in the table below where both variables are nominal. It cross-classifies poll respondents according to their gender and their political party affiliation.\n\n\n\n\n\n\nDem\nInd\nRep\n\n\n\n\nfemale\n762\n327\n468\n\n\nmale\n484\n239\n477\n\n\n\n\n\nHere is R code to make a barchart for this data.\n\nlibrary(lattice)\nx &lt;- matrix(c(762,327,468,484,239,477), ncol=3, byrow=TRUE)\ndimnames(x) &lt;- list(c(\"female\", \"male\"), \n                    c(\"Dem\", \"Ind\", \"Rep\"))\npolitical_tab &lt;- as.table(x)\nbarchart(political_tab/rowSums(political_tab), \n         horizontal = FALSE, auto.key=TRUE)\n\n\n\n\n\n\n\n\nWe can see that the proportion of Republicans is higher for males than for females. The corresponding proportion of Democrats is lower. However, note that the barchart does not reflect that the marginal count for males was much less than that for females.\n\nLet us turn to making bar charts with pandas and Python.\n\nExample 4.3 (Claritin and Nervousness Barchart) \nClaritin is a drug for treating allergies. However, it has a side effect of inducing nervousness in patients. From a sample of 450 subjects, 188 of them were randomly assigned to take Claritin, and the remaining were assigned to take the placebo. The observed data was as follows:\n\n\n\n\n\n\nnervous\nnot nervous\n\n\n\n\nclaritin\n4\n184\n\n\nplacebo\n2\n260\n\n\n\n\n\nA barchart can be created in Python with the following code. This barchart is slightly different from the one in Example 4.2 because it is not stacked.\n\nimport numpy as np\nimport pandas as pd\n\nclaritin_tab = np.array([[4, 184], [2, 260]])\nclaritin_prop = claritin_tab/claritin_tab.sum(axis=1).reshape((2,1))\n\nxx = pd.DataFrame(claritin_prop, \n                  columns=['nervous', 'not_nervous'], \n                  index=['claritin', 'placebo'])\n\nax = xx.plot(kind='bar', stacked=False, rot=1.0, figsize=(10,4))\nax.legend(loc='upper left');\n\n\n\n\n\n\n\n\nThe proportion of patients who are not nervous is similar in both groups.\n\n\n\nMosaic plots\nUnlike a bar chart, a mosaic plot reflects the count in each cell (through the area), along with the proportions of interest. Let us inspect how we can make mosaic plots for the political association data earlier.\n\nExample 4.4 (Political Association and Gender Mosaic Plot) \nThe colours in the output for the R code reflect the sizes of the standardised residuals. We shall discuss this in more detail in Section 4.4.3.\n\nR codePython code\n\n\n\nmosaicplot(political_tab, shade=TRUE)\n\n\n\n\n\n\n\n\n\n\n\nfrom statsmodels.graphics.mosaicplot import mosaic\nimport matplotlib.pyplot as plt\n\npolitical_tab = np.asarray([[762,327,468], [484,239,477]])\nmosaic(political_tab, statistic=True, gap=0.05);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConditional Density Plots\nWhen we have one categorical and one quantitative variable, the kind of plot we make really depends on which is the response, and which is the explanatory variable. If the response variable is the quantitative one, it makes sense to create boxplots or histograms. However, if the response variable is a the categorical one, we should really be making something along these lines:\n\nExample 4.5 \nThe dataset at UCI repository contains records on 299 patients who had heart failure. The data was collected during the follow-up period; each patient had 13 clinical features recorded. The primary variable of interest was whether they died or not. Suppose we wished to plot how this varied with age (a quantitative variable):\n\ndata_path &lt;- file.path(\"data\", \"heart+failure+clinical+records\", \n                       \"heart_failure_clinical_records_dataset.csv\")\nheart_failure &lt;- read.csv(data_path)\nspineplot(as.factor(DEATH_EVENT) ~ age, data=heart_failure)\n\n\n\n\n\n\n\n\nIt reflects how the probability of an event varies with the quantitative explanatory variable. A smoothed version of this is known as the conditional density plot:\n\ncdplot(as.factor(DEATH_EVENT) ~ age, data=heart_failure)\n\n\n\n\n\n\n\n\nFrom either plot, it is clear to see that there is an increased proportion of death during follow-up associated with older patients.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploring Categorical Data</span>"
    ]
  },
  {
    "objectID": "04-categorical_data_analysis.html#tests-for-independence",
    "href": "04-categorical_data_analysis.html#tests-for-independence",
    "title": "4  Exploring Categorical Data",
    "section": "4.4 Tests for Independence",
    "text": "4.4 Tests for Independence\nIn the contingency table above, the two categorical variables are Gender and Presence/Absence of Pain. With contingency tables, the main inferential task usually relates to assessing the association between the two categorical variables.\n\n\n\n\n\n\nIndependent Categorical Variables\n\n\n\nIf two categorical variables are independent, then the joint distribution of the variables would be equal to the product of the marginals. If two variables are not independent, we say that they are associated.\n\n\nIn the remainder of this section, we are going to discuss and apply statistical hypothesis tests. Refer to Section 7.2 for a quick recap about hypothesis testing.\n\n\\(\\chi^2\\)-Test for Independence\nThe \\(\\chi^2\\)-test uses the definition above to assess if two variables in a contingency table are associated. The null and alternative hypotheses are\n\\[\\begin{eqnarray*}\nH_0 &:& \\text{The two variables are indepdendent.}  \\\\\nH_1 &:& \\text{The two variables are not indepdendent.}\n\\end{eqnarray*}\\]\nUnder the null hypothesis, we can estimate the joint distribution from the observed marginal counts. Based on this estimated joint distribution, we then compute expected counts (which may not be integers) for each cell. The test statistic essentially compares the deviation of observed cell counts from the expected cell counts.\n\nExample 4.6 (Chest Pain and Gender Expected Counts) \nContinuing from Example 4.1, we can compute the estimated marginals using row and column proportions\n\n\n------------------------------------------------------------------------------ \nchest_tab (table)\n\nSummary: \nn: 1'073, rows: 2, columns: 2\n\nPearson's Chi-squared test (cont. adj):\n  X-squared = 1.4555, df = 1, p-value = 0.2276\nFisher's exact test p-value = 0.2089\n\n                                     \n                 pain   no pain   Sum\n                                     \nmale     freq      46       474   520\n         p.row   8.8%     91.2%     .\n         p.col  55.4%     47.9% 48.5%\n                                     \nfemale   freq      37       516   553\n         p.row   6.7%     93.3%     .\n         p.col  44.6%     52.1% 51.5%\n                                     \nSum      freq      83       990 1'073\n         p.row   7.7%     92.3%     .\n         p.col      .         .     .\n                                     \n\n----------\n' 95% conf. level\n\n\nIn the output above, ignore the \\(p\\)-values for now.. We’ll get to those in a minute. Let \\(X\\) represent gender and \\(Y\\) represent chest pain. Then from the estimated proportions in the “Sum” row and colums, we can read off the following estimated probabilities\n\\[\\begin{eqnarray*}\n\\widehat{P}(X = \\text{male}) &=& 0.485  \\\\\n\\widehat{P}(Y = \\text{pain}) &=& 0.077\n\\end{eqnarray*}\\]\nConsequently, under \\(H_0\\), we would estimate \\[\n\\widehat{P}(X = \\text{male},\\, Y= \\text{pain}) = 0.485 \\times 0.077 \\approx 0.04\n\\]\nFrom a sample of size 1073, the expected count for this cell is then\n\\[\n0.04 \\times 1073 = 42.92\n\\]\n\nUsing the approach above, we can derive a general formula for the expected count in each cell: \\[\n\\text{Expected count} = \\frac{\\text{Row total} \\times \\text{Column total}}{\\text{Total sample size}}\n\\]\nThe formula for the \\(\\chi^2\\)-test statistic (with continuity correction) is: \\[\n\\chi^2 = \\sum \\frac{(|\\text{expected} - \\text{observed} | - 0.50 )^2}{\\text{expected count}}\n\\tag{4.1}\\]\nThe sum is taken over every cell in the table. Hence in a \\(2\\times2\\) table, as in Example 4.1, there would be 4 terms in the summation.\n\nExample 4.7 (Chest Pain and Gender \\(\\chi^2\\) Test) \nLet us see how we can apply and interpret the \\(\\chi^2\\)-test for the data in Example 4.1.\n\nR codePython code\n\n\n\nx &lt;- matrix(c(46, 37, 474, 516), nrow=2)\ndimnames(x) &lt;- list(c(\"male\", \"female\"), c(\"pain\", \"no pain\"))\nchest_tab &lt;- as.table(x)\n\nchisq_output &lt;- chisq.test(chest_tab)\nchisq_output\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  chest_tab\nX-squared = 1.4555, df = 1, p-value = 0.2276\n\n\n\n\n\nfrom scipy import stats\n\nchest_array = np.array([[46, 474], [37, 516]])\n\nchisq_output = stats.chi2_contingency(chest_array)\n\nprint(f\"The p-value is {chisq_output.pvalue:.3f}.\")\n## The p-value is 0.228.\nprint(f\"The test-statistic value is {chisq_output.statistic:.3f}.\")\n## The test-statistic value is 1.456.\n\n\n\n\nSince the \\(p\\)-value is 0.2276, we would not reject the null hypothesis at significance level 5%. We do not have sufficient evidence to conclude that the variables are not independent.\nTo extract the expected cell counts, we can use the following code:\n\nR codePython code\n\n\n\nchisq_output$expected\n\n           pain  no pain\nmale   40.22367 479.7763\nfemale 42.77633 510.2237\n\n\n\n\n\nchisq_output.expected_freq\n\narray([[ 40.22367195, 479.77632805],\n       [ 42.77632805, 510.22367195]])\n\n\n\n\n\nThe test statistic compares the above table to the observed table, earlier in Example 4.1.\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is only suitable to use the \\(\\chi^2\\)-test when all expected cell counts are larger than 5.\n\n\n\n\nFisher’s Exact Test\nWhen the condition above is not met, we turn to Fisher’s Exact Test. The null and alternative hypothesis are the same, but the test statistic is not derived in the same way.\nIf the marginal totals are fixed, and the two variables are independent, it can be shown that the individual cell counts arise from the hypergeometric distribution. The hypergeometric distribution is defined as follows.\nSuppose we have an urn with \\(m\\) black balls and \\(n\\) red balls. From this urn, we draw a random sample (without replacement) of size \\(k\\). If we let \\(W\\) be the number of red balls drawn, then \\(W\\) follows a hypergeometric distribution. The pmf is:\n\\[\nP(W=w) = \\frac{\\binom{n}{w} \\binom{m}{k-w}}{\\binom{n+m}{k}}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWhat is the support of \\(W\\)?\n\n\nTo transfer this to the context of \\(2 \\times 2\\) tables, suppose we have fix the marginal counts of the table, and consider the count in the top-left corner to be a random variable following a hypergeometric distribution, with \\(r_{1\\cdot}\\) black balls and \\(r_{2 \\cdot}\\) red balls. Consider drawing a sample of size \\(c_{1 \\cdot}\\) from these \\(r_{1\\cdot} + r_{2 \\cdot}\\) balls.\n\n\n\n\n\n\n\n\n\\(W\\)\n-\n\\(r_{1 \\cdot}\\) (black balls)\n\n\n-\n-\n\\(r_{2\\cdot}\\) (red balls)\n\n\n\\(c_{1 \\cdot}\\) (sample size drawn)\n\\(c_{2\\cdot}\\)\n\n\n\n\nNote that, assuming the marginal counts are fixed, knowledge of one of the four entries in the table is sufficient to compute all the counts in the table.\nThe test statistic is the observed count, \\(w\\), in this cell. The \\(p\\)-value is calculated as\n\\[\nP(W \\le w)\n\\] In practice, instead of summing over all values, the \\(p\\)-value is obtained by simulating tables with the same marginals as the observed dataset, and estimating the above probability.\nUsing Fisher’s test sidesteps the need for a large sample size (which is required for the \\(\\chi^2\\) approximation to hold); hence the “Exact” in the name of the test.\n\nExample 4.8 (Claritin and Nervousness) \nHere is the code to perform the Fisher Exact Test for the Claritin data.\n\nR codePython code\n\n\n\ny &lt;-  matrix(c(4, 2, 184, 260), nrow=2)\ndimnames(y) &lt;- list(c(\"claritin\", \"placebo\"), c(\"nervous\", \"not nervous\"))\nclaritin_tab &lt;- as.table(y)\nfisher.test(claritin_tab)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  claritin_tab\np-value = 0.2412\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.399349 31.473382\nsample estimates:\nodds ratio \n  2.819568 \n\n\n\n\n\nfe_output = stats.fisher_exact(claritin_tab)\n\nprint(f\"The p-value for the test is {fe_output.pvalue:.4f}.\")\n\nThe p-value for the test is 0.2412.\n\n\n\n\n\nAs the \\(p\\)-value is 0.2412, we again do not have sufficient evidence to reject the null hypothesis and conclude that there is a significant association.\nBy the way, we can check (in R) to see that the \\(\\chi^2\\)-test would not have been appropriate:\n\nchisq.test(claritin_tab)$expected\n\nWarning in chisq.test(claritin_tab): Chi-squared approximation may be incorrect\n\n\n          nervous not nervous\nclaritin 2.506667    185.4933\nplacebo  3.493333    258.5067\n\n\n\n\n\n\\(\\chi^2\\)-Test for \\(r \\times c\\) Tables\nSo far, we have considered the situation of two categorical variables where each one has only two outcomes (2x2 table). However, it is common that we want to check the association between two nominal variables where one of them or both have more than 2 outcomes. Consider data given in the table below where both variables are nominal.\n\nExample 4.9 (Political Association and Gender) \nLet us return to the political association data, which we plotted in Example 4.2. The R code for applying the test is identical to before\n\nchisq.test(political_tab)\n\n\n    Pearson's Chi-squared test\n\ndata:  political_tab\nX-squared = 30.07, df = 2, p-value = 2.954e-07\n\n\nIn this case, there is strong evidence to reject \\(H_0\\). At 5% level, we would reject the null hypothesis and conclude there is an association between gender and political affiliation.\n\nIn general, we might have \\(r\\) rows and \\(c\\) columns. The null and alternative hypotheses are identical to the 2x2 case, and the test statistic is computed in the same way. However, under the null hypothesis, the test statistic follows a \\(\\chi^2\\) distribution with \\((r-1)(c-1)\\) degrees of freedom.\nThe \\(\\chi^2\\)-test is based on a model of independence - the expected counts are derived under this assumption. As such, it is possible to derive residuals and study them, to see where the data deviates from this model.\nWe define the standardised residuals to be\n\\[\nr_{ij} = \\frac{n_{ij} - \\mu_{ij}}{\\sqrt{\\mu_{ij} (1 - p_{i+})(1 -p_{+j} )}}\n\\] where\n\n\\(n_{ij}\\) is the observed cell count in row \\(i\\) and column \\(j\\) (cell \\(ij\\)).\n\\(\\mu_{ij}\\) is the expected cell count in row \\(i\\) and column \\(j\\)\n\\(p_{i+}\\) is the marginal probability of row \\(i\\)\n\\(p_{+j}\\) is the marginal probability of column \\(j\\).\n\nThe residuals can be obtained from the test output. Under \\(H_0\\), the residuals should be close to a standard Normal distribution. If the residual for a particular cell is very large (or small), we suspect that lack of fit (to the independence model) arises from that cell.\nFor the political association table, the standardised residuals (from R) are:\n\nchisq.test(political_tab)$stdres\n\n              Dem        Ind        Rep\nfemale  4.5020535  0.6994517 -5.3159455\nmale   -4.5020535 -0.6994517  5.3159455",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploring Categorical Data</span>"
    ]
  },
  {
    "objectID": "04-categorical_data_analysis.html#measures-of-association",
    "href": "04-categorical_data_analysis.html#measures-of-association",
    "title": "4  Exploring Categorical Data",
    "section": "4.5 Measures of Association",
    "text": "4.5 Measures of Association\nThis sections covers bivariate measures of association for contingency tables.\n\nOdds Ratio\nThe most generally applicable measure of association, for 2x2 tables, is the Odds Ratio (OR). Suppose we have \\(X\\) and \\(Y\\) to be Bernoulli random variables with (population) success probabilities \\(p_1\\) and \\(p_2\\).\nWe define the odds of success for \\(X\\) to be \\[\n\\frac{p_1}{1-p_1}\n\\] Similarly, the odds of success for random variable \\(Y\\) is \\(\\frac{p_2}{1-p_2}\\).\nIn order to measure the strength of their association, we use the odds ratio: \\[\n\\frac{p_1/ (1-p_1)}{p_2/(1-p_2)}\n\\]\nThe odds ratio can take on any value from 0 to \\(\\infty\\).\n\nA value of 1 indicates no association between \\(X\\) and \\(Y\\). If \\(X\\) and \\(Y\\) were independent, this is what we would observe.\nDeviations from 1 indicate stronger association between the variables.\nNote that deviations from 1 are not symmetric. For a given pair of variables, an association of 0.25 or 4 is the same - it is just a matter of which variable we put in the numerator odds.\n\nDue to the above asymmetry, we often use the log-odds-ratio instead: \\[\n\\log \\frac{p_1/ (1-p_1)}{p_2/(1-p_2)}\n\\]\n\nLog-odds-ratios can take values from \\(-\\infty\\) to \\(\\infty\\).\nA value of 0 indicates no association between \\(X\\) and \\(Y\\).\nDeviations from 0 indicate stronger association between the variables, and deviations are now symmetric; a log-odds-ratio of -0.2 indicates the same strength as 0.2, just the opposite direction.\n\nTo obtain a confidence interval for the odds-ratio, we work with the log-odds ratio and then exponentiate the resulting interval. Here are the steps for a \\(2\\times 2\\):\n\nThe sample data in a 2x2 table can be labelled as \\(n_{11}, n_{12}, n_{21}, n_{22}\\).\nThe sample odds ratio is \\[\n\\widehat{OR} = \\frac{n_{11} \\times n_{22}}{n_{12} \\times n_{21}}\n\\]\nFor a large sample size, it can be shown that \\(\\log \\widehat{OR}\\) follows a Normal distribution. Hence a 95% confidence interval can be obtained through \\[\n\\log \\frac{n_{11} \\times n_{22}}{n_{12} \\times n_{21}} \\pm z_{0.025}\n\\times ASE(\\log \\widehat{OR})\n\\]\n\nwhere\n\nthe ASE (Asymptotic Standard Error) of the estimator is \\[\n\\sqrt{\\frac{1}{n_{11}} + \\frac{1}{n_{12}} + \\frac{1}{n_{21}} + \\frac{1}{n_{22}}}\n\\]\n\n\nExample 4.10 (Chest Pain and Gender Odds Ratio) \nLet us compute the confidence interval for the odds ratio in the chest pain and gender example from earlier.\n\nR codePython code\n\n\n\nlibrary(DescTools)\nOddsRatio(chest_tab,conf.level = .95)\n\nodds ratio     lwr.ci     upr.ci \n 1.3534040  0.8626023  2.1234612 \n\n\n\n\n\nimport statsmodels.api as sm\nchest_tab2 = sm.stats.Table2x2(chest_array)\n\nprint(chest_tab2.summary())\n\n               Estimate   SE   LCB    UCB  p-value\n--------------------------------------------------\nOdds ratio        1.353        0.863 2.123   0.188\nLog odds ratio    0.303 0.230 -0.148 0.753   0.188\nRisk ratio        1.322        0.872 2.004   0.188\nLog risk ratio    0.279 0.212 -0.137 0.695   0.188\n--------------------------------------------------\n\n\n\n\n\n\n\n\nFor Ordinal Variables\nWhen both variables are ordinal, it is often useful to compute the strength (or lack) of any monotone trend association. It allows us to assess if\n\nAs the level of \\(X\\) increases, responses on \\(Y\\) tend to increase toward higher levels, or responses on \\(Y\\) tend to decrease towards lower levels.\n\nFor instance, perhaps job satisfaction tends to increase as income does. In this section, we shall discuss a measure for ordinal variables, analogous to Pearson’s correlation for quantitative variables, that describes the degree to which the relationship is monotone. It is based on the idea of a concordant or discordant pair of subjects.\n\nDefinition 4.1  \n\nA pair of subjects is concordant if the subject ranked higher on \\(X\\) also ranks higher on \\(Y\\).\nA pair is discordant if the subject ranking higher on \\(X\\) ranks lower on \\(Y\\).\nA pair is tied if the subjects have the same classification on \\(X\\) and/or \\(Y\\).\n\n\nIf we let\n\n\\(C\\): number of concordant pairs in a dataset, and\n\\(D\\): number of discordant pairs in a dataset.\n\nThen if \\(C\\) is much larger than \\(D\\), we would have reason to believe that there is a strong positive association between the two variables. Here are two measures of association based on \\(C\\) and \\(D\\):\n\nGoodman-Kruskal \\(\\gamma\\) is computed as \\[\n\\gamma = \\frac{C - D}{C + D}\n\\]\nKendall \\(\\tau_b\\) is \\[\n\\tau_b = \\frac{C - D}{A}\n\\] where \\(A\\) is a normalising constant that results in a measure that works better with ties, and is less sensitive than \\(\\gamma\\) to the cut-points defining the categories. \\(\\gamma\\) has the advantage that it is more easily interpretable.\n\nFor both measures, values close to 0 indicate a very weak trend, while values close to 1 (or -1) indicate a strong positive (negative) association.\n\nExample 4.11 (Job Satisfaction by Income) \nConsider the following table, obtained from Agresti (2012). The original data come from a nationwide survey conducted in the US in 1996.\n\nR codePython code\n\n\n\nx &lt;- matrix(c(1, 3, 10, 6,\n              2, 3, 10, 7,\n              1, 6, 14, 12,\n              0, 1,  9, 11), ncol=4, byrow=TRUE)\ndimnames(x) &lt;- list(c(\"&lt;15,000\", \"15,000-25,000\", \"25,000-40,000\", \"&gt;40,000\"), \n                    c(\"Very Dissat.\", \"Little Dissat.\", \"Mod. Sat.\", \n                      \"Very Sat.\"))\nus_svy_tab &lt;- as.table(x)\n\noutput &lt;- Desc(x, plotit = FALSE, verbose = 3)\noutput[[1]]$assocs\n\n                       estimate  lwr.ci  upr.ci\nContingency Coeff.       0.2419       -       -\nCramer V                 0.1439  0.0000  0.1693\nKendall Tau-b            0.1524 -0.0083  0.3130\nGoodman Kruskal Gamma    0.2211 -0.0085  0.4507\nStuart Tau-c             0.1395 -0.0082  0.2871\nSomers D C|R             0.1417 -0.0080  0.2915\nSomers D R|C             0.1638 -0.0116  0.3392\nPearson Correlation      0.1772 -0.0241  0.3647\nSpearman Correlation     0.1769 -0.0245  0.3645\nLambda C|R               0.0377  0.0000  0.2000\nLambda R|C               0.0159  0.0000  0.0693\nLambda sym               0.0259  0.0000  0.1056\nUncertainty Coeff. C|R   0.0311 -0.0076  0.0699\nUncertainty Coeff. R|C   0.0258 -0.0069  0.0585\nUncertainty Coeff. sym   0.0282 -0.0072  0.0637\nMutual Information       0.0508       -       -\n\n\n\n\n\nfrom scipy import stats\n\nus_svy_tab = np.array([[1, 3, 10, 6], \n                      [2, 3, 10, 7],\n                      [1, 6, 14, 12],\n                      [0, 1,  9, 11]])\n\ndim1 = us_svy_tab.shape\nx = []; y=[]\nfor i in range(0, dim1[0]):\n    for j in range(0, dim1[1]):\n        for k in range(0, us_svy_tab[i,j]):\n            x.append(i)\n            y.append(j)\n\nkt_output = stats.kendalltau(x, y)\nprint(f\"The estimate of tau-b is {kt_output.statistic:.4f}.\")\n\nThe estimate of tau-b is 0.1524.\n\n\n\n\n\nThe output shows that both \\(\\gamma = 0.22\\) and \\(\\tau_b =0.15\\) are close to significant. The lower confidence limit is close to being positive.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploring Categorical Data</span>"
    ]
  },
  {
    "objectID": "04-categorical_data_analysis.html#further-readings",
    "href": "04-categorical_data_analysis.html#further-readings",
    "title": "4  Exploring Categorical Data",
    "section": "4.6 Further readings",
    "text": "4.6 Further readings\nIn general, I have found that R packages seem to have a lot more measures of association for categorical variables. In Python, the measures are spread out across packages.\nAbove, we have only scratched the surface of what is available. If you are keen, do read up on\n\nSomer’s D (for association between nominal and ordinal)\nMutual Information (for association between all types of pairs of categorical variables)\nPolychoric correlation (for association between two ordinal variables)\n\nAlso, take note of how log odds ratios, \\(\\tau_b\\) and \\(\\gamma\\) work - they range between -1 to 1 (in general), and values close to 0 reflect weak association. Values of \\(a\\) and \\(-a\\) indicate the same strength, but different direction of association. This allows the same intuition that Pearson’s correlation does. When you are presented with new metrics, try to understand them by asking similar questions about them.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploring Categorical Data</span>"
    ]
  },
  {
    "objectID": "04-categorical_data_analysis.html#references",
    "href": "04-categorical_data_analysis.html#references",
    "title": "4  Exploring Categorical Data",
    "section": "4.7 References",
    "text": "4.7 References\n\nWebsite References\n\nDocumentation pages from statsmodels:\n\nMosaic plots\nContingency tables\n\nDocumentation pages from scipy:\n\nFisher test with scipy\n\\(\\chi^2\\)-test\n\nHeart failure data\nMore information on Kendall \\(\\tau_b\\) statistic\nThe \\(\\chi^2\\) test we studied is a test of independence. It is a variant of the \\(\\chi^2\\) goodness-of-fit test, which is used to assess if data come from a particular distribution. It’s just in our case, the presumed distribution is one with independence between the groups. Read more about the goodness-of-fit test here.\n\n\n\n\n\nAgresti, Alan. 2012. Categorical Data Analysis. Vol. 792. John Wiley & Sons.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploring Categorical Data</span>"
    ]
  },
  {
    "objectID": "05-robust_statistics.html",
    "href": "05-robust_statistics.html",
    "title": "5  Robust Statistics",
    "section": "",
    "text": "5.1 Introduction\nIntroductory statistics courses describe and discuss inferential methods based on the assumption that data is Normally distributed. However, we never know the true distribution from which our data has arisen; even from the sample, we may observe that it deviates from Normality in various ways. To begin with, we might observe that the data has heavier tails than a Normal distribution. Second, the data could suggest that the originating distribution is heavily skewed, unlike the symmetric Normal.\nContinuing to use Normal based methods will result in confidence intervals and hypothesis tests that have low power. Instead, statisticians have developed a suite of methods that are robust to the assumption of Normality. These techniques may be sub-optimal when the data is truly Normal, but they quickly outperform the Normal-based method as soon as the distribution starts to deviate from Normality.\nA third way in which the Normal-based method could breakdown is when our dataset has extreme values, referred to as outliers. In such cases, many investigators will drop the anomalous points and proceed with the analysis on the remaining observations. This is not ideal for the following reasons:\nRobust statistical techniques are those that have high efficiency over a collection of distributions. Efficiency can be measured in terms of variance of a particular estimator, or in terms of power of a statistical test. In this topic, we shall introduce the concept of robustness and estimators of location and scale that have this property. Although we only touch on basic statistics in this topic, take note that robust techniques exist for regression and ANOVA as well. It is a vastly under-used technique.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Robust Statistics</span>"
    ]
  },
  {
    "objectID": "05-robust_statistics.html#sec-robust",
    "href": "05-robust_statistics.html#sec-robust",
    "title": "5  Robust Statistics",
    "section": "",
    "text": "The sharp decision to reject an observation is wasteful. We can do better by down-weighting the dubious observations.\nIt can be difficult to spot or detect outliers in multivariate data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Robust Statistics</span>"
    ]
  },
  {
    "objectID": "05-robust_statistics.html#notation",
    "href": "05-robust_statistics.html#notation",
    "title": "5  Robust Statistics",
    "section": "5.2 Notation",
    "text": "5.2 Notation\nFor the rest of this topic, let us settle on some notation. Suppose we have an i.i.d sample \\(X_i\\) from a continuous pdf \\(f\\), where \\(i=1,ldots,n\\).\n\nWe use \\(q_{f,p}\\) to refer to the \\(p\\)-th quantile of \\(f\\), i.e.  \\[\nP( X \\le q_{f,p}) = p\n\\]\nFor standard Normal quantiles, we use \\(z_p\\). \\[\n\\Phi(z_p) = P( Z \\le z_p) = p\n\\]\nWe denote the order statistics from the sample with \\(X_{(i)}\\). In other words, \\[\nX_{(1)} \\le X_{(2)} \\le \\cdots \\le X_{(n)}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Robust Statistics</span>"
    ]
  },
  {
    "objectID": "05-robust_statistics.html#datasets",
    "href": "05-robust_statistics.html#datasets",
    "title": "5  Robust Statistics",
    "section": "5.3 Datasets",
    "text": "5.3 Datasets\nFor this topic, we shall use a couple of datasets that are clearly not Normal.\n\nExample 5.1 (Copper in Wholemeal Flour) \nThe dataset chem comes from the package MASS. The data was recorded as part of an analytical chemistry experiment – the amount of copper (\\(\\mu g^{-1}\\)) in wholemeal flour was measured for 24 samples.\n\nlibrary(MASS)\nhist(chem, breaks = 20)\n\n\n\n\nCopper measurements dataset\n\n\n\nsort(chem)\n\n [1]  2.20  2.20  2.40  2.40  2.50  2.70  2.80  2.90  3.03  3.03  3.10  3.37\n[13]  3.40  3.40  3.40  3.50  3.60  3.70  3.70  3.70  3.70  3.77  5.28 28.95\n\nmean(chem)\n\n[1] 4.280417\n\n\nAlthough 22 out of the 24 points are less than 4, the mean is 4.28. This statistic is clearly being affected by the largest two values. Removing them would yield a summary statistic that is more representative of the majority of observations. This topic is about techniques that will work well even in the presence of such large anomalous values.\n\nThe second dataset is also from a textbook:\n\nExample 5.2 (Self-awareness Dataset) \nFor a second dataset, we use one on self-awareness from (Wilcox and R 2011), where participants in a psychometric study were timed how long they could keep a portion of an apparatus in contact with a specified target.\n\nawareness &lt;- c(77, 87, 88, 114, 151, 210, 219, 246, 253, 262, 296, 299, 306,\n               376, 428, 515, 666, 1310, 2611)\nhist(awareness, breaks=10)\n\n\n\n\nSelf-awareness study timing\n\n\n\nmean(awareness)\n\n[1] 448.1053\n\n\nJust like the data in Example 5.1, this data too is highly skewed to the right. The mean of the full dataset is larger than the 3rd quartile!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Robust Statistics</span>"
    ]
  },
  {
    "objectID": "05-robust_statistics.html#sec-arob",
    "href": "05-robust_statistics.html#sec-arob",
    "title": "5  Robust Statistics",
    "section": "5.4 Assessing Robustness",
    "text": "5.4 Assessing Robustness\nThe focus of this course is on the computational aspects of performing data analyses. However, this section and the next are a little theoretical. They only exist to provide a better overview of robust statistical techniques.\nSection 5.4.1 introduces an approach for comparing two estimators in general. The remaining subsections (Section 5.5 onwards) briefly list ways in which robust statistics are evaluated.\n\nAsymptotic Relative Efficiency\nSuppose we wish to estimate a parameter \\(\\theta\\) of a distribution using a sample of size \\(n\\). We have two candidate estimators \\(\\hat{\\theta}\\) and \\(\\tilde{\\theta}\\).\n\nDefinition 5.1 (Asymptotic Relative Efficiency (ARE)) The asymptotic relative efficiency of \\(\\tilde{\\theta}\\) relative to \\(\\hat{\\theta}\\) is\n\\[\nARE(\\tilde{\\theta}; \\hat{\\theta}) = \\lim_{n \\rightarrow \\infty}\n\\frac{\\text{variance of } \\hat{\\theta}}{\\text{variance of } \\tilde{\\theta}}\n\\]\nUsually, \\(\\hat{\\theta}\\) is the optimal estimator according to some criteria. The intuitive interpretation is that when using \\(\\hat{\\theta}\\), we only need \\(ARE\\) times as many observations as when using \\(\\tilde{\\theta}\\). Smaller values of ARE indicate that \\(\\hat{\\theta}\\) is better than \\(\\tilde{\\theta}\\).\n\nHere are a couple of commonly used estimators and their AREs.\n\nExample 5.3 (Median versus Mean) If our data is known to originate from a Normal distribution, due to its symmetry, we can use either the sample median or the sample mean to estimate \\(\\mu\\). Let \\(\\hat{\\theta} = \\bar{X}\\) and \\(\\tilde{\\theta} = X_{(1/2)}\\).\nThen it can be shown that\n\\[\nARE(\\tilde{\\theta}; \\hat{\\theta}) = 2/\\pi \\approx 64\\%\n\\]\nThe sample median is less efficient than the sample mean, when the true distribution is Normal.\n\nHere is an example of when robust statistics prove to be superior to non-robust ones.\n\nExample 5.4 (Contaminated Normal Variance Estimate) Suppose first that we have \\(n\\) observations \\(Y_i \\sim N(\\mu, \\sigma^2)\\), and we wish to estimate \\(\\sigma^2\\). Consider the two estimators:\n\n\\(\\hat{\\sigma}^2 = s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (Y_i - \\bar{Y})^2\\)\n\\(\\tilde{\\sigma}^2 = d^2 \\pi/2\\), where \\[\nd = \\frac{1}{n} \\sum_i {|Y_i - \\bar{Y}|}\n\\]\n\nIn this case, when the underlying distribution truly is Normal, we have that \\[\nARE(\\tilde{\\sigma}^2; \\hat{\\sigma}^2) =  87.6\\%\n\\]\nHowever, now consider a situation where \\(Y_i \\sim N(\\mu, \\sigma^2)\\) with probability \\(1 - \\epsilon\\) and \\(Y_i \\sim N(\\mu, 9\\sigma^2)\\) with probability \\(\\epsilon\\). Let us refer to this as a contaminated Normal distribution.\n\n\n\n\n\n\n\n\nFigure 5.1: Contaminated Normal\n\n\n\n\n\nAs you can see from Figure 5.1, the two pdfs are almost indistinguishable by eye. However, the ARE values are very different:\n\n\n\n\\(\\epsilon\\)\nARE\n\n\n\n\n0\n87.6%\n\n\n0.01\n144%\n\n\n\nThe usual \\(s^2\\) loses optimality very quickly; we can obtain more precise estimates using \\(\\tilde{\\sigma}^2\\). This example was taken from section 5.5 of Venables and Ripley (2013).\n\nThere are three main approaches of assessing the robustness of an estimator. Let us cover the intuitive ideas here. In this section, \\(F\\) refers to the cdf from which the sample was obtained.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Robust Statistics</span>"
    ]
  },
  {
    "objectID": "05-robust_statistics.html#sec-req-rob",
    "href": "05-robust_statistics.html#sec-req-rob",
    "title": "5  Robust Statistics",
    "section": "5.5 Requirements of Robust Summaries",
    "text": "5.5 Requirements of Robust Summaries\n\nQualitative Robustness: The first requirement of a robust statistic is that it if the underlying distribution \\(F\\) changes slightly, then the estimate should not change too much.\nInfinitesimal Robustness: The second requirement is tied to the concept of the influence function of an estimator. Roughly speaking, the influence function measures the relative extent that a small perturbation in \\(F\\) has on the value of the estimate. In other words, it reflects the influence of adding one more observation to a large sample.\nQuantitative Robustness: The final requirement is related to the contaminated distribution we touched on in Example 5.4. Consider \\[\nF_{x,\\epsilon} = (1- \\epsilon)F + \\epsilon \\Delta_x\n\\] where \\(\\Delta_x\\) is the degenerate probability distribution at \\(x\\). The minimum value of \\(\\epsilon\\) for which the estimator goes to infinity as \\(x\\) gets large, is referred to as the breakdown point. For the sample mean, the breakdown point is \\(\\epsilon = 0\\). For the sample median, the breakdown point is \\(\\epsilon = 0.5\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Robust Statistics</span>"
    ]
  },
  {
    "objectID": "05-robust_statistics.html#measures-of-location",
    "href": "05-robust_statistics.html#measures-of-location",
    "title": "5  Robust Statistics",
    "section": "5.6 Measures of Location",
    "text": "5.6 Measures of Location\nThe location parameter of a distribution is a value that characterises “a typical” observation, or the middle of the distribution. It is not always the mean of the distribution, but in the case of a symmetric distribution it will be.\n\nM-estimators\nBefore we introduce robust estimators for the location, let us revisit the most commonly used one - the sample mean. Suppose we have observed \\(x_1, x_2, \\ldots, x_n\\), a random sample from a \\(N(\\mu,\\, \\sigma^2)\\) distribution. As a reminder, here is how we derive the MLE for \\(\\mu\\).\nThe likelihood function is \\[\nL(\\mu, \\sigma^2) = \\prod_{i=1}^n  \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-(x_i - \\mu)^2 / (2\\sigma^2) }\n\\] The log-likelihood is \\[\n\\log L = l(\\mu, \\sigma^2) = -n \\log \\sigma - \\frac{n}{2} \\log(2\\pi) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\n\\tag{5.1}\\]\nSetting the partial derivative with respect to \\(\\mu\\) to be 0, we can solve for the MLE: \\[\\begin{eqnarray*}\n\\frac{\\partial l }{\\partial \\mu} &=& 0 \\\\\n\\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\hat{\\mu}) &=& 0 \\\\\n\\hat{\\mu} &=& \\bar{x}\n\\end{eqnarray*}\\]\nObserve that in Equation 5.1, we minimised the sum of squared errors, which arose from minimising \\[\n\\sum_{i=1}^n - \\log f (x_i - \\mu)\n\\] where \\(f\\) is the standard normal pdf. Instead of using \\(\\log f\\), Huber proposed using alternative functions (let’s call the function \\(\\rho\\)) to derive estimators (Huber 1992). The new estimator corresponds to \\[\n\\arg \\min_\\mu \\sum_{i=1}^n \\rho (x_i - \\mu)\n\\tag{5.2}\\]\nThere are constraints on the choice of \\(\\rho\\) above, but we can understand the resulting estimator through \\(\\rho\\). For instance, \\(\\psi = \\rho'\\) is referred to as the influence function, which measures the relative change in a statistic as a new observation is added. To find the \\(\\hat{\\mu}\\) that minimised Equation 5.3, it is equivalent to setting the derivative to zero and solving for \\(\\hat{\\mu}\\): \\[\n\\sum_{i=1}^n \\psi (x_i - \\mu) = 0\n\\tag{5.3}\\]\nNote that, in general, the use of the sample mean corresponds to the use of \\(\\rho(x) = x^2\\). In that case, \\(\\psi=2x\\) is unbounded, which results in high importance/weight placed on very large values. Instead, robust estimators should have a bounded \\(\\psi\\) function.\nThe approach outlined above - the use of \\(\\rho\\) and \\(\\psi\\) to define estimators, gave rise to a class of estimators known as M-estimators, since they are MLE-like. In the following sections, we shall introduce estimators corresponding to various choices of \\(\\rho\\). It is not always easy to identify the \\(\\rho\\) being used, but inspection of the form of \\(\\psi\\) leads to an understanding of how much emphasis the estimator places on large outlying values.\n\n\nTrimmed mean\nThe \\(\\gamma\\)-trimmed mean \\((0 &lt; \\gamma \\le 0.5)\\) is the mean of a distribution after the distribution has been truncated at the \\(\\gamma\\) and \\(1-\\gamma\\) quantiles. Note that the truncated function has to be renormalised in order to be a pdf.\nIn formal terms, suppose that \\(X\\) is a continuous random variable with pdf \\(f\\). The usual mean is of course just \\(\\mu = \\int x f(x) dx\\). The trimmed mean of the distribution is\n\\[\n\\mu_t = \\int_{q_{f,\\gamma}}^{q_{f,1-\\gamma}} x \\frac{f(x)}{1 - 2 \\gamma} dx\n\\tag{5.4}\\]\nUsing the trimmed mean focuses on the middle portion of a distribution. The recommended value of \\(\\gamma\\) is (0, 0.2]. For a sample \\(X_1, X_2, \\ldots, X_n\\), the estimate is computed (see Wilcox and R (2011) page 54) using the following algorithm:\n\nCompute the value \\(g = \\lfloor \\gamma n \\rfloor\\), where \\(\\lfloor x \\rfloor\\) refers to the floor function1.\nDrop the largest \\(g\\) and smallest \\(g\\) values from the sample.\nCompute \\[\n\\hat{\\mu_t} = X_t = \\frac{X_{(g+1)} + \\cdots X_{(n-g)}}{n - 2g}\n\\]\n\nIt can be shown that the influence function for the trimmed mean is \\[\n\\psi(x) = \\begin{cases}\nx, & -c &lt; x &lt; c \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\] which indicates that, with this estimator, large outliers have no effect on the estimator.\n\n\nWinsorised Mean\nThe Winsorised mean is similar to the trimmed mean in the sense that it modifies the tail of the distribution. However, it works by replacing extreme observations with fixed moderate values. The corresponding \\(\\psi\\) function is\n\\[\n\\psi(x) = \\begin{cases}\n-c, & x &lt; - c \\\\\nx, & |x| &lt; c \\\\\nc, & x &gt; c\n\\end{cases}\n\\]\nJust like in the trimmed mean case, we decide on the value \\(c\\) by choosing a value \\(\\gamma \\in (0, 0.2]\\). To calculate the Winsorised mean from a sample \\(X_1, X_2, \\ldots, X_n\\), we use the following algorithm:\n\nCompute the value \\(g = \\lfloor \\gamma n \\rfloor\\).\nReplace the smallest \\(g\\) values in the sample with \\(X_{(g+1)}\\) and the largest \\(g\\) values with \\(X_{(n-g)}\\).\nCompute the arithmetic mean of the resulting \\(n\\) values. \\[\nX_w = \\frac{g\\cdot X_{(g+1)} + X_{(g+1)} + \\cdots + X_{(n-g)} + g \\cdot X_{(n-g)}}{n}\n\\]\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that the trimmed mean and the Winsorised mean are no longer estimating the population distribution mean \\(\\int x f(x) dx\\). The three quantities coincide only if the population distribution is symmetric.\nWhen this is not the case, it is important to be aware of what we are estimating. For instance, using the trimmed/winsorised mean is appropriate if we are interested in what a “typical” observation in the middle of the distribution looks like.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Robust Statistics</span>"
    ]
  },
  {
    "objectID": "05-robust_statistics.html#measures-of-scale",
    "href": "05-robust_statistics.html#measures-of-scale",
    "title": "5  Robust Statistics",
    "section": "5.7 Measures of Scale",
    "text": "5.7 Measures of Scale\n\nSample Standard Deviation\nJust as in Section 5.6.1, the MLE of the population variance \\(\\sigma^2\\) is not robust to outliers. It is given by \\[\ns^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\]\nHere are a few robust alternatives to this estimator. However, take note that, just like in the case of location estimators, the following estimators are not estimating the standard deviation. We can modify them so that if the underlying distribution truly is Normal, then they do estimate \\(\\sigma\\). However, if the distribution is not Normal, we should treat them as they are: robust measures of the spread of the distribution.\n\n\nMedian Absolute Deviation\nFor a random variable \\(X \\sim f\\), the median absolute deviation \\(w\\) is defined by \\[\nP(|X - q_{f,0.5} | \\le w) = 0.5\n\\] We sometimes refer to \\(w\\) as \\(MAD(X)\\). In other words, it is the median of the distribution associated with \\(|X - q_{f,0.5}|\\); it is the median of absolute deviations from the median.\nIf observations are truly from a Normal distribution, it can be shown that \\(MAD\\) estimates \\(z_{0.75} \\sigma\\). Hence, in general, MAD is divided by \\(z_{0.75}\\) so that it coincides with \\(\\sigma\\) if the underlying distribution is Normal.\n\nProposition 5.1 (MAD for Normal) For \\(X \\sim N(\\mu, \\sigma^2)\\), the following property holds: \\[\n\\sigma \\approx 1.4826 \\times MAD(X)\n\\]\n\nProof. Note that, since the distribution is symmetric, \\(\\text{median}(X) = \\mu\\). Thus \\[\\begin{eqnarray*}\nMAD(X) &=& \\text{median}(| X - \\text{median(X)}|) \\\\\n&=& \\text{median}(| X - \\mu |)\n\\end{eqnarray*}\\]\nThus, the \\(MAD(X)\\) is a value \\(q\\) such that \\[\nP(| X - \\mu | \\le q ) = 0.5\n\\] Equivalently, we need \\(q\\) such that \\[\nP\\left( \\left| \\frac{X - \\mu}{\\sigma} \\right| \\le q/\\sigma \\right) = P(|Z| \\le q / \\sigma) = 0.5\n\\] Remember that we can retrieve values for the standard Normal cdf easily from R or Python:\n\\[\\begin{eqnarray*}\nP(-q / \\sigma \\le Z \\le q / \\sigma) &=& 0.5 \\\\\n1 - 2 \\times \\Phi(-q / \\sigma) &=& 0.5 \\\\\n-q / \\sigma &=& -0.6745 \\\\\nq &=& 0.6745 \\sigma\n\\end{eqnarray*}\\]\nThus \\(MAD(X) = 0.6745 \\sigma\\). The implication is that we can estimate \\(\\sigma\\) in a standard Normal with \\[\n\\hat{\\sigma} = \\frac{1}{0.6745} MAD(X) \\approx \\frac{1}{0.6745} MAD(X)\n\\]\n\n\n\n\nInterquartile Range\nThe general definition of \\(IQR(X)\\) is \\[\nq_{f, 0.75} - q_{f,0.25}\n\\]\nIt is a linear combination of quantiles. Again, we can modify the IQR so that, if the underlying distribution is Normal, we are estimating the standard deviation \\(\\sigma\\).\n\nProposition 5.2 (IQR for Normal) For \\(X \\sim N(\\mu, \\sigma^2)\\), the following property holds:\n\\[\n\\sigma \\approx \\frac{IQR(X)}{1.35}\n\\]\n\nProof. For \\(X \\sim N(\\mu, \\sigma^2)\\), let \\(q_{0.25}\\) and \\(q_{0.75}\\) represent the 1st and 3rd quartiles of the distribution.\n\\[\\begin{eqnarray*}\nP(X \\le q_{0.25}) &=& 0.25 \\\\\nP \\left(\\frac{X - \\mu}{\\sigma} \\le \\frac{q_{0.25} - \\mu}{\\sigma} \\right) &=& 0.25 \\\\\nP \\left(Z \\le  \\frac{q_{0.25} - \\mu}{\\sigma} \\right) &=& 0.25\n\\end{eqnarray*}\\]\nThus (from R or Python2) we know that\n\\[\\begin{eqnarray*}\n\\frac{q_{0.25} - \\mu}{\\sigma}   = z_{0.25} &=& -0.6745 \\\\\n\\therefore \\; q_{0.25} &=& \\mu - 0.6745 \\sigma\n\\end{eqnarray*}\\]\nSimilarly, we can derive that \\(q_{0.75} = \\mu + 0.6745 \\sigma\\). Now we can derive that \\[\nIQR(X) = q_{0.75} - q_{0.25} \\approx 1.35 \\sigma\n\\]\nThe implication is that, if we have sample data from standard Normal, we can estimate \\(\\sigma\\) from the IQR using: \\[\n\\hat{\\sigma} = \\frac{IQR(\\{X_1, \\ldots X_n\\})}{1.35}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Robust Statistics</span>"
    ]
  },
  {
    "objectID": "05-robust_statistics.html#examples",
    "href": "05-robust_statistics.html#examples",
    "title": "5  Robust Statistics",
    "section": "5.8 Examples",
    "text": "5.8 Examples\n\nExample 5.5 (Location Estimates: Copper Dataset) \nTo begin, let us apply the three estimators of location to the chemical dataset.\n\nR codePython code\n\n\n\nmean(chem)\n## [1] 4.280417\n\nmean(chem, trim = 0.1) # using gamma = 0.1\n## [1] 3.205\n\nlibrary(DescTools)\nvals = quantile(chem, probs=c(0.05, 0.95))\nwin_sample &lt;- Winsorize(chem, vals) # gamma = 0.1\nmean(win_sample)\n## [1] 3.277792\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\nchem = pd.read_csv(\"data/mass_chem.csv\")\n\nchem.chem.mean()\n## np.float64(4.2804166666666665)\n\nstats.trim_mean(chem, proportiontocut=0.1)\n## array([3.205])\n\nstats.mstats.winsorize(chem.chem, limits=0.1).mean()\n## np.float64(3.185)\n\n\n\n\nAs we observe, the robust estimates are less affected by the extreme and isolate value 28.95. They are more indicative of the general set of observations.\n\n\nExample 5.6 (Scale Estimates: Copper Dataset) \nNow we turn the scale estimators for the self-awareness dataset.\n\nR codePython code\n\n\n\nsd(awareness)\n## [1] 594.6295\n\nmad(awareness, constant=1) \n## [1] 114\n\nIQR(awareness)\n## [1] 221.5\n\n\n\n\nawareness = np.array([77, 87, 88, 114, 151, 210, 219, 246, 253, 262, 296, \n                      299, 306, 376, 428, 515, 666, 1310, 2611])\n\nawareness.std()\n## np.float64(578.7698292373723)\n\nstats.median_abs_deviation(awareness)\n## np.float64(114.0)\n\nstats.iqr(awareness)\n## np.float64(221.5)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Robust Statistics</span>"
    ]
  },
  {
    "objectID": "05-robust_statistics.html#summary",
    "href": "05-robust_statistics.html#summary",
    "title": "5  Robust Statistics",
    "section": "5.9 Summary",
    "text": "5.9 Summary\nIn this topic, we have introduced the concept of robust statistics. Understanding how these methods work requires a large amount of theoretical derivations and set-up. However, although we have not gone into much depth in the notes, we shall investigate the value of these methods in our tutorials.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Robust Statistics</span>"
    ]
  },
  {
    "objectID": "05-robust_statistics.html#references",
    "href": "05-robust_statistics.html#references",
    "title": "5  Robust Statistics",
    "section": "5.10 References",
    "text": "5.10 References\n\nWebsite References\n\nWikipedia on Robust Statistics This page contains visualisations of the \\(\\psi\\) curves for the estimators mentioned above, along with others.\nScipy stats: This page contains information on the median_abs_deviation(), trim_mean(), mstats.winsorize() methods in Python.\n\n\n\n\n\nHuber, Peter J. 1992. “Robust Estimation of a Location Parameter.” In Breakthroughs in Statistics: Methodology and Distribution, 492–518. Springer.\n\n\nVenables, William N, and Brian D Ripley. 2013. Modern Applied Statistics with s-PLUS. Springer Science & Business Media.\n\n\nWilcox, R, and R. 2011. Introduction to Robust Estimation and Hypothesis Testing. Academic press.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Robust Statistics</span>"
    ]
  },
  {
    "objectID": "05-robust_statistics.html#footnotes",
    "href": "05-robust_statistics.html#footnotes",
    "title": "5  Robust Statistics",
    "section": "",
    "text": "The largest integer less than or equal to \\(x\\).↩︎\nIn R: qnorm(0.25)↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Robust Statistics</span>"
    ]
  },
  {
    "objectID": "06-intro_to_sas.html",
    "href": "06-intro_to_sas.html",
    "title": "6  Introduction to SAS",
    "section": "",
    "text": "6.1 Introduction\nSAS (Statistical Analysis System) is a software that was originally created in the 1960s. Today, it is widely used by statisticians working in biostatistics and the pharmaceutical industries. Unlike Python and R, it is a proprietary software. The full license is quite expensive for a low-usage case such as ours. Thankfully, there is a free web-based version that we can use for our course.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to SAS</span>"
    ]
  },
  {
    "objectID": "06-intro_to_sas.html#registering-for-a-sas-studio-account",
    "href": "06-intro_to_sas.html#registering-for-a-sas-studio-account",
    "title": "6  Introduction to SAS",
    "section": "6.2 Registering for a SAS Studio Account",
    "text": "6.2 Registering for a SAS Studio Account\nThe first step is to create a SAS profile: Use your NUS email address to register and create your SAS profile using this link.\nOnce you have verified your account using the email that would be sent to you, the following link should take you to the login page shown in Figure 6.1.\n\n\n\n\n\n\nFigure 6.1: SAS Studio Login\n\n\n\nSubsequently logging in should take you to the landing page, where you can begin writing SAS code and using SAS. This interface can be seen in Figure 6.2.\n\n\n\n\n\n\nFigure 6.2: SAS Studio",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to SAS</span>"
    ]
  },
  {
    "objectID": "06-intro_to_sas.html#an-overview-of-sas-language",
    "href": "06-intro_to_sas.html#an-overview-of-sas-language",
    "title": "6  Introduction to SAS",
    "section": "6.3 An Overview of SAS Language",
    "text": "6.3 An Overview of SAS Language\nThe SAS language is not a fully-fledged programming language like Python is, or even R. For the most part, we are going to capitalise on the point-and-click interface of SAS Studio in our course. However, even so, it is good to understand a little about the language so that we can modify the options for different procedures as necessary.\nA SAS program is a sequence of statements executed in order. Keep in mind that:\n\nEvery SAS statement ends with a semicolon.\n\nSAS programs are constructed from two basic building blocks: DATA steps and PROC steps. A typical program starts with a DATA step to create a SAS data set and then passes the data to a PROC step for processing.\n\nExample 6.1 (Creating and Printing a Dataset) Here is a simple program that converts miles to kilometres in a DATA step and then prints the results with a PROC step:\n\nDATA distance;\n    Miles = 26.22;\n    Kilometer = 1.61 * Miles;\n\nPROC PRINT DATA=distance;\nRUN;\n\nTo run the above program, click on the “Running Man” icon in SAS studio. You should obtain the output shown in Figure 6.3.\n\n\n\n\n\n\nFigure 6.3: SAS output\n\n\n\nThis dataset has only one observation (row).\n\nData steps start with the DATA keyword. This is followed by the name for the dataset. Procedures start with PROC followed by the name of the particular procedure (e.g. PRINT, SORT or PLOT) you wish to run on the dataset. Most SAS procedures have only a handful of possible statements. A step ends when SAS encounters a new step (marked by a DATA or PROC statement) or a RUN statement. RUN statements are not part of a DATA or PROC step; they are global statements.\n\nExample 6.2 (Creating a Dataset Inline) The following program explicitly creates a dataset within the DATA step.\n\n/*CREATING DATA MANUALLY:; */\n\nDATA ex_1;\nINPUT subject gender $ CA1 CA2 HW $;\nDATALINES;\n10 m 80 84 a\n7 m 85 89 a\n4 f 90 86 b\n20 m 82 85 b\n25 f 94 94 a\n14 f 88 84 c\n;\n\nPROC MEANS DATA=ex_1;\nVAR CA1 CA2;\nRUN;\n\nThe output for the above code is shown in Figure 6.4 and Figure 6.5.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.4: Dataset output\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.5: Proc output\n\n\n\n\n\n\nIn the statements above, the $’s in the INPUT statement inform SAS that the preceding variables (gender and HW) are character. Note how the semi-colon for the DATALINES appears after all the data has been listed.\nPROC MEANS creates basic summary statistics for the variables listed.\n\nTo review, there are only 2 types of steps in SAS programs:\n\n\n\n\n\n\nDATA steps\n\nbegin with DATA statements.\nread and modify data.\ncreate a SAS dataset.\n\n\n\nPROC steps\n\nbegin with PROC statements.\nperform specific analysis or function.\nproduce reports or results.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to SAS</span>"
    ]
  },
  {
    "objectID": "06-intro_to_sas.html#basic-rules-for-sas-programs",
    "href": "06-intro_to_sas.html#basic-rules-for-sas-programs",
    "title": "6  Introduction to SAS",
    "section": "6.4 Basic Rules for SAS Programs",
    "text": "6.4 Basic Rules for SAS Programs\n\nFor SAS statements\n\nAll SAS statements (except those containing data) must end with a semicolon (;).\nSAS statements typically begin with a SAS keyword. (DATA, PROC).\nSAS statements are not case sensitive, that is, they can be entered in lowercase, uppercase, or a mixture of the two.\n\nExample : SAS keywords (DATA, PROC) are not case sensitive\n\nA delimited comment begins with a forward slash-asterisk (/) and ends with an asterisk-forward slash (/). All text within the delimiters is ignored by SAS.\n\n\n\nFor SAS names\n\nAll names must contain between 1 and 32 characters.\nThe first character appearing in a name must be a letter (A, B, … Z, a, b, …, z) or an underscore ( ). Subsequent characters must be letters, numbers, or underscores. That is, no other characters, such as $, %, or & are permitted.\nBlanks also cannot appear in SAS names.\nSAS names are not case sensitive, that is, they can be entered in lowercase, uppercase, or a mixture of the two. (SAS is only case sensitive within quotation marks.)\n\n\n\nFor SAS variables\n\nIf the variable in the INPUT statement is followed by a dollar sign ($), SAS assumes this is a character variable. Otherwise, the variable is considered as a numeric variable.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to SAS</span>"
    ]
  },
  {
    "objectID": "06-intro_to_sas.html#reading-data-into-sas",
    "href": "06-intro_to_sas.html#reading-data-into-sas",
    "title": "6  Introduction to SAS",
    "section": "6.5 Reading Data into SAS",
    "text": "6.5 Reading Data into SAS\nIn this topic, we shall introduce a new dataset, also from the UCI Machine Learning repository.\n\nExample 6.3 (Bike Rentals) \nThe dataset was collected by the authors in Fanaee-T and Gama (2013). It contains information on bike-sharing rentals in Washington D.C. USA for the years 2011 and 2012, along with measurements of weather. The original dataset contained hourly and daily aggregated data. For our class, we use a re-coded version of the daily data. Our dataset can be found on Canvas as bike2.csv.\nHere is the data dictionary:\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\ninstant\nRecord index\n\n\ndteday\nDate\n\n\nseason\nspring, summer, fall, winter)\n\n\nyr\nYear (0: 2011, 1: 2012)\n\n\nmnth\nAbbreviated month\n\n\nholiday\nWhether the day is a holiday or not\n\n\nweekday\nAbbreviated day of the week\n\n\nworkingday\nyes: If day is neither weekend nor holiday is 1, no: Otherwise\n\n\nweathersit\nWeather situation: clear: Clear, Few clouds, Partly cloudy, Partly cloudy; mist: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist; light_precip: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds; heavy_precip: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\n\ntemp\nNormalized temperature in Celsius. Divided by 41 (max)\n\n\natemp\nNormalized feeling temperature in Celsius. Divided by 50 (max)\n\n\nhum\nNormalized humidity. Divided by 100 (max)\n\n\nwindspeed\nNormalized wind speed. Divided by 67 (max)\n\n\ncasual\nCount of casual users\n\n\nregistered\nCount of registered users\n\n\ncnt\nCount of total rental bikes including both casual and registered\n\n\n\nOur first step will be to load the dataset into SAS Studio.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to SAS</span>"
    ]
  },
  {
    "objectID": "06-intro_to_sas.html#uploading-and-using-datasets",
    "href": "06-intro_to_sas.html#uploading-and-using-datasets",
    "title": "6  Introduction to SAS",
    "section": "6.6 Uploading and Using Datasets",
    "text": "6.6 Uploading and Using Datasets\nTo use our own datasets on SAS Studio, we have to execute the following steps:\n\nCreate a new library. In SAS, a library is a collection of datasets. If you already have a library created, you can simply import datasets into it. The default library on SAS is called WORK. However, the datasets will be purged every time you sign out. Hence it is better to create a new one.\nImport your dataset (csv, xlsx, etc.) into the library.\nAfter this, the data will be available for use with the reference name &lt;library-name&gt;.&lt;dataset-name&gt;.\n\nFrom the “Libraries” menu on the left of SAS studio, click on the “New library” icon (the one circled in red in Figure 6.6), and create a new library called “ST2137”. You can use the default suggested path for the library.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.6: New library\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.7: Upload data\n\n\n\n\n\n\nNow expand the menu for “Server Files and Folders” and upload bike2.csv file to SAS, using the circled icon in Figure 6.7.\nFinally, right-click on the top of the main Studio area (where we write code) and select “New Import Data”. Select the bike2.csv that has just been uploaded, and modify the OUTPUT DATA settings to Library ST2137 and Data set name BIKE2. Click on the running man, and your dataset is now ready for use in SAS studio!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to SAS</span>"
    ]
  },
  {
    "objectID": "06-intro_to_sas.html#summarising-numerical-data",
    "href": "06-intro_to_sas.html#summarising-numerical-data",
    "title": "6  Introduction to SAS",
    "section": "6.7 Summarising Numerical Data",
    "text": "6.7 Summarising Numerical Data\nThe SAS routines we are going to work with can be found in the “Tasks and Utilities” section (see highlighted tasks in Figure 6.8).\n\n\n\n\n\n\nFigure 6.8: Common ST2137 Tasks\n\n\n\n\nNumerical Summaries\n\nExample 6.4 (5-number Summaries) \nWe expect that the total count of users will vary by the seasons. Hence, we begin by computing five-number summaries for each season.\nUnder Tasks, go to Statistcs &gt; Summary Statistics. Select cnt as the analysis variable, and season as the classification variable. Under the options tab, select the lower and upper quartiles, along with comparative boxplots. The output should look like this Figure 6.9:\n\n\n\n\n\n\nFigure 6.9: Summaries, Bike data\n\n\n\nWe observe that the median count is highest for fall, followed by summer, winter and lastly spring. The spreads, as measured by IQR, are similar across the seasons: approximately 2000 users. In the middle 50%, the count distribution for spring is the most right-skewed.\n\n\n\nScatter Plots\n\nExample 6.5 (Casual vs Registered Scatterplot) \nTo create a scatterplot in SAS, go to Tasks &gt; Graphs &gt; Scatter Plot.\nSpecify casual on the x-axis, registered on the y-axis, and workingday as the Group. You should observe the plot created Figure 6.10:\n\n\n\n\n\n\nFigure 6.10: Scatter plot, Bike Data\n\n\n\nWe can see that there seem to be two different relationships between the counts of casual and registered users. The two relationships correspond to whether it as a working day or not.\n\n\n\nHistograms\n\nExample 6.6 (Casual Users Distribution) \nNow suppose we focus on casual users, and study the distribution of counts by whether a day is a working day or not. To create a histogram, go to Tasks &gt; Graph &gt; Histogram. Select casual as the analysis variable, and workingday as the group variable.\n\n\n\n\n\n\nFigure 6.11: Histograms, Bike Data\n\n\n\nFrom Figure 6.11, we can see that the distribution is right-skewed in both cases. However, the range of counts for non-working days extends further, to about 3500.\n\n\n\nBoxplots\n\n\nExample 6.7 (Boxplots for Casual Users, by Season) In Example 6.4, we observed that total counts vary by users, and in Example 6.6, we observed that working days seem to have fewer casual users. Let us investigate if this difference is related to season.\nTo create boxplots, go to Tasks &gt; Box Plot. Select casual as the analysis variable, season as the category and workingday as the subcategory. You should obtain a plot like this Figure 6.12:\n\n\n\n\n\n\nFigure 6.12: Boxplots, Bike Data\n\n\n\nIn order to order the seasons according to the calendar, I had to add this line to the code:\n\nproc sgplot data=ST2137.BIKE2;\n    vbox casual / category=season group=workingday grouporder=ascending;\n    xaxis values=('spring' 'summer' 'fall' 'winter');\n    yaxis grid;\nrun;\n\nThere is little insight from the previous two examples. However, now try the same plots, but on the log scale (modify the APPEARANCE tab and re-run). You should now obtain Figure 6.13:\n\n\n\n\n\n\nFigure 6.13: Boxplots log scale, Bike Data\n\n\n\nNow, we can observe that the difference within each season, is constant across seasons. Because the difference in logarithms is constant, it means that, on the original scale, it is a constant multiplicative factor that increases counts from workingday to non-working day.\nWe have arrived at a more succint representation of the relationship by using the log transform.\n\n\n\nQQ-plots\n\n\nExample 6.8 (Normality Check for Humidity) To create QQ-plots, we go to Tasks &gt; Statistics &gt; Distribution Analysis.\nSelect hum for the analysis variable. Under options, add the normal curve, the kernel density estimate, and the Normal quantile-quantile plot. You should obtain the following two charts:\n\n\n\n\n\n\n\n\n\nHistogram for humidity\n\n\n\n\n\n\n\nQQ-plot for humidity\n\n\n\n\n\nThe plot shows that humidity values are quite close to a Normal distribution, apart from a single observation on the left.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to SAS</span>"
    ]
  },
  {
    "objectID": "06-intro_to_sas.html#categorical-data",
    "href": "06-intro_to_sas.html#categorical-data",
    "title": "6  Introduction to SAS",
    "section": "6.8 Categorical Data",
    "text": "6.8 Categorical Data\nWe now turn to categorical data methods with SAS. We return to the dataset on student performance that we used in the topic on summarising data. Upload and store student-mat.csv as ST2137.STUD_PERF on the SAS Studio website.\n\nExample 6.9 (\\(\\chi^2\\) Test for Independence) \nFor a test of independence of address and paid, go to Tasks &gt; Table Analysis, and select:\n\naddress as the column variable\npaid as the row variable.\nUnder OPTIONS, check the “Chi-square statistics” box.\n\nThe following output should enable you to perform the test (Figure 6.14 and Figure 6.15).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.14: Observed & Expected Counts\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.15: Test statistic, p-value\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.16: Mosaic Plot\n\n\n\n\n\n\n\nFor measures of association, we only need to select the option for “Measures of Association” to generate the Kendall \\(\\tau_b\\) that we covered earlier.\n\nExample 6.10 (Kendall \\(\\tau\\) for Walc and Dalc) \nOnce we load the data ST2137.STUD_PERF, we go to Tasks &gt; Table Analysis. After selecting the two variables, we check the appropriate box to obtain Figure 6.17.\n\n\n\n\n\n\nFigure 6.17: Walc vs Dalc\n\n\n\nYou may observe that the particular associations computed and returned are similar to those by the Desc R package that we used in Example 4.10.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to SAS</span>"
    ]
  },
  {
    "objectID": "06-intro_to_sas.html#references",
    "href": "06-intro_to_sas.html#references",
    "title": "6  Introduction to SAS",
    "section": "6.9 References",
    "text": "6.9 References",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to SAS</span>"
    ]
  },
  {
    "objectID": "06-intro_to_sas.html#sec-web-ref-06",
    "href": "06-intro_to_sas.html#sec-web-ref-06",
    "title": "6  Introduction to SAS",
    "section": "6.10 Website References",
    "text": "6.10 Website References\n\nSAS account sign-up Use this link to sign up for a SAS account.\nSAS Studio link Once you have activated your account, use this link to login to your SAS studio online.\nSAS Studio Help This link contains help on SAS studio features and commands.\n\n\n\n\n\nFanaee-T, Hadi, and Joao Gama. 2013. “Event Labeling Combining Ensemble Detectors and Background Knowledge.” Progress in Artificial Intelligence, 1–15. https://doi.org/10.1007/s13748-013-0040-3.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to SAS</span>"
    ]
  },
  {
    "objectID": "07-2_sample_tests.html",
    "href": "07-2_sample_tests.html",
    "title": "7  Two-sample Hypothesis Tests",
    "section": "",
    "text": "7.1 Introduction\nIn this topic, we introduce the routines for a common class of hypothesis tests: the scenario of comparing the location parameter of two groups. This technique is commonly used in A/B testing to assess if an intervention has resulted in a significant difference between two groups.\nHypothesis tests are routinely abused in many ways by investigators. Such tests typically require strong assumptions to hold, and can result in false positives or negatives. As such, personally, I prefer to use confidence intervals to make an assessment of the significance of a result. However, in this topic, we introduce how the \\(p\\)-values can be obtained for these hypothesis tests.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-sample Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "07-2_sample_tests.html#sec-sig-test-proc",
    "href": "07-2_sample_tests.html#sec-sig-test-proc",
    "title": "7  Two-sample Hypothesis Tests",
    "section": "7.2 Procedure for Significance Tests",
    "text": "7.2 Procedure for Significance Tests\nAs a recap, here is the general approach for conducting a hypothesis test:\n\nStep 1: Assumptions\nIn this step, we make a note of the assumptions required for the test to be valid. In some tests, this step is carried out at the end of the others, but it is always essential to perform. Some tests are very sensitive to the assumptions - this is the main reason that the class of robust statistics was invented.\n\n\nStep 2: State the hypotheses and significance level\nThe purpose of hypothesis testing is to make an inferential statement about the population from which the data arose. This inferential statement is what we refer to as the hypothesis regarding the population.\n\n\n\n\n\n\nNote\n\n\n\nA hypothesis is a statement about population, usually claiming that a parameter takes a particular numerical value or falls in a certain range of values.\n\n\nThe hypotheses will be stated as a pair: The first hypothesis is the null hypothesis \\(H_0\\) and the second is the alternative hypothesis \\(H_1\\). Both statements will involve the population parameter (not the data summary) of interest. For example, if we have a sample of observations from two groups \\(A\\) and \\(B\\), and we wish to assess if the mean of the populations is different, the hypotheses would be\n\\[\\begin{eqnarray*}\nH_0: & \\mu_A = \\mu_B \\\\\nH_1: & \\mu_A \\ne \\mu_B\n\\end{eqnarray*}\\]\n\\(H_0\\) is usually a statement that indicates “no difference”, and \\(H_1\\) is usually the complement of \\(H_0\\).\nAt this stage, it is also crucial to state the significance level of the test. The significance level corresponds to the Type I error of the test - the probability of rejecting \\(H_0\\) when in fact it was true. This level is usually denoted as \\(\\alpha\\), and is usually taken to be 5%, but there is no reason to adopt this blindly. Think of the choice of 5% as corresponding to accepting an error rate of 1 in 20 - that’s how it was originally decided upon by Fisher. Where possible, the significance level should be chosen to be appropriate for the problem at hand.\n\n\n\n\n\n\nWarning\n\n\n\nIt is important to state the significance level at this stage, because if it is chosen after inspecting the data, the test is no longer valid. This is because, after knowing the \\(p\\)-value, one could always choose the significance level such that it yields the desired decision (reject or not).\n\n\nIt is also possible to test one-sided alternatives. In such scenarios, the hypotheses would be of the form:\n\\[\\begin{eqnarray*}\nH_0: & \\mu_A = \\mu_B \\\\\nH_1: & \\mu_A &gt; \\mu_B\n\\end{eqnarray*}\\]\nSuch a test is known as a one-tailed test.\n\n\nStep 3: Compute the test statistic\nThe test statistic is usually a measure of how far the observed data deviates from the scenario defined by \\(H_0\\). Usually, the larger it is, the more evidence we have against \\(H_0\\).\nThe construction of a hypothesis test involves the derivation of the exact or approximate distribution of the test statistic under \\(H_0\\). Deviations from the assumption could render this distribution incorrect.\n\n\nStep 4: Compute the \\(p\\)-value\nThe \\(p\\)-value quantifies the chance of observing such a test statistic, or one that is more extreme in the direction of \\(H_1\\), under \\(H_0\\). The distribution of the test statistic under \\(H_0\\) is used to compute this value between 0 and 1. A value closer to 0 indicates stronger evidence against \\(H_0\\).\nIn the case of a one-tailed test, try to understand the behaviour of the test statistic, and identify the signal that would yield more evidence supporting the alternative hypothesis.\n\n\nStep 5: State your conclusion\nThis is the binary decision stage. If the \\(p\\)-value is less than the stated significance level, we conclude that we reject \\(H_0\\). Otherwise, we say that we do not reject \\(H_0\\). It is conventional to use this terminology (instead of “accepting \\(H_1\\)”) since our \\(p\\)-value is obtained with respect to \\(H_0\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-sample Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "07-2_sample_tests.html#confidence-intervals",
    "href": "07-2_sample_tests.html#confidence-intervals",
    "title": "7  Two-sample Hypothesis Tests",
    "section": "7.3 Confidence Intervals",
    "text": "7.3 Confidence Intervals\nConfidence intervals are an alternative method of inference for population parameters. Instead of yielding a binary reject/do-not-reject result, they return a confidence interval that contains the plausible values for the population parameter. Many confidence intervals are derived by inverting hypothesis tests, and almost all confidence intervals are of the form\n\\[\n\\text{Sample estimate}\\; \\pm \\; \\text{margin of error}\n\\]\nFor instance, if we observe \\(x_1, \\ldots, x_n\\) from a Normal distribution, and wish to estimate the mean of the distribution, the 95% confidence interval based on the the \\(t\\) distribution is\n\\[\n\\bar{x} \\pm t_{0.025, n-1} \\times \\frac{s}{\\sqrt{n}}\n\\] where\n\n\\(s\\) is the sample standard deviation, and\n\\(t_{0.025, n-1}\\) is the 0.025-quantile from the \\(t\\) distribution with \\(n-1\\) degrees of freedom.\n\nThe formulas for many confidence intervals rely on asymptotic Normality of the estimator. However, this is an assumption that can be overcome with the technique of bootstrapping. We shall touch on this in the final topic of our course.\nBootstrapping can also be used to sidestep the distributional assumptions in hypothesis tests, but I still much prefer confidence intervals to tests because they yield an interval; they provide much more information than a binary outcome.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-sample Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "07-2_sample_tests.html#parametric-tests",
    "href": "07-2_sample_tests.html#parametric-tests",
    "title": "7  Two-sample Hypothesis Tests",
    "section": "7.4 Parametric Tests",
    "text": "7.4 Parametric Tests\nParametric tests are hypothesis tests that assume some form of distribution for the sample (or population) to follow. An example of such a test is the \\(t\\)-test, which assumes that the data originate from a Normal distribution.\nConversely, nonparametric tests are hypothesis tests that do not assume any form of distribution for the sample. It seems we should always use non-parametric tests since distributional assumptions would not be violated, right? Unfortunately, since nonparametric tests are so general, they do not have a high discriminative ability - we say that they have low power. In other words, if a dataset truly comes from a Normal distribution, using the \\(t\\)-test would be able to detect smaller differences between the groups better than a non-parametric test.\nIn this section, we cover parametric tests for comparing the difference in mean between two groups.\n\nIndependent Samples Test\nIn an independent samples \\(t\\)-test, observations in one group yield no information about the observations in the other group. Independent samples can arise in a few ways:\n\nIn an experimental study, study units could be assigned randomly to different treatments, thus forming the two groups.\nIn an observational study, we could draw a random sample from the population, and then record an explanatory categorical variable on each unit, such as the gender or senior-citizen status.\nIn an observational study, we could draw a random sample from a group (say smokers), and then a random sample from another group (say non-smokers). This would result in a situation where the independent 2-sample \\(t\\)-test is appropriate.\n\n\nFormal Set-up\nFormally speaking, this is how the independent 2-sample t-test works:\nSuppose that \\(X_1,X_2,\\ldots,X_{n_1}\\) are independent observations from group 1, and \\(Y_1, \\ldots Y_{n_2}\\) are independent observations from group 2. It is assumed that\n\\[\\begin{eqnarray}\nX_i &\\sim& N(\\mu_1,\\, \\sigma^2),\\; i=1,\\ldots,n_1 \\\\\nY_j &\\sim& N(\\mu_2,\\, \\sigma^2),\\; j=1,\\ldots,n_2\n\\end{eqnarray}\\]\nThe null and alternative hypotheses would be\n\\[\\begin{eqnarray*}\nH_0: & \\mu_1 = \\mu_2 \\\\\nH_1: & \\mu_1 \\ne \\mu_2 \\\\\n\\end{eqnarray*}\\]\nThe test statistic for this test is:\n\\[\nT_1 = \\frac{(\\bar{X} - \\bar{Y}) - 0 }{s_p\\sqrt{1/n_1 + 1/n_2} }\n\\] where \\[\ns^2_p = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1) s_2^2}{n_1 + n_2 -2 }\n\\]\nUnder \\(H_0\\), the test statistic \\(T_1 \\sim t_{n_1 + n_2 -2}\\). When we use a software to apply the test above, it will typically also return a confidence interval, computed as\n\\[\n(\\bar{X} - \\bar{Y}) \\pm t_{n_1 + n_2 -2, 1 - \\alpha/2} \\times s_p\\sqrt{1/n_1 + 1/n_2}\n\\]\nThe set-up above corresponds to the case where the variance within each group is assumed to be the same. We use information from both groups to estimate the common variance. If we find evidence in the data to the contrary, we used the unpooled variance in the denominator of \\(T_1\\):\n\\[\nT_{1,unpooled} = \\frac{(\\bar{X} - \\bar{Y}) - 0 }{\\sqrt{s^2_1/n_1 + s^2_2/n_2} }\n\\]\nwhere \\(s^2_1\\) and \\(s^2_2\\) are the sample variance from groups 1 and 2. The test statistic still follows a \\(t\\) distribution, but the degrees of freedom are approximated. This approximation is known as the Satterthwaite approximation.\n\nExample 7.1 (Abalone Measurements) \nThe dataset on abalone measurements from the UCI machine learning repository contains measurements of physical characteristics, along with the gender status. We derive a sample of 50 measurements of male and female abalone records for use here. Our goal is to study if there is a significant difference between the viscera weight1 between males and females. The derived dataset can be found on Canvas.\n\nR codePython codeSAS Output\n\n\n\nabl &lt;- read.csv(\"data/abalone_sub.csv\")\nx &lt;- abl$viscera[abl$gender == \"M\"]\ny &lt;- abl$viscera[abl$gender == \"F\"]\n\nt.test(x, y, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  x and y\nt = 0.91008, df = 98, p-value = 0.365\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.02336287  0.06294287\nsample estimates:\nmean of x mean of y \n  0.30220   0.28241 \n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\n\nabl = pd.read_csv(\"data/abalone_sub.csv\")\n#abl.head()\n#abalone_df.describe()\n\nx = abl.viscera[abl.gender == \"M\"]\ny = abl.viscera[abl.gender == \"F\"]\n\nt_out = stats.ttest_ind(x, y)\nci_95 = t_out.confidence_interval()\n\nprint(f\"\"\"\n* The p-value for the test is {t_out.pvalue:.3f}. \n* The actual value of the test statistic is {t_out.statistic:.3f}.\n* The upper and lower limits of the CI are ({ci_95[0]:.3f}, {ci_95[1]:.3f}).\n\"\"\")\n\n\n* The p-value for the test is 0.365. \n* The actual value of the test statistic is 0.910.\n* The upper and lower limits of the CI are (-0.023, 0.063).\n\n\n\n\nWhen we run the independent samples t-test on SAS, we should observe the following output Figure 7.1.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: SAS Abalone, Independent\n\n\n\n\n\n\n\n\n\nTo assess the normality assumption, we make histograms and qq-plots.\n\n\n\n\n\n\nNote\n\n\n\nOnly the R code is shown here.\n\n\nlibrary(lattice)\nhistogram(~viscera | gender, data=abl, type=\"count\")\nqqnorm(y, main=\"Female Abalones\");  qqline(y)\nqqnorm(x, main=\"Male Abalones\");  qqline(x)\n\n\n\n\n\n\nHistograms\n\n\n\n\n\n\n\n\n\nQQ-plot for females\n\n\n\n\n\n\n\nQQ-plot for males\n\n\n\n\n\nWe also need to assess if the variances are equal. While there are many hypothesis tests specifically for assessing if variances are equal (e.g. Levene, Bartlett), in our class, I advocate a simple rule of thumb. If the larger s.d is more than twice the smaller one, than we should not use the equal variance form of the test. This rule of thumb is widely used in practice (see Section 7.7.1).\n\nR codePython code\n\n\n\naggregate(viscera ~ gender, data=abl, sd)\n\n  gender   viscera\n1      F 0.1087070\n2      M 0.1087461\n\n\n\n\n\nabl.groupby('gender').describe()\n\n       viscera                                                            \n         count     mean       std    min       25%    50%       75%    max\ngender                                                                    \nF         50.0  0.28241  0.108707  0.095  0.201250  0.275  0.365125  0.575\nM         50.0  0.30220  0.108746  0.040  0.253125  0.310  0.348750  0.638\n\n\n\n\n\nWe would conclude that there is no significant difference between the mean viscera weight of males and females.\n\n\nApart from qq-plots, it is worthwhile to touch on additional methods that are used to assess how much a dataset deviates from Normality.\n\n\n\nMore on Assessing Normality\n\nSkewness\nThe Normal distribution is symmetric about it’s mean. Hence if we observe asymmetry in our histogram, we might suspect deviation from Normality. To quantify this asymmetry, we use skewness. As we can see from Figure 7.2, a histogram with a long tail on the right (left) is referred to as right-skewed (corr. left-skewed).\n\n\n\n\n\n\nFigure 7.2: Right/Non/Left-Skewed histograms\n\n\n\nOne method of estimating the skewness of a distribution from data is:\n\\[\ng_1 = \\frac{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^3}{[\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2 ]^{3/2}}\n\\] This is the method-of-moments estimator for the distribution skewness parameter. A value close to 0 indicates low skewness (i.e. high symmetry). Positive values correspond to right-skew and negative values to left-skew.\n\n\nKurtosis\nKurtosis measures the thickness of the tails of a distribution. Positive kurtosis implies that the tails are “fatter” than those of a Normal. Negative values indicate that the tails are “thinner” than those of a Normal.\nThe method of moments estimator is\n\\[\ng_2 = \\frac{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^4}{[\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2 ]^2} - 3\n\\]\n\n\nHypothesis tests for Normality\nThe Shapiro-Wilk test and the Kolmogorov-Smirnov test are formal hypothesis tests with the following hypotheses:\n\\[\\begin{eqnarray*}\nH_0: & \\text{ Data follows a Normal distribution} \\\\\nH_1: & \\text{ Data does not follow a Normal distribution}\n\\end{eqnarray*}\\]\nYou can read more about them in the references, but take note that applying multiple tests leads to a higher Type I error. Moreover, a large small sample size will almost always reject \\(H_0\\) because small deviations are being classed as significant. I advocate a more graphical approach in assessing Normality, especially since the solution to Non-normality (the bootstrap) is readily accessible today.\n\nExample 7.2 (Abalone Measurements) \nLet us apply the above computations to the abalone measurements.\n\nR codePython codeSAS output\n\n\n\nlibrary(DescTools)\naggregate(viscera ~ gender, data=abl, Skew, method=1)\n##   gender   viscera\n## 1      F 0.4060918\n## 2      M 0.2482997\n\naggregate(viscera ~ gender, data=abl, Kurt, method=1)\n##   gender    viscera\n## 1      F -0.2431501\n## 2      M  1.1660593\n\n# Shapiro-Wilk Test only for males:\nshapiro.test(x)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  x\n## W = 0.96779, p-value = 0.1878\n\n\n\n\nabl.groupby(\"gender\").skew()\n##          viscera\n## gender          \n## F       0.418761\n## M       0.256046\n\nfor i,df in abl.groupby('gender'):\n    print(f\"{df.gender.iloc[0]}: {df.viscera.kurt():.4f}\")\n## F: -0.1390\n## M: 1.4220\n    \nstats.shapiro(x)\n## ShapiroResult(statistic=np.float64(0.9677872659314101), pvalue=np.float64(0.1878490793650714))\n\n\n\n\n\n\n\n\n\nFigure 7.3: SAS Normality tests\n\n\n\n\n\n\nAlthough the skewness seems large for the female group, the Normality tests do not reject the null hypothesis (see Figure 7.3).\n\n\n\n\n\nPaired Sample Test\nThe data in a paired sample test also arises from two groups, but the two groups are not independent. A very common scenario that gives rise to this test is when the same subject receives both treatments. His/her measurement under each treatment gives rise to a measurement in each group. However, the measurements are no longer independent.\n\nExample 7.3 (Reaction time of drivers) Consider a study on 32 drivers sampled from a driving school. Each driver is put in a simulation of a driving situation, where a target flashes red and green at random periods. Whenever the driver sees red, he/she has to press a brake button.\nFor each driver, the study is carried out twice - at one of the repetitions, the individual carries on a phone conversation while at the other, the driver listens to the radio.\nEach measurement falls under one of two groups - “phone” or “radio”, but the measurements for driver \\(i\\) are clearly related. Some people might just have a slower/faster baseline reaction time!\nThis is a situation where a paired sample test is appropriate, not an independent sample test.\n\n\nFormal Set-up\nSuppose that we observe \\(X_1, \\ldots , X_n\\) independent observations from group 1 and \\(Y_1, \\ldots, Y_n\\) independent observations from group 2. However the pair \\((X_i, Y_i)\\) are correlated. It is assumed that\n\\[\\begin{eqnarray}\nX_i &\\sim& N(\\mu_1,\\, \\sigma_1^2),\\; i=1,\\ldots,n \\\\\nY_j &\\sim& N(\\mu_2,\\, \\sigma_2^2),\\; j=1,\\ldots,n\n\\end{eqnarray}\\]\nWe let \\(D_i = X_i - Y_i\\) for \\(i=1, \\ldots, n\\). It follows that \\[\nD_i \\sim N(\\mu_1 - \\mu_2,\\; \\sigma^2_1 + \\sigma^2_2 - 2 cov(X_i, Y_i))\n\\] The null and alternative hypotheses are stated in terms of the distribution of \\(D_i\\):\n\\[\\begin{eqnarray*}\nH_0: & \\mu_D = 0 \\\\\nH_1: & \\mu_D \\ne 0\n\\end{eqnarray*}\\]\nThe test statistic for this test is:\n\\[\nT_2 = \\frac{\\bar{D} - 0 }{s / \\sqrt{n} }\n\\] where \\[\ns^2 = \\frac{\\sum_{i=1}^n (D_i - \\bar{D})^2}{(n - 1)}\n\\]\nUnder \\(H-0\\), the test statistic \\(T_2 \\sim t_{n - 1}\\). When we use a software to apply the test above, it will typically also return a confidence interval, computed as\n\\[\n\\bar{D} \\pm t_{n - 1, 1 - \\alpha/2} \\times s / \\sqrt{n}\n\\]\n\nExample 7.4 (Heart Rate Before/After Treadmill) \nThe following dataset comes from the textbook (Rosner 2015), where an individual recorded his heart rate before using a treadmill (baseline) and 5 minutes after use, for 12 days in 2006.\n\nR codePython codeSAS Output\n\n\nTo run the paired test, we use the same function t.test as earlier, but we set the argument paired.\n\nhr_df &lt;- read.csv(\"data/health_promo_hr.csv\")\nbefore &lt;- hr_df$baseline\nafter &lt;- hr_df$after5\nt.test(before, after, paired=TRUE)\n\n\n    Paired t-test\n\ndata:  before and after\nt = -21.714, df = 11, p-value = 2.209e-10\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -16.77286 -13.68548\nsample estimates:\nmean difference \n      -15.22917 \n\n\n\n\n\nhr_df = pd.read_csv(\"data/health_promo_hr.csv\")\n#hr_df.head()\n\npaired_out = stats.ttest_rel(hr_df.baseline, hr_df.after5)\nprint(f\"\"\"\nTest statistic: {paired_out.statistic:.3f}.\np-val: {paired_out.pvalue:.3f}.\"\"\")\n\n\nTest statistic: -21.714.\np-val: 0.000.\n\n\n\n\n\n\n\nSAS HR Paired t-test\n\n\n\n\n\nIt is imperative to also make the checks for Normality. If you were to make them, you would realise that the sample size is rather small - it is difficult to make the case for Normality here.\nA couple of interesting plots from SAS are shown in Figure 7.4 and Figure 7.5.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.4: Paired Profiles\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.5: Agreement\n\n\n\n\n\n\nWhen we inspect the paired plot, we are looking for a similar gradient for each line, or at least similar in sign. If instead, we observed a combination of positive and negative gradients, then we would be less confident that there is a difference in means between the groups.\nFor the agreement plot, if we were to observe the points scattered around the line \\(y=x\\), then we would be more inclined to believe that the mean difference is indeed 0.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-sample Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "07-2_sample_tests.html#non-parametric-tests",
    "href": "07-2_sample_tests.html#non-parametric-tests",
    "title": "7  Two-sample Hypothesis Tests",
    "section": "7.5 Non-parametric Tests",
    "text": "7.5 Non-parametric Tests\nIf the distributional assumptions of the \\(t\\)-test are not met, we can take action in several ways. Historically, one method was to transform the data (if it was skewed) to make the histogram symmetric and thus closer to a Normal. But this was not ideal - it did not help in cases where the data was symmetric but had fatter tails than the Normal. As time progressed, statisticians invented tools to overcome the distributional assumptions. One sub-field was robust statistics, which keep the assumptions to a minimum, e.g. only requiring the underlying distribution to be symmetric. Another sub-field was the area of non-parametric statistics, where almost no distributional assumptions are made about the data.\nIn this section, we cover the non-parametric analogues for independent and paired 2-sample tests.\n\nIndependent Samples Test\nThe non-parametric analogue of the independent 2-sample test is the Wilcoxon Rank Sum (WRS) test (equivalent to the Mann-Whitney test).\n\nFormal Set-up\nSuppose that we observe \\(X_1, \\ldots , X_{n_1}\\) independent observations from group 1 (with distribution \\(F\\)) and \\(Y_1, \\ldots, Y_{n_2}\\) independent observations from group 2 (with distribution \\(G\\)).\nThe hypotheses associated with this test are:\n\\[\\begin{eqnarray*}\nH_0 &: & F = G \\\\\nH_1 &: & F(x) = G(x - \\Delta), \\Delta \\neq 0\n\\end{eqnarray*}\\]\nIn other words, the alternative hypothesis is that the distribution of group 1 is a location shift of the distribution of group 2.\nThe WRS test begins by pooling the \\(n_1 + n_2\\) data points and ranking them. The smallest observation is awarded rank 1, and the largest observation is awarded rank \\(n_1 + n_2\\), assuming there are no tied values. If there are tied values, the observations with the same value receive an averaged rank.\nCompute \\(R_1\\), the sum of the ranks in group 1. If this sum is large, it means that most of the values in group 1 were larger than those in group 2. Note that the average rank in the combined sample is \\[\n\\frac{n_1 + n_2 + 1}{2}\n\\]\nUnder \\(H_0\\), the expected rank sum of group 1 is \\[\nE(R_1) = n_1 \\times  \\frac{n_1 + n_2 + 1}{2}\n\\] The test statistic is a comparison of \\(R_1\\) with the above expected value:\n\\[\nW_1 = \\begin{cases}\n\\frac{\\left|R_1 - \\frac{n_1(n_1+n_2+1)}{2} \\right| - \\frac{1}{2}}{\\sqrt{n_1n_2(n_1 + n_2 +1)/12}}, & R_1 \\ne \\frac{n_1 (n_1 + n_2 + 1)}{2} \\text{ and no ties} \\\\\n\\frac{\\left|R_1 - \\frac{n_1(n_1+n_2+1)}{2} \\right| - \\frac{1}{2}}{\\sqrt{n_1n_2 \\left( n_1 + n_2 +1 - \\frac{\\sum_{i=1}^g t_i (t_i^2-1)}{(n_1+n_2)(n_1+n_2-1)} \\right) /12}}, & R_1 \\ne \\frac{n_1 (n_1 + n_2 + 1)}{2} \\text{ and ties present} \\\\\n0, & R_1 = \\frac{n_1 (n_1 + n_2 + 1)}{2}\n\\end{cases}\n\\]\nwhere \\(g\\) refers to the number of groups with ties, and \\(t_i\\) refers to the number of tied values in each group.\nThe test above should only be used if both \\(n_1\\) and \\(n_2\\) are at least 10, and if the observations (not the ranks) come from an underlying continuous distribution. If these assumptions hold, then the test statistic \\(W_1\\) follows a \\(N(0,1)\\) distribution.\n\nExample 7.5 (Abalone Measurements) \n\nR codePython codeSAS Output\n\n\nWe can perform the WRS test in R:\n\nwilcox.test(x, y)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  x and y\nW = 1415.5, p-value = 0.2553\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\nAs mentioned, the Mann-Whitney test will return the same \\(p\\)-value as the WRS test.\n\nwrs_out = stats.mannwhitneyu(x, y)\n\nprint(f\"\"\"Test statistic: {wrs_out.statistic:.3f}.\np-val: {wrs_out.pvalue:.3f}.\"\"\")\n\nTest statistic: 1415.500.\np-val: 0.255.\n\n\n\n\nThe SAS output can be seen in Figure 7.6.\n\n\n\n\n\n\nFigure 7.6: SAS Output, Wilcoxon Rank Sum\n\n\n\n\n\n\nSince we know the number of observations in each group to be more than 10, the approximation holds. Comparing to the Example 7.1, observe that we have a similar conclusion.\nNotice that the test statistic appears different in SAS. However, it is simply a matter of a location shift of the test statistic. R and Python subtract the smallest possible sum of ranks from group 1, but SAS does not. In our example, the group size is 50. Hence we can recover the SAS test statistic from the R test statistic with\n\\[\n1415.5 + \\frac{50(50+1)}{2} = 2690.5\n\\]\nThe \\(p\\)-value is identical in all three software.\n\n\n\n\n\nPaired Samples Test\nThe analogue of the paired sample \\(t\\)-test is known as the Wilcoxon Sign Test (WST).\n\nFormal Set-up\nAgain, suppose that we observe \\(X_1, \\ldots , X_n\\) observations from group 1 and \\(Y_1, \\ldots, Y_n\\) observations from group 2. Groups 1 and 2 are paired (or correlated) in some way.\nOnce again, we compute \\(D_i = X_i - Y_i\\). The null hypothesis is that\n\\[\\begin{eqnarray*}\nH_0 &: & \\text{median of $D_i$ is 0.} \\\\\nH_1 &: & \\text{median of $D_i$ is not 0.}\n\\end{eqnarray*}\\]\nWe begin by ranking the \\(|D_i|\\). Ignoring pairs for which \\(D_i = 0\\), we rank the remaining observations from 1 for the pair with the smallest absolute value, up to \\(n\\) for the pair with the largest absolute value (assuming no ties).\nWe then compute \\(R_1\\), the sum of ranks for the positive \\(D_i\\). If this sum is large, we expect that the pairs with \\(X_i &gt; Y_i\\) have a larger difference (in absolute values) than those with \\(X_i &lt; Y_i\\). Under \\(H_0\\), it can be shown that\n\\[\nE(R_1) = m(m+1)/4\n\\] where \\(m\\) is the number of of non-zero differences.\nThus the test statistic is a comparison of \\(R_1\\) with the above expected value:\n\\[\nW_2 = \\begin{cases}\n\\frac{\\left|R_1 - \\frac{n(n + 1)}{4} \\right| - \\frac{1}{2}}{\\sqrt{n (n+1)(2n + 1)/24}}, & R_1 \\ne \\frac{n(n + 1)}{4} \\text{ and no ties} \\\\\n\\frac{\\left|R_1 - \\frac{n(n+1)}{4} \\right| - \\frac{1}{2}}{\\sqrt{n (n+1)(2n+1)/(24 -\n\\sum_{i=1}^g (t^3_i - t_i) / 48 )}}, & R_1 \\ne \\frac{n (n + 1)}{4} \\text{ and ties present} \\\\\n0, & R_1 = \\frac{n (n + 1)}{4}\n\\end{cases}\n\\] where \\(g\\) denotes the number of tied groups, and \\(t_i\\) refers to the number of differences with the same absolute value in the \\(i\\)-th tied group.\nIf the number of non-zero \\(D_i\\)’s is at least 16, then the test statistic \\(W_2\\) follows a \\(N(0,1)\\) distribution approximately.\n\nExample 7.6 (Heart Rate Before/After Treadmill) \n\nR codePython codeSAS Output\n\n\nWe can perform the Wilcoxon Signed Rank test in R:\n\nwilcox.test(before, after, paired = TRUE, exact = FALSE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  before and after\nV = 0, p-value = 0.002507\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\nwsr_out = stats.wilcoxon(hr_df.baseline, hr_df.after5,  \n                         correction=True, method='approx')\nprint(f\"\"\"Test statistic: {wsr_out.statistic:.3f}.\np-val: {wsr_out.pvalue:.3f}.\"\"\")\n\nTest statistic: 0.000.\np-val: 0.003.\n\n\n\n\n\n\n\nSAS Signed Rank Test\n\n\n\n\n\nIn this problem, we do not have 16 non-zero \\(D_i\\)’s. Hence, we should in fact be using the “exact” version of the test (R and Python). However, the exact version of the test cannot be used when there are ties.\nSAS does indeed use the “exact” version of the test, so that accounts for the difference in \\(p\\)-values. The test statistic in SAS is computed by subtracting \\(E(R_1)\\), but in R and Python this subtraction is not done. To get from test statistic in R (0) to the one in SAS (-39):\n\\[\n0 - \\frac{12(12 + 1)}{4} = -39\n\\]\nIn cases like this, we can turn to the bootstrap, or we can use a permutation test. We shall revisit these in our final topic Section 10.1.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-sample Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "07-2_sample_tests.html#summary",
    "href": "07-2_sample_tests.html#summary",
    "title": "7  Two-sample Hypothesis Tests",
    "section": "7.6 Summary",
    "text": "7.6 Summary\nWhile the test statistics are shown in detail, we do not need the full details for our class. I should also point out that the definitions of the test statistic in Section 7.5.1 and Section 7.5.2 were both taken from Rosner (2015). However, take note that different software have slightly different implementations, for instance in how they deal with ties. As always, my advice is to read the documentation as much as possible, and then to use the output of the tests as a guide to your decision-making (instead of the absolute truth).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-sample Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "07-2_sample_tests.html#references",
    "href": "07-2_sample_tests.html#references",
    "title": "7  Two-sample Hypothesis Tests",
    "section": "7.7 References",
    "text": "7.7 References\n\nWebsite References\n\nUCI full abalone dataset: The dataset in the examples above, starting from Example 7.1, consists of samples from the full dataset.\nInference recap from Penn State:\n\nHypothesis testing recap\nConfidence intervals recap\n\nTests for Normality More information on the Kolmogorov-Smirnov and Shapiro-Wilks Tests for Normality.\nOverview of \\(t\\)-tests This page includes the rule of thumb about deciding when to use the equal variance test, and when to use the unequal variances version.\nSAS vs. R/Python This link provides an explanation why the WRS test statistic for SAS is different from R in Example 7.5.\n\n\n\n\n\nRosner, Bernard. 2015. Fundamentals of Biostatistics. Cengage learning.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-sample Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "07-2_sample_tests.html#footnotes",
    "href": "07-2_sample_tests.html#footnotes",
    "title": "7  Two-sample Hypothesis Tests",
    "section": "",
    "text": "Viscera is the gut weight after bleeding out the abalone (in grams).↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-sample Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "08-anova.html",
    "href": "08-anova.html",
    "title": "8  ANOVA",
    "section": "",
    "text": "8.1 Introduction\nIn the previous topic, we learned how to run two-sample \\(t\\)-tests. The objective of these procedures is to compare the means from two groups. Frequently, however, the means of more than two groups need to be compared.\nIn this topic, we introduce the one-way analysis of variance (ANOVA), which generalises the \\(t\\)-test methodology to more than 2 groups. Hypothesis tests in the ANOVA framework require the assumption of Normality. When this does not hold, we turn to the Kruskal-Wallis test - the non-parametric version, to compare distributions between groups.\nWhile the \\(F\\)-test in ANOVA provides a determination of whether or not the group means are different, in practice, we would always want to follow up with specific comparisons between groups as well. This topic covers how we can construct confidence intervals in those cases as well.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "08-anova.html#introduction",
    "href": "08-anova.html#introduction",
    "title": "8  ANOVA",
    "section": "",
    "text": "Example 8.1 (Effect of Antibiotics) \nThe following example was taken from Ekstrøm and Sørensen (2015). An experiment with dung from heifers1 was carried out in order to explore the influence of antibiotics on the decomposition of dung organic material. As part of the experiment, 36 heifers were randomly assigned into six groups.\nAntibiotics of different types were added to the feed for heifers in five of the groups. The remaining group served as a control group. For each heifer, a bag of dung was dug into the soil, and after 8 weeks the amount of organic material was measured for each bag.\nFigure 8.1 contains a boxplot of the data from each group.\n\n\n\n\n\n\n\n\nFigure 8.1\n\n\n\n\n\nCompared to the control group, it does appear that the median organic weight of the dung from the other heifer groups is higher. The following table displays the mean, standard deviation, and count from each group:\n\n\n      type  mean    sd count\n1  Control 2.603 0.119     6\n2  Alfacyp 2.895 0.117     6\n3 Enroflox 2.710 0.162     6\n4 Fenbenda 2.833 0.124     6\n5 Ivermect 3.002 0.109     6\n6 Spiramyc 2.855 0.054     4\n\n\nObserve that the Spiramycin group only yielded 4 readings instead of 6. Our goal in this topic is to apply a technique for assessing if group means are statistically different from one another. Here are the specific analyses that we shall carry out:\n\nIs there any significant difference, at 5% level, between the mean decomposition level of the groups?\nAt 5% level, is the mean level for Enrofloxacin different from the control group?\nPharmacologically speaking, Ivermectin and Fenbendazole are similar to each other. Let us call this sub-group (A). They work differently than Enrofloxacin. At 5% level, is there a significant difference between the mean from sub-group A and Enrofloxacin?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "08-anova.html#one-way-analysis-of-variance",
    "href": "08-anova.html#one-way-analysis-of-variance",
    "title": "8  ANOVA",
    "section": "8.2 One-Way Analysis of Variance",
    "text": "8.2 One-Way Analysis of Variance\n\nFormal Set-up\nSuppose there are \\(k\\) groups with \\(n_i\\) observations in the \\(i\\)-th group. The \\(j\\)-th observation in the \\(i\\)-th group will be denoted by \\(Y_{ij}\\). In the One-Way ANOVA, we assume the following model:\n\\[\\begin{equation}\nY_{ij}  = \\mu + \\alpha_i + e_{ij},\\; i=1,\\ldots,k,\\; j=1,\\ldots,n_i\n\\end{equation}\\]\n\n\\(\\mu\\) is a constant, representing the underlying mean of all groups taken together.\n\\(\\alpha_i\\) is a constant specific to the \\(i\\)-th group. It represents the difference between the mean of the \\(i\\)-th group and the overall mean.\n\\(e_{ij}\\) represents random error about the mean \\(\\mu + \\alpha_i\\) for an individual observation from the \\(i\\)-th group.\n\nIn terms of distributions, we assume that the \\(e_{ij}\\) are i.i.d from a Normal distribution with mean 0 and variance \\(\\sigma^2\\). This leads to the model for each observation:\n\\[\nY_{ij} \\sim N(\\mu + \\alpha_i,\\; \\sigma^2)\n\\tag{8.1}\\]\nIt is not possible to estimate both \\(\\mu\\) and all the \\(k\\) different \\(\\alpha_i\\)’s, since we only have \\(k\\) observed mean values for the \\(k\\) groups. For identifiability purposes, we need to constrain the parameters. There are two common constraints used, and note that different software have different defaults:\n\nSetting \\(\\sum_{i=1}^k \\alpha_i = 0\\), or\nSetting \\(\\alpha_1= 0\\).\n\nContinuing on from Equation 8.1, let us denote the mean for the \\(i\\)-th group as \\(\\overline{Y_i}\\), and the overall mean of all observations as \\(\\overline{\\overline{Y}}\\). We can then write the deviation of an individual observation from the overall mean as:\n\\[\nY_{ij} - \\overline{\\overline{Y}} = \\underbrace{(Y_{ij} - \\overline{Y_i})}_{\\text{within}} +\n\\underbrace{(\\overline{Y_i} - \\overline{\\overline{Y}})}_{\\text{between}}\n\\tag{8.2}\\]\nThe first term on the right of the above equation is the source of within-group variability. The second term on the right gives rise to between-group variability. The intuition behind the ANOVA procedure is that if the between-group variability is large and the within-group variability is small, then we have evidence that the group means are different.\nIf we square both sides of Equation 8.2 and sum over all observations, we arrive at the following equation; the essence of ANOVA:\n\\[\n\\sum_{i=1}^k \\sum_{j=1}^{n_i} \\left( Y_{ij} - \\overline{\\overline{Y}} \\right)^2 =\n\\sum_{i=1}^k \\sum_{j=1}^{n_i} \\left( Y_{ij} - \\overline{Y_i} \\right)^2 +\n\\sum_{i=1}^k \\sum_{j=1}^{n_i} \\left( \\overline{Y_i} -\n                                     \\overline{\\overline{Y}} \\right)^2\n\\]\nThe squared sums above are referred to as: \\[\nSS_T = SS_W + SS_B\n\\]\n\n\\(SS_T\\): Sum of Squares Total,\n\\(SS_W\\): Sum of Squares Within, and\n\\(SS_B\\): Sum of Squares Between.\n\nIn addition the following definitions are important for understanding the ANOVA output:\n\nThe Between Mean Square: \\[\nMS_B = \\frac{SS_B}{k-1}\n\\]\nThe Within Mean Square: \\[\nMS_W = \\frac{SS_W}{n - k}\n\\]\n\nThe mean squares are estimates of the variability between and within groups. The ratio of these quantities is the test statistic.\n\n\n\\(F\\)-Test in One-Way ANOVA\nThe null and alternative hypotheses are:\n\\[\\begin{eqnarray*}\nH_0 &:& \\alpha_i = 0 \\text{ for all } i \\\\\nH_1 &:& \\alpha_i \\ne 0 \\text{ for at least one } i\n\\end{eqnarray*}\\]\nThe test statistic is given by \\[\nF = \\frac{MS_B}{MS_W}\n\\]\nUnder \\(H_0\\), the statistic \\(F\\) follows an \\(F\\) distribution with \\(k-1\\) and \\(n-k\\) degrees of freedom.\n\n\nAssumptions\nThese are the assumptions that will need to be validated.\n\nThe observations are independent of each other. This is usually a characteristic of the design of the experiment, and is not something we can always check from the data.\nThe errors are Normally distributed. Residuals can be calculated as follows: \\[\nY_{ij} - \\overline{Y_i}\n\\] The distribution of these residuals should be checked for Normality.\nThe variance within each group is the same. In ANOVA, the \\(MS_W\\) is a pooled estimate (across the groups) that is used; in order for this to be valid, the variance within each group should be identical. As in the 2-sample situation, we shall avoid separate hypotheses tests and proceed with the rule-of-thumb that if the ratio of the largest to smallest standard deviation is less than 2, we can proceed with the analysis.\n\n\nExample 8.2 (F-test) \nWe being by applying the overall \\(F\\)-test to the heifers data, to assess if there is any significant difference between the means.\n\nR codePython codeSAS output\n\n\n\n#R \nheifers &lt;- read.csv(\"data/antibio.csv\")\nu_levels &lt;- sort(unique(heifers$type))\nheifers$type &lt;- factor(heifers$type, \n                       levels=u_levels[c(2, 1, 3, 4, 5, 6)])\nheifers_lm &lt;- lm(org ~ type, data=heifers)\nanova(heifers_lm)\n\nAnalysis of Variance Table\n\nResponse: org\n          Df  Sum Sq  Mean Sq F value    Pr(&gt;F)    \ntype       5 0.59082 0.118165  7.9726 8.953e-05 ***\nResiduals 28 0.41500 0.014821                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n#Python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nheifers = pd.read_csv(\"data/antibio.csv\")\nheifer_lm = ols('org ~ type', data=heifers).fit()\nanova_tab = sm.stats.anova_lm(heifer_lm, type=3,)\nprint(anova_tab)\n\n            df    sum_sq   mean_sq         F   PR(&gt;F)\ntype       5.0  0.590824  0.118165  7.972558  0.00009\nResidual  28.0  0.415000  0.014821       NaN      NaN\n\n\n\n\n\n\n\n\n\n\n\n\nAt the 5% significance level, we reject the null hypothesis to conclude that the group means are significantly different from one another. This answers question (1) from Example 8.1.\nTo extract the estimated parameters, we can use the following code:\n\nR codePython codeSAS output\n\n\n\n# R\nsummary(heifers_lm)\n\n\nCall:\nlm(formula = org ~ type, data = heifers)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.29000 -0.06000  0.01833  0.07250  0.18667 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.60333    0.04970  52.379  &lt; 2e-16 ***\ntypeAlfacyp   0.29167    0.07029   4.150 0.000281 ***\ntypeEnroflox  0.10667    0.07029   1.518 0.140338    \ntypeFenbenda  0.23000    0.07029   3.272 0.002834 ** \ntypeIvermect  0.39833    0.07029   5.667  4.5e-06 ***\ntypeSpiramyc  0.25167    0.07858   3.202 0.003384 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1217 on 28 degrees of freedom\nMultiple R-squared:  0.5874,    Adjusted R-squared:  0.5137 \nF-statistic: 7.973 on 5 and 28 DF,  p-value: 8.953e-05\n\n\n\n\n\n# Python\nprint(heifer_lm.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    org   R-squared:                       0.587\nModel:                            OLS   Adj. R-squared:                  0.514\nMethod:                 Least Squares   F-statistic:                     7.973\nDate:                Fri, 20 Dec 2024   Prob (F-statistic):           8.95e-05\nTime:                        16:04:37   Log-Likelihood:                 26.655\nNo. Observations:                  34   AIC:                            -41.31\nDf Residuals:                      28   BIC:                            -32.15\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept            2.8950      0.050     58.248      0.000       2.793       2.997\ntype[T.Control]     -0.2917      0.070     -4.150      0.000      -0.436      -0.148\ntype[T.Enroflox]    -0.1850      0.070     -2.632      0.014      -0.329      -0.041\ntype[T.Fenbenda]    -0.0617      0.070     -0.877      0.388      -0.206       0.082\ntype[T.Ivermect]     0.1067      0.070      1.518      0.140      -0.037       0.251\ntype[T.Spiramyc]    -0.0400      0.079     -0.509      0.615      -0.201       0.121\n==============================================================================\nOmnibus:                        2.172   Durbin-Watson:                   2.146\nProb(Omnibus):                  0.338   Jarque-Bera (JB):                1.704\nSkew:                          -0.545   Prob(JB):                        0.427\nKurtosis:                       2.876   Cond. No.                         6.71\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen estimating, both R and Python set one of the \\(\\alpha_i\\) to be equal to 0. In the case of R, it is the coefficient for Control, since we set it as the first level in the factor. For Python, we can tell from the output that the constraint has been placed on the coefficient for Alfacyp (since it is missing).\nHowever, all estimates are group means are identical. From the R output, we can compute that the estimate of the mean for the Alfacyp group is \\[\n2.603 + 0.292 = 2.895\n\\] From the Python output, we can read off (the Intercept term) that the estimate for Alfacyp is precisely \\[\n2.895 + 0 = 2.895\n\\]\nTo check the assumptions, we can use the following code:\n\nR codePython code\n\n\n# R\nr1 &lt;- residuals(heifers_lm)\nhist(r1)\nqqnorm(r1); qqline(r1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Python\nimport matplotlib.pyplot as plt\n\nf, axs = plt.subplots(1, 2, figsize=(8,4))\ntmp = plt.subplot(121)\nheifer_lm.resid.hist();\ntmp = plt.subplot(122)\nsm.qqplot(heifer_lm.resid, line=\"q\", ax=tmp);\n\n\n\n\n\n\n\n\n\n\n\nFor SAS, we have to create a new column containing the residuals in a temporary dataset before creating these plots.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "08-anova.html#sec-spec-grps",
    "href": "08-anova.html#sec-spec-grps",
    "title": "8  ANOVA",
    "section": "8.3 Comparing specific groups",
    "text": "8.3 Comparing specific groups\nThe \\(F\\)-test in a One-Way ANOVA indicates if all means are equal, but does not provide further insight into which particular groups differ. If we had specified beforehand that we wished to test if two particular groups \\(i_1\\) and \\(i_2\\) had different means, we could do so with a t-test. Here are the details to compute a Confidence Interval in this case:\n\nCompute the estimate of the difference between the two means: \\[\n\\overline{Y_{i_1}} - \\overline{Y_{i_2}}\n\\]\nCompute the standard error of the above estimator: \\[\n\\sqrt{MS_W \\left( \\frac{1}{n_{i_1}} + \\frac{1}{n_{i_2}} \\right) }\n\\]\nCompute the \\(100(1- \\alpha)%\\) confidence interval as: \\[\n\\overline{Y_{i_1}} - \\overline{Y_{i_2}} \\pm\nt_{n-k, \\alpha/2}  \\times\n\\sqrt{MS_W \\left( \\frac{1}{n_{i_1}} + \\frac{1}{n_{i_2}} \\right) }\n\\]\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you notice from the output in Example 8.1, the rule-of-thumb regarding standard deviations has not been satisfied. The ratio of largest to smallest standard deviations is slightly more than 2. Hence we should in fact switch to the non-parametric version of the test; the pooled estimate of the variance may not be valid. However, we shall proceed with this dataset just to demonstrate the next few techniques, instead of introducing a new dataset.\n\n\n\nExample 8.3 (Enrofloxacin vs. Control) \nLet us attempt to answer question (2), that we had set out earlier in Example 8.1.\n\nR codePython codeSAS code\n\n\n\n# R \nsummary_out &lt;- anova(heifers_lm)\nest_coef &lt;- coef(heifers_lm)\nest1  &lt;- unname(est_coef[3]) # coefficient for Enrofloxacin\nMSW &lt;- summary_out$`Mean Sq`[2]\ndf &lt;- summary_out$Df[2]\nq1 &lt;- qt(0.025, df, 0, lower.tail = FALSE)\n\nlower_ci &lt;- est1 - q1*sqrt(MSW * (1/6 + 1/6))\nupper_ci &lt;- est1 + q1*sqrt(MSW * (1/6 + 1/6))\ncat(\"The 95% CI for the diff. between Enrofloxacin and Control is (\",\n    format(lower_ci, digits = 3), \",\", \n    format(upper_ci, digits = 3), \").\", sep=\"\")\n\nThe 95% CI for the diff. between Enrofloxacin and Control is (-0.0373,0.251).\n\n\n\n\n\n# Python\nest1  = heifer_lm.params.iloc[2] - heifer_lm.params.iloc[1]\nMSW = heifer_lm.mse_resid\ndf = heifer_lm.df_resid\nq1 = -stats.t.ppf(0.025, df)\n\nlower_ci = est1 - q1*np.sqrt(MSW * (1/6 + 1/6))\nupper_ci = est1 + q1*np.sqrt(MSW * (1/6 + 1/6))\nprint(f\"\"\"The 95% CI for the diff. between Enrofloxacin and control is\n({lower_ci:.3f}, {upper_ci:.3f}).\"\"\") \n\nThe 95% CI for the diff. between Enrofloxacin and control is\n(-0.037, 0.251).\n\n\n\n\nIn order to get SAS to generate the estimate, modify the code to include clparm in the model statement, and include the estimate statement.\n\nproc glm data=ST2137.HEIFERS;\n    class type;\n    model org=type / clparm;\n    means type / hovtest=levene welch plots=none;\n    lsmeans type / adjust=tukey pdiff alpha=.05;\n    estimate 'enro_vs_control' type 0 -1 1 0 0 0;\n    output out=work.Oneway_stats r=residual;\n    run;\nquit;\n\n\n\n\n\n\n\n\n\nAs the confidence interval contains the value 0, the binary conclusion would be to not reject the null hypothesis at the 5% level.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "08-anova.html#contrast-estimation",
    "href": "08-anova.html#contrast-estimation",
    "title": "8  ANOVA",
    "section": "8.4 Contrast Estimation",
    "text": "8.4 Contrast Estimation\nA more general comparison, such as the comparison of a collection of \\(l_1\\) groups with another collection of \\(l_2\\) groups, is also possible. First, note that a linear contrast is any linear combination of the individual group means such that the linear coefficients add up to 0. In other words, consider \\(L\\) such that\n\\[\nL = \\sum_{i=1}^k c_i \\overline{Y_i}, \\text{ where } \\sum_{i=1}^k c_i = 0\n\\]\nNote that the comparison of two groups in Section 8.3 is a special case of this linear contrast.\nHere is the procedure for computing confidence intervals for a linear contrast:\n\nCompute the estimate of the contrast: \\[\nL = \\sum_{i=1}^k c_i \\overline{Y_i}\n\\]\nCompute the standard error of the above estimator: \\[\n\\sqrt{MS_W \\sum_{i=1}^k \\frac{c_i^2}{n_i} }\n\\]\nCompute the \\(100(1- \\alpha)%\\) confidence interval as: \\[\nL \\pm\nt_{n-k, \\alpha/2}  \\times\n\\sqrt{MS_W \\sum_{i=1}^k \\frac{c_i^2}{n_i} }\n\\]\n\n\nExample 8.4 (Comparing collection of groups) \nLet sub-group 1 consist of Ivermectin and Fenbendazole. Here is how we can compute a confidence interval for the difference between this sub-group, and Enrofloxacin.\n\nR codePython codeSAS code\n\n\n\nc1 &lt;- c(-1, 0.5, 0.5)\nn_vals &lt;- c(6, 6, 6)\nL &lt;- sum(c1*est_coef[3:5])\n\n#MSW &lt;- summary_out[[1]]$`Mean Sq`[2]\n#df &lt;- summary_out[[1]]$Df[2]\nse1 &lt;- sqrt(MSW * sum( c1^2 / n_vals ) )\n\nq1 &lt;- qt(0.025, df, 0, lower.tail = FALSE)\n\nlower_ci &lt;- L - q1*se1\nupper_ci &lt;- L + q1*se1\ncat(\"The 95% CI for the diff. between the two groups is (\",\n    format(lower_ci, digits = 2), \",\", \n    format(upper_ci, digits = 2), \").\", sep=\"\")\n\nThe 95% CI for the diff. between the two groups is (0.083,0.33).\n\n\n\n\n\nc1 = np.array([-1, 0.5, 0.5])\nn_vals = np.array([6, 6, 6,])\nL = np.sum(c1 * heifer_lm.params.iloc[2:5])\n\nMSW = heifer_lm.mse_resid\ndf = heifer_lm.df_resid\nq1 = -stats.t.ppf(0.025, df)\nse1 = np.sqrt(MSW*np.sum(c1**2 / n_vals))\n\nlower_ci = L - q1*se1\nupper_ci = L + q1*se1\nprint(f\"\"\"The 95% CI for the diff. between the two groups is \n({lower_ci:.3f}, {upper_ci:.3f}).\"\"\") \n\nThe 95% CI for the diff. between the two groups is \n(0.083, 0.332).\n\n\n\n\n\nproc glm data=ST2137.HEIFERS;\n    class type;\n    model org=type / clparm;\n    means type / hovtest=levene welch plots=none;\n    lsmeans type / adjust=tukey pdiff alpha=.05;\n    estimate 'group_A_vs_enro' type 0 0 -1 0.5 0.5 0;\n    output out=work.Oneway_stats r=residual;\n    run;\nquit;",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "08-anova.html#multiple-comparisons",
    "href": "08-anova.html#multiple-comparisons",
    "title": "8  ANOVA",
    "section": "8.5 Multiple Comparisons",
    "text": "8.5 Multiple Comparisons\nThe procedures in the previous two subsections correspond to contrasts that we had specified before collecting or studying the data. If, instead, we wished to perform particular comparisons after studying the group means, or if we wish to compute all pairwise contrasts, then we need to adjust for the fact that we are conducting multiple tests. If we do not do so, the chance of making at least one false positive increases greatly.\n\nBonferroni\nThe simplest method for correcting for multiple comparisons is to use the Bonferroni correction. Suppose we wish to perform \\(m\\) pairwise comparisons, either as a test or by computing confidence intervals. If we wish to maintain the significance level of each test at \\(\\alpha\\), then we should perform each of the \\(m\\) tests/confidence intervals at \\(\\alpha/m\\).\n\n\nTukeyHSD\nThis procedure is known as Tukey’s Honestly Significant Difference. It is designed to construct confidence intervals for all pairwise comparisons. For the same \\(\\alpha\\)-level, Tukey’s HSD method provides shorter confidence intervals than a Bonferroni correction for all pairwise comparisons.\n\nR codePython codeSAS output\n\n\n\nTukeyHSD(aov(heifers_lm), ordered = TRUE)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n    factor levels have been ordered\n\nFit: aov(formula = heifers_lm)\n\n$type\n                        diff         lwr       upr     p adj\nEnroflox-Control  0.10666667 -0.10812638 0.3214597 0.6563131\nFenbenda-Control  0.23000000  0.01520695 0.4447930 0.0304908\nSpiramyc-Control  0.25166667  0.01152074 0.4918126 0.0358454\nAlfacyp-Control   0.29166667  0.07687362 0.5064597 0.0034604\nIvermect-Control  0.39833333  0.18354028 0.6131264 0.0000612\nFenbenda-Enroflox 0.12333333 -0.09145972 0.3381264 0.5093714\nSpiramyc-Enroflox 0.14500000 -0.09514593 0.3851459 0.4549043\nAlfacyp-Enroflox  0.18500000 -0.02979305 0.3997930 0.1225956\nIvermect-Enroflox 0.29166667  0.07687362 0.5064597 0.0034604\nSpiramyc-Fenbenda 0.02166667 -0.21847926 0.2618126 0.9997587\nAlfacyp-Fenbenda  0.06166667 -0.15312638 0.2764597 0.9488454\nIvermect-Fenbenda 0.16833333 -0.04645972 0.3831264 0.1923280\nAlfacyp-Spiramyc  0.04000000 -0.20014593 0.2801459 0.9953987\nIvermect-Spiramyc 0.14666667 -0.09347926 0.3868126 0.4424433\nIvermect-Alfacyp  0.10666667 -0.10812638 0.3214597 0.6563131\n\n\n\n\n\nimport statsmodels.stats.multicomp as mc\n\ncp = mc.MultiComparison(heifers.org, heifers.type)\ntk = cp.tukeyhsd()\nprint(tk)\n\n  Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n========================================================\n group1   group2  meandiff p-adj   lower   upper  reject\n--------------------------------------------------------\n Alfacyp  Control  -0.2917 0.0035 -0.5065 -0.0769   True\n Alfacyp Enroflox   -0.185 0.1226 -0.3998  0.0298  False\n Alfacyp Fenbenda  -0.0617 0.9488 -0.2765  0.1531  False\n Alfacyp Ivermect   0.1067 0.6563 -0.1081  0.3215  False\n Alfacyp Spiramyc    -0.04 0.9954 -0.2801  0.2001  False\n Control Enroflox   0.1067 0.6563 -0.1081  0.3215  False\n Control Fenbenda     0.23 0.0305  0.0152  0.4448   True\n Control Ivermect   0.3983 0.0001  0.1835  0.6131   True\n Control Spiramyc   0.2517 0.0358  0.0115  0.4918   True\nEnroflox Fenbenda   0.1233 0.5094 -0.0915  0.3381  False\nEnroflox Ivermect   0.2917 0.0035  0.0769  0.5065   True\nEnroflox Spiramyc    0.145 0.4549 -0.0951  0.3851  False\nFenbenda Ivermect   0.1683 0.1923 -0.0465  0.3831  False\nFenbenda Spiramyc   0.0217 0.9998 -0.2185  0.2618  False\nIvermect Spiramyc  -0.1467 0.4424 -0.3868  0.0935  False\n--------------------------------------------------------",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "08-anova.html#kruskal-wallis-procedure",
    "href": "08-anova.html#kruskal-wallis-procedure",
    "title": "8  ANOVA",
    "section": "8.6 Kruskal-Wallis Procedure",
    "text": "8.6 Kruskal-Wallis Procedure\nIf the assumptions of the ANOVA procedure are not met, we can turn to a non-parametric version - the Kruskal Wallis test. This latter procedure is a generalisation of the Wilcoxon Rank-Sum test for 2 independent samples.\n\nFormal Set-up\nThe test statistic compares the average ranks in the individual groups. If these are close together, we would be inclined to conclude the treatments are equally effective.\nThe null hypothesis is that all groups follow the same distribution. The alternative hypothesis is that at least one of the groups’ distribution differs from another by a location shift. We then proceed with:\n\nPool the observations over all samples, thus constructing a combined sample of size \\(N = \\sum n_i\\). Assign ranks to individual observations, using average rank in the case of tied observations. Compute the rank sum \\(R_i\\) for each of the \\(k\\) samples.\nIf there are no ties, compute the test statistic as \\[\nH = \\frac{12}{N(N+1)} \\sum_{i=1}^k \\frac{R_i^2}{n_i} - 3(N+1)\n\\]\nIf there are ties, compute the test statistic as \\[\nH^* = \\frac{H}{1 - \\frac{\\sum_{j=1}^g (t^3_j - t_j)}{N^3 - N}}\n\\]\nwhere \\(t_j\\) refers to the number of observations with the same value in the \\(j\\)-th cluster of tied observations and \\(g\\) is the number of tied groups.\n\nUnder \\(H_0\\), the test statistic follows a \\(\\chi^2\\) distribution with \\(k-1\\) degrees of freedom.\n\n\n\n\n\n\nImportant\n\n\n\nThis test should only be used if \\(n_i \\ge 5\\) for all groups.\n\n\n\nExample 8.5 (Kruskal-Wallis Test) \nHere is the code and output from running the Kruskal-Wallis test in the three software.\n\nR codePython codeSAS output\n\n\n\nkruskal.test(heifers$org, heifers$type)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  heifers$org and heifers$type\nKruskal-Wallis chi-squared = 19.645, df = 5, p-value = 0.001457\n\n\n\n\n\nout = [x[1] for x in heifers.org.groupby(heifers.type)]\nkw_out = stats.kruskal(*out)\nprint(f\"\"\"The test statistic is {kw_out.statistic:.3f},\nthe p-value is {kw_out.pvalue:.3f}.\"\"\")\n\nThe test statistic is 19.645,\nthe p-value is 0.001.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "08-anova.html#summary",
    "href": "08-anova.html#summary",
    "title": "8  ANOVA",
    "section": "8.7 Summary",
    "text": "8.7 Summary\nThe purpose of this topic is to introduce you to the one-way ANOVA model. While there are restrictive distributional assumptions that it entails, I once again urge you to look past, at the information the method conveys. It attempts to compare the within-group variance to the between-group variance. Try to avoid viewing statistical procedures as flowcharts. If an assumption does not hold, or a p-value is borderline significant, try to investigate further on how sensitive the result is to those assumptions.\nOur job as analysts does not end after reporting the p-value from the \\(F\\)-test. We should try to dig deeper to uncover which groups are the ones that are different from the rest.\nFinally, take note that we should specify the contrasts we wish to test/estimate upfront, even before collecting the data. Only the Tukey comparison method (HSD) is valid if we perform multiple comparisons after inspecting the data.\nMost of the theoretical portions in this topic were taken from the textbook Rosner (2015).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "08-anova.html#references",
    "href": "08-anova.html#references",
    "title": "8  ANOVA",
    "section": "8.8 References",
    "text": "8.8 References\n\nWebsite References\n\nWelch’s ANOVA This website discusses an alternative test when the equal variance assumption has not been satisfied. It is for information only; it will not be tested.\nscipy stats This website contains documentation on the distribution-related functions that we might need from scipy stats, e.g. retrieving quantiles.\nContrast coding\nType I,II,III SS\n\n\n\n\n\nEkstrøm, Claus Thorn, and Helle Sørensen. 2015. “Statistical Data Analysis for the Life Sciences.” CRC Press, London.\n\n\nRosner, Bernard. 2015. Fundamentals of Biostatistics. Cengage learning.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "08-anova.html#footnotes",
    "href": "08-anova.html#footnotes",
    "title": "8  ANOVA",
    "section": "",
    "text": "A heifer is a young, female cow that has not had her first calf yet.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "09-regression.html",
    "href": "09-regression.html",
    "title": "9  Linear Regression",
    "section": "",
    "text": "9.1 Introduction\nRegression analysis is a technique for investigating and modeling the relationship between variables like X and Y. Here are some examples:\nIn all the above cases, we refer to \\(X\\) as the explanatory or independent variable. It is also sometimes referred to as a predictor. \\(Y\\) is referred to as the response or dependent variable. In this topic, we shall first introduce the case of simple linear regression, where we model the \\(Y\\) on a single \\(X\\). In later sections, we shall model the \\(Y\\) on multiple \\(X\\)’s. This latter technique is referred to as multiple linear regression.\nRegression models are used for two primary purposes:\nIn our course, we shall focus on the estimation aim, since prediction models require a paradigm of their own, and are best learnt alongside a larger suite of models e.g. decision trees, support vector machines, etc.\nIn subsequent sections, we shall revisit a couple of datasets from earlier topics to run linear regression models on them.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-regression.html#introduction",
    "href": "09-regression.html#introduction",
    "title": "9  Linear Regression",
    "section": "",
    "text": "Within a country, we may wish to use per capita income (X) to estimate the life expectancy (Y) of residents.\nWe may wish to use the size of a crab claw (X) to estimate the closing force that it can exert (Y).\nWe may wish to use the height of a person (X) to estimate their weight (Y).\n\n\n\n\nTo understand how certain explanatory variables affect the response variable. This aim is typically known as estimation, since the primary focus is on estimating the unknown parameters of the model.\nTo predict the response variable for new values of the explanatory variables. This is referred to as prediction.\n\n\n\n\nExample 9.1 (Concrete Data: Flow on Water) Recall the concrete dataset that we first encountered in the topic on summarising data. We shall go on to fit a linear regression to understand the relationship between the output of the flow test, and the amount of water used to create the concrete.\n\n\n\n\n\n\n\n\nFigure 9.1: Scatterplot with simple linear regression model\n\n\n\n\n\nNote that trend in the scatterplot in Figure 9.1. In this topic, we shall figure out how to estimate this line in this topic.\n\n\n\nExample 9.2 (Bike Rental Data) In the introduction to SAS, we encountered data on bike rentals in the USA over a period of 2 years. Here, we shall attempt to model the number of registered users on the number of casual users.\n\n\n\n\n\n\n\n\nFigure 9.2: Scatterplot of registered vs. casual bike renters\n\n\n\n\n\nContingent on whether the day is a working one or not, it does appear that the trendline is different (see Figure 9.2).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-regression.html#simple-linear-regression",
    "href": "09-regression.html#simple-linear-regression",
    "title": "9  Linear Regression",
    "section": "9.2 Simple Linear Regression",
    "text": "9.2 Simple Linear Regression\n\nFormal Set-up\nThe simple linear regression model is applicable when we have observations \\((X_i, Y_i)\\) for \\(n\\) individuals. For now, let’s assume both the \\(X\\) and \\(Y\\) variables are quantitative.\nThe simple linear regression model is given by\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + e_i\n\\tag{9.1}\\] where\n\n\\(\\beta_0\\) is intercept term,\n\\(\\beta_1\\) is the slope, and\n\\(e_i\\) is an error term, specific to each individual in the dataset.\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown constants that need to be estimated from the data. There is an implicit assumption in the formulation of the model that there is a linear relationship between \\(Y_i\\) and \\(X_i\\). In terms of distributions, we assume that the \\(e_i\\) are i.i.d Normal.\n\\[\ne_i \\sim N(0, \\sigma^2), \\; i =1\\ldots, n\n\\tag{9.2}\\]\nThe constant variance assumption is also referred to as homoscedascity (homo-skee-das-city). The validity of the above assumptions will have to be checked after the model is fitted. All in all, the assumptions imply that:\n\n\\(E(Y_i | X_i) = \\beta_0 + \\beta_1 X_i\\), for \\(i=1, \\ldots, n\\).\n\\(Var(Y_i | X_i) = Var(e_i) = \\sigma^2\\), for \\(i=1, \\ldots, n\\).\nThe \\(Y_i\\) are independent.\nThe \\(Y_i\\)’s are Normally distributed.\n\n\n\nEstimation\nBefore deploying or using the model, we need to estimate optimal values to use for the unknown \\(\\beta_0\\) and \\(\\beta_1\\). We shall introduce the method of Ordinary Least Squares (OLS) for the estimation. Let us define the error Sum of Squares to be\n\\[\nSS_E = S(\\beta_0, \\beta_1) = \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\n\\tag{9.3}\\]\nThen the OLS estimates of \\(\\beta_0\\) and \\(\\beta_1\\) are given by \\[\n\\mathop{\\arg \\min}_{\\beta_0, \\beta_1} \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\n\\] The minimisation above can be carried out analytically, by taking partial derivative with respect to the two parameters and setting them to 0.\n\\[\\begin{eqnarray*}\n\\frac{\\partial S}{\\partial \\beta_0}  &=& -2 \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i) = 0 \\\\\n\\frac{\\partial S}{\\partial \\beta_1}  &=& -2 \\sum_{i=1}^n X_i (Y_i - \\beta_0 - \\beta_1 X_i) = 0\n\\end{eqnarray*}\\]\nSolving and simplifying, we arrive at the following: \\[\\begin{eqnarray*}\n\\hat{\\beta_1} &=& \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\\\\n\\hat{\\beta_0} &=& \\bar{Y} - \\hat{\\beta_0} \\bar{X}\n\\end{eqnarray*}\\] where \\(\\bar{Y} = (1/n)\\sum Y_i\\) and \\(\\bar{X} = (1/n)\\sum X_i\\).\nIf we define the following sums: \\[\\begin{eqnarray*}\nS_{XY} &=& \\sum_{i=1}^n X_i Y_i - \\frac{(\\sum_{i=1}^n X_i )(\\sum_{i=1}^n Y_i )}{n} \\\\\nS_{XX} &=& \\sum_{i=1}^n X_i^2 - \\frac{(\\sum_{i=1}^n X_i )^2}{n}\n\\end{eqnarray*}\\] then a form convenient for computation of \\(\\hat{\\beta_1}\\) is \\[\n\\hat{\\beta_1} = \\frac{S_{XY}}{S_{XX}}\n\\]\nOnce we have the estimates, we can use Equation 9.1 to compute fitted values for each observation, corresponding to our best guess of the mean of the distributions from which the observations arose: \\[\n\\hat{Y_i} = \\hat{\\beta_0} + \\hat{\\beta_1} X_i, \\quad i = 1, \\ldots, n\n\\] As always, we can form residuals as the deviations from fitted values. \\[\nr_i = Y_i - \\hat{Y}_i\n\\tag{9.4}\\] Residuals are our best guess at the unobserved error terms \\(e_i\\). Squaring the residuals and summing over all observations, we can arrive at the following decomposition, which is very similar to the one in the ANOVA model:\n\\[\n\\underbrace{\\sum_{i=1}^n (Y_i  - \\bar{Y})^2}_{SS_T} =  \n\\underbrace{\\sum_{i=1}^n (Y_i  - \\hat{Y_i})^2}_{SS_{Res}} +\n\\underbrace{\\sum_{i=1}^n (\\hat{Y_i}  - \\bar{Y})^2}_{SS_{Reg}}\n\\]\nwhere\n\n\\(SS_T\\) is known as the total sum of squares.\n\\(SS_{Res}\\) is known as the residual sum of squares.\n\\(SS_{Reg}\\) is known as the regression sum of squares.\n\nIn our model, recall from Equation 9.2 that we had assumed equal variance for all our observations. We can estimate \\(\\sigma^2\\) with \\[\n\\hat{\\sigma^2} = \\frac{SS_{Res}}{n-2}\n\\] Our distributional assumptions lead to the following for our estimates \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\):\n\\[\\begin{eqnarray}\n\\hat{\\beta_0} &\\sim& N(\\beta_0,\\; \\sigma^2(1/n + \\bar{X}^2/S_{XX})) \\\\\n\\hat{\\beta_1} &\\sim& N(\\beta_1,\\; \\sigma^2/S_{XX})\n\\end{eqnarray}\\]\nThe above are used to construct confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\), based on \\(t\\)-distributions.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-regression.html#sec-slr-F",
    "href": "09-regression.html#sec-slr-F",
    "title": "9  Linear Regression",
    "section": "9.3 Hypothesis Test for Model Significance",
    "text": "9.3 Hypothesis Test for Model Significance\nThe first test that we introduce here is to test if the coefficient \\(\\beta_1\\) is significantly different from 0. It is essentially a test of whether it was worthwhile to use a regression model of the form in Equation 9.1, instead of a simple mean to represent the data.\nThe null and alternative hypotheses are:\n\\[\\begin{eqnarray*}\nH_0 &:& \\beta_1 = 0\\\\\nH_1 &:& \\beta_1 \\ne 0\n\\end{eqnarray*}\\]\nThe test statistic is\n\\[\nF_0 = \\frac{SS_{Reg}/1}{SS_{Res}/(n-2)}\n\\tag{9.5}\\]\nUnder the null hypothesis, \\(F_0 \\sim F_{1,n-2}\\).\nIt is also possible to perform this same test as a \\(t\\)-test, using the result earlier. The statement of the hypotheses is equivalent to the \\(F\\)-test. The test statistic \\[\nT_0 = \\frac{\\hat{\\beta_1}}{\\sqrt{\\hat{\\sigma^2}/S_{XX}}}\n\\tag{9.6}\\] Under \\(H_0\\), the distribution of \\(T_0\\) is \\(t_{n-2}\\). This \\(t\\)-test and the earlier \\(F\\)-test in this section are identical. It can be proved that \\(F_0 = T_0^2\\); the obtained \\(p\\)-values will be identical.\n\nCoefficient of Determination, \\(R^2\\)\nThe coefficient of determination \\(R^2\\) is defined as\n\\[\nR^2 = 1 - \\frac{SS_{Res}}{SS_T} = \\frac{SS_{Reg}}{SS_T}\n\\] It can be interpreted as the proportion of variation in \\(Yi\\), explained by the inclusion of \\(X_i\\). Since \\(0 \\le SS_{Res} \\le SS_T\\), we can easily prove that \\(0 \\le R^2 \\le 1\\). The larger the value of \\(R^2\\) is, the better the model is.\nWhen we get to the case of multiple linear regression, take note that simply including more variables in the model will increase \\(R^2\\). This is undesirable; it is preferable to have a parsimonious model that explains the response variable well.\n\nExample 9.3 (Concrete Data Model) \nIn this example, we focus on the estimation of the model parameters for the two variables we introduced in Example 9.1\n\nR codePython codeSAS output\n\n\n\n#R \nconcrete &lt;- read.csv(\"data/concrete+slump+test/slump_test.data\")\nnames(concrete)[c(1,11)] &lt;- c(\"id\", \"Comp.Strength\")\nlm_flow_water &lt;- lm(FLOW.cm. ~ Water, data=concrete)\nsummary(lm_flow_water)\n\n\nCall:\nlm(formula = FLOW.cm. ~ Water, data = concrete)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.211 -10.836   2.734  11.031  22.163 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -58.72755   13.28635  -4.420 2.49e-05 ***\nWater         0.54947    0.06704   8.196 8.10e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.68 on 101 degrees of freedom\nMultiple R-squared:  0.3995,    Adjusted R-squared:  0.3935 \nF-statistic: 67.18 on 1 and 101 DF,  p-value: 8.097e-13\n\n\n\n\n\n#Python\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nconcrete = pd.read_csv(\"data/concrete+slump+test/slump_test.data\")\nconcrete.rename(columns={'No':'id', \n                         'Compressive Strength (28-day)(Mpa)':'Comp_Strength',\n                         'FLOW(cm)': 'Flow'},\n                inplace=True)\nlm_flow_water = ols('Flow ~ Water', data=concrete).fit()\nprint(lm_flow_water.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   Flow   R-squared:                       0.399\nModel:                            OLS   Adj. R-squared:                  0.394\nMethod:                 Least Squares   F-statistic:                     67.18\nDate:                Fri, 20 Dec 2024   Prob (F-statistic):           8.10e-13\nTime:                        15:52:54   Log-Likelihood:                -414.60\nNo. Observations:                 103   AIC:                             833.2\nDf Residuals:                     101   BIC:                             838.5\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -58.7276     13.286     -4.420      0.000     -85.084     -32.371\nWater          0.5495      0.067      8.196      0.000       0.416       0.682\n==============================================================================\nOmnibus:                        6.229   Durbin-Watson:                   1.843\nProb(Omnibus):                  0.044   Jarque-Bera (JB):                5.873\nSkew:                          -0.523   Prob(JB):                       0.0530\nKurtosis:                       2.477   Cond. No.                     1.95e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.95e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the output, we can note that the estimated model for Flow (\\(Y\\)) against Water (\\(X\\)) is: \\[\nY = -58.73 + 0.55 X\n\\] The estimates are \\(\\hat{\\beta_0} = -58.73\\) and \\(\\hat{\\beta_1} = 0.55\\). This is the precise equation that was plotted in Figure 9.1. The \\(R^2\\) is labelled as “Multiple R-squared” in the R output. The value is 0.3995, which means that about 40% of the variation in \\(Y\\) is explained by \\(X\\).\nA simple interpretation1 of the model is as follows:\n\nFor every 1 unit increase in Water, there is an average associated increase in Flow rate of 0.55 units.\n\nTo obtain confidence intervals for the parameters, we can use the following code in R. The Python summary already contains the confidence intervals.\n\nR code\n\n\n\n#R \nconfint(lm_flow_water)\n\n                  2.5 %      97.5 %\n(Intercept) -85.0841046 -32.3709993\nWater         0.4164861   0.6824575\n\n\n\n\n\nWe can read off that the 95% Confidence intervals are:\n\nFor \\(\\beta_0\\): (-85.08, -32.37)\nFor \\(\\beta_1\\): (0.42, 0.68)\n\n\n\n\nExample 9.4 (Bike Data F-test) \nIn this example, we shall fit a simple linear regression model to the bike data, constrained to the non-working days. In other words, we shall focus on fitting just the blue line, from the blue points, in Figure 9.2.\n\nR codePython codeSAS output\n\n\n\n#R \nbike2 &lt;- read.csv(\"data/bike2.csv\")\nbike2_sub &lt;- bike2[bike2$workingday == \"no\", ]\nlm_reg_casual &lt;- lm(registered ~ casual, data=bike2_sub)\nanova(lm_reg_casual)\n\nAnalysis of Variance Table\n\nResponse: registered\n           Df    Sum Sq   Mean Sq F value    Pr(&gt;F)    \ncasual      1 237654556 237654556  369.25 &lt; 2.2e-16 ***\nResiduals 229 147386970    643611                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n#Python\nbike2 = pd.read_csv(\"data/bike2.csv\")\nbike2_sub = bike2[bike2.workingday == \"no\"]\n\nlm_reg_casual = ols('registered ~ casual', bike2_sub).fit()\nanova_tab = sm.stats.anova_lm(lm_reg_casual,)\nanova_tab\n\n             df        sum_sq       mean_sq           F        PR(&gt;F)\ncasual      1.0  2.376546e+08  2.376546e+08  369.251728  1.183368e-49\nResidual  229.0  1.473870e+08  6.436112e+05         NaN           NaN\n\n\n\n\n\n\n\n\n\n\n\n\nThe output above includes the sum-of-squares that we need to perform the \\(F\\)-test outlined in Section 9.3. From the output table, we can see that \\(SS_{Reg} = 237654556\\) and \\(SS_{Res} = 147386970\\). The value of \\(F_0\\) for this dataset is 369.25. The \\(p\\)-value is extremely small (\\(2 \\times 10^{-16}\\)), indicating strong evidence against \\(H_0\\), i.e. that \\(\\beta_1 = 0\\).\n\n\nActually, if you observe carefully in Example 9.3, the output from R contains both the \\(t\\)-test for significance of \\(\\beta_1\\), and the \\(F\\)-test statistic based on sum-of-squares. The \\(p\\)-value in both cases is \\(8.10 \\times 10^{1-3}\\).\nIn linear regression, we almost always wish to use the model to understand what the mean of future observations would be. In the concrete case, we may wish to use the model to understand how the Flow test output values change as the amount of Water in the mixture changes. This is because, based on our formulation,\n\\[\nE(Y | X) = \\beta_0 + \\beta_1 X\n\\]\nAfter estimating the parameters, we would have: \\[\n\\widehat{E(Y | X)} = \\hat{\\beta_0} + \\hat{\\beta_1} X\n\\]\nThus we can vary the values of \\(X\\) to study how the mean of \\(Y\\) changes. Here is how we can do so in the concrete model for data.\n\nExample 9.5 (Concrete Data Predicted Means) \nIn order to create the predicted means, we shall have to create a dataframe with the new values for which we require the predictions. We are first going to set up a new matrix of \\(X\\)-values corresponding to the desired range.\n\nR codePython codeSAS Output\n\n\n\n#R \nnew_df &lt;- data.frame(Water = seq(160, 240, by = 5))\nconf_intervals &lt;- predict(lm_flow_water, new_df, interval=\"conf\")\n\nplot(concrete$Water, concrete$FLOW.cm., ylim=c(0, 100),\n     xlab=\"Water\", ylab=\"Flow\", main=\"Confidence and Prediction Intervals\")\nabline(lm_flow_water, col=\"red\")\nlines(new_df$Water, conf_intervals[,\"lwr\"], col=\"red\", lty=2)\nlines(new_df$Water, conf_intervals[,\"upr\"], col=\"red\", lty=2)\nlegend(\"bottomright\", legend=c(\"Fitted line\", \"Lower/Upper CI\"), \n       lty=c(1,2), col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n# Python\nnew_df = sm.add_constant(pd.DataFrame({'Water' : np.linspace(160,240, 10)}))\n\npredictions_out = lm_flow_water.get_prediction(new_df)\n\nax = concrete.plot(x='Water', y='Flow', kind='scatter', alpha=0.5 )\nax.set_title('Flow vs. Water');\nax.plot(new_df.Water, predictions_out.conf_int()[:, 0].reshape(-1), \n        color='blue', linestyle='dashed');\nax.plot(new_df.Water, predictions_out.conf_int()[:, 1].reshape(-1), \n        color='blue', linestyle='dashed');\nax.plot(new_df.Water, predictions_out.predicted, color='blue');\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe fitted line is the straight line formed using \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). The dashed lines are 95% Confidence Intervals for \\(E(Y|X)\\), for varying values of \\(X\\). They are formed by joining up the lower bounds and the upper bounds separately. Notice how the limits get wider the further away we are from \\(\\bar{X} \\approx 200\\).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-regression.html#multiple-linear-regression",
    "href": "09-regression.html#multiple-linear-regression",
    "title": "9  Linear Regression",
    "section": "9.4 Multiple Linear Regression",
    "text": "9.4 Multiple Linear Regression\n\nFormal Setup\nWhen we have more than 1 explanatory variable, we turn to multiple linear regression - generalised version of what we have been dealing with so far. We would still have observed information from \\(n\\) individuals, but for each one, we now observe a vector of values: \\[\nY_i, \\, X_{1,i},  \\, X_{2,i}, \\ldots, \\, X_{p-1,i},  X_{p,i}\n\\] In other words, we observe \\(p\\) independent variables and 1 response variable for each individual in our dataset. The analogous equation to Equation 9.1 is \\[\nY_i = \\beta_0 + \\beta_1 X_{1,i} + \\cdots + \\beta_p  X_{p,i} + e\n\\tag{9.7}\\]\nIt is easier to write things with matrices for multiple linear regression:\n\\[\n\\textbf{Y} = \\begin{bmatrix}\nY_1 \\\\\nY_2 \\\\\n\\vdots \\\\\nY_n\n\\end{bmatrix}, \\;\n\\textbf{X} = \\begin{bmatrix}\n1 & X_{1,1} & X_{2,1} & \\cdots &X_{p,1}\\\\\n1 & X_{1,2} & X_{2,2} & \\cdots &X_{p,2}\\\\\n\\vdots & \\vdots & \\vdots & {} & \\vdots \\\\\n1 & X_{1,n} & X_{2,n} & \\cdots &X_{p,n}\\\\\n\\end{bmatrix}, \\;\n\\boldsymbol{ \\beta } = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix}, \\;\n\\boldsymbol{e} = \\begin{bmatrix}\ne_1 \\\\\ne_2 \\\\\n\\vdots \\\\\ne_n\n\\end{bmatrix}\n\\]\nWith the above matrices, we can re-write Equation 9.7 as \\[\n\\textbf{Y} = \\textbf{X} \\boldsymbol{\\beta} + \\textbf{e}\n\\] We retain the same distributional assumptions as in Section 9.2.1.\n\n\nEstimation\nSimilar to Section 9.2.2, we can define \\(SS_E\\) to be \\[\nSS_E = S(\\beta_0, \\beta_1,\\ldots,\\beta_p) = \\sum_{i=1}^n (Y_i - \\beta_0 -\n\\beta_1 X_{1,i} - \\cdots - \\beta_p X_{p,i} )^2\n\\tag{9.8}\\]\nMinimising the above cost function leads to the OLS estimates: \\[\n\\hat{\\boldsymbol{\\beta}} =  (\\textbf{X}'\\textbf{X})^{-1} \\textbf{X}'\\textbf{Y}\n\\] The fitted values can be computed with \\[\n\\hat{\\textbf{Y}} = \\textbf{X} \\hat{\\boldsymbol{\\beta}} =\n\\textbf{X} (\\textbf{X}'\\textbf{X})^{-1} \\textbf{X}'\\textbf{Y}\n\\] Residuals are obtained as \\[\n\\textbf{r} = \\textbf{Y} - \\hat{\\textbf{Y}}\n\\] Finally, we estimate \\(\\sigma^2\\) using \\[\n\\hat{\\sigma^2} = \\frac{SS_{Res}}{n-p} = \\frac{\\textbf{r}' \\textbf{r}}{n-p}\n\\]\n\n\nCoefficient of Determination, \\(R^2\\)\nIn the case of multiple linear regression, \\(R^2\\) is calculated exactly as in simple linear regression, and its interpretation remains the same: \\[\nR^2 = 1 - \\frac{SS_{Res}}{SS_T}\n\\]\nHowever, note that \\(R^2\\) can be inflated simply by adding more terms to the model (even insignificant terms). Thus, we use the adjusted \\(R^2\\), which penalizes us for adding more and more terms to the model: \\[\nR^2_{adj} = 1 - \\frac{SS_{Res}/(n-p)}{SS_T/(n-1)}\n\\]\n\n\nHypothesis Tests\nThe \\(F\\)-test in the multiple linear regression helps determine if our regression model provides any advantage over the simple mean model. The null and alternative hypotheses are:\n\\[\\begin{eqnarray*}\nH_0 &:& \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\\\\\nH_1 &:& \\beta_j \\ne 0 \\text{ for at least one } j \\in \\{1, 2, \\ldots, p\\}\n\\end{eqnarray*}\\]\nThe test statistic is\n\\[\nF_1 = \\frac{SS_{Reg}/p}{SS_{Res}/(n-p-1)}\n\\tag{9.9}\\]\nUnder the null hypothesis, \\(F_0 \\sim F_{p,n-p-1}\\).\nIt is also possible to test for the significance of individual \\(\\beta\\) terms, using a \\(t\\)-test. The output is typically given for all the coefficients in a table. The statement of the hypotheses pertaining to these tests is: \\[\\begin{eqnarray*}\nH_0 &:& \\beta_j = 0\\\\\nH_1 &:& \\beta_j \\ne 0\n\\end{eqnarray*}\\]\nHowever, note that these \\(t\\)-tests are partial because it should be interpreted as a test of the contribution of \\(\\beta_j\\), given that all other terms are already in the model.\n\nExample 9.6 (Concrete Data Multiple Linear Regression) \nIn this second model for concrete, we add a second predictor variable, Slag. The updated model is \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + e\n\\] where \\(X_1\\) corresponds to Water, and \\(X_2\\) corresponds to Slag.\n\nR codePython codeSAS output\n\n\n\n# R \nlm_flow_water_slag &lt;- lm(FLOW.cm. ~ Water + Slag, data=concrete)\nsummary(lm_flow_water_slag)\n\n\nCall:\nlm(formula = FLOW.cm. ~ Water + Slag, data = concrete)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.687 -10.746   2.010   9.224  23.927 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -50.26656   12.38669  -4.058 9.83e-05 ***\nWater         0.54224    0.06175   8.781 4.62e-14 ***\nSlag         -0.09023    0.02064  -4.372 3.02e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.6 on 100 degrees of freedom\nMultiple R-squared:  0.4958,    Adjusted R-squared:  0.4857 \nF-statistic: 49.17 on 2 and 100 DF,  p-value: 1.347e-15\n\n\n\n\n\n# Python\nlm_flow_water_slag = ols('Flow ~ Water + Slag', data=concrete).fit()\nprint(lm_flow_water_slag.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   Flow   R-squared:                       0.496\nModel:                            OLS   Adj. R-squared:                  0.486\nMethod:                 Least Squares   F-statistic:                     49.17\nDate:                Fri, 20 Dec 2024   Prob (F-statistic):           1.35e-15\nTime:                        15:52:54   Log-Likelihood:                -405.59\nNo. Observations:                 103   AIC:                             817.2\nDf Residuals:                     100   BIC:                             825.1\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -50.2666     12.387     -4.058      0.000     -74.841     -25.692\nWater          0.5422      0.062      8.781      0.000       0.420       0.665\nSlag          -0.0902      0.021     -4.372      0.000      -0.131      -0.049\n==============================================================================\nOmnibus:                        5.426   Durbin-Watson:                   2.029\nProb(Omnibus):                  0.066   Jarque-Bera (JB):                4.164\nSkew:                          -0.371   Prob(JB):                        0.125\nKurtosis:                       2.353   Cond. No.                     2.14e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.14e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\n\n\n\n\n\n\n\nThe \\(F\\)-test is now concerned with the hypotheses: \\[\\begin{eqnarray*}\nH_0 &:& \\beta_1 = \\beta_2 = 0\\\\\nH_1 &:& \\beta_1 \\ne 0 \\text{ or } \\beta_2 \\ne 0\n\\end{eqnarray*}\\]\nFrom the output above, we can see that \\(F_1 = 49.17\\), with a corresponding \\(p\\)-value of \\(1.3 \\times 10^{-15}\\). The individual \\(t\\)-tests for the coefficients all indicate significant differences from 0. The final estimated model can be written as \\[\nY = -50.27 + 0.54 X_1 - 0.09 X_2\n\\] Notice that the coefficients have changed slightly from the model in Example 9.3. Notice also that we have an improved \\(R^2\\) of 0.50. However, as we pointed out earlier, we should be using the adjusted \\(R^2\\), which adjusts for the additional variable included. This value is 0.49.\nWhile we seem to have found a better model than before, we still have to assess if all the assumptions listed in Section 9.2.1 have been met. We shall do so in subsequent sections.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-regression.html#indicator-variables",
    "href": "09-regression.html#indicator-variables",
    "title": "9  Linear Regression",
    "section": "9.5 Indicator Variables",
    "text": "9.5 Indicator Variables\n\nIncluding a Categorical Variable\nThe explanatory variables in a linear regression model do not need to be continuous. Categorical variables can also be included in the model. In order to include them, they have to be coded using dummy variables.\nFor instance, suppose that we wish to include gender in a model as \\(X_3\\). There are only two possible genders in our dataset: Female and Male. We can represent \\(X_3\\) as an indicator variable, with\n\\[\nX_{3,i} =\n\\begin{cases}\n1 & \\text{individual $i$ is male}\\\\\n0 & \\text{individual $i$ is female}\n\\end{cases}\n\\]\nThe model (without subscripts for the \\(n\\) individuals) is then: \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + e\n\\] For females, the value of \\(X_3\\) is 0. Hence the model reduces to \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + e\n\\] On the other hand, for males, the model reduces to \\[\nY = (\\beta_0 + \\beta_3) + \\beta_1 X_1 + \\beta_2 X_2 + e\n\\] The difference between the two models is in the intercept. The other coefficients remain the same.\nIn general, if the categorical variable has \\(a\\) levels, we will need \\(a-1\\) columns of indicator variables to represent it. This is in contrast to machine learning models which use one-hot encoding. The latter encoding results in columns that are linearly dependent if we include an intercept term in the model.\n\nExample 9.7 (Bike Data Working Day) \nIn this example, we shall improve on the simple linear regression model from Example 9.4. Instead of a single model for just non-working days, we shall fit separate models for working and non-working days by including that variable as a categorical one.\n\nR codePython codeSAS output\n\n\n\n# R \nlm_reg_casual2 &lt;- lm(registered ~ casual + workingday, data=bike2)\nsummary(lm_reg_casual2)\n\n\nCall:\nlm(formula = registered ~ casual + workingday, data = bike2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3381.8  -674.8   -22.5   792.4  2683.6 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.052e+02  1.188e+02   5.095 4.45e-07 ***\ncasual        1.717e+00  6.893e-02  24.905  &lt; 2e-16 ***\nworkingdayyes 2.332e+03  1.017e+02  22.921  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1094 on 728 degrees of freedom\nMultiple R-squared:  0.5099,    Adjusted R-squared:  0.5086 \nF-statistic: 378.7 on 2 and 728 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n# Python\nlm_reg_casual2 = ols('registered ~ casual + workingday', bike2).fit()\nprint(lm_reg_casual2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             registered   R-squared:                       0.510\nModel:                            OLS   Adj. R-squared:                  0.509\nMethod:                 Least Squares   F-statistic:                     378.7\nDate:                Fri, 20 Dec 2024   Prob (F-statistic):          1.81e-113\nTime:                        15:52:54   Log-Likelihood:                -6150.8\nNo. Observations:                 731   AIC:                         1.231e+04\nDf Residuals:                     728   BIC:                         1.232e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nIntercept           605.2254    118.790      5.095      0.000     372.013     838.438\nworkingday[T.yes]  2331.7334    101.730     22.921      0.000    2132.015    2531.452\ncasual                1.7167      0.069     24.905      0.000       1.581       1.852\n==============================================================================\nOmnibus:                        2.787   Durbin-Watson:                   0.595\nProb(Omnibus):                  0.248   Jarque-Bera (JB):                2.479\nSkew:                          -0.059   Prob(JB):                        0.290\nKurtosis:                       2.740   Cond. No.                     4.05e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.05e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\n\n\n\n\n\n\n\nThe estimated model is now \\[\nY = 605 + 1.72 X_1 + 2330 X_2\n\\]\nBut \\(X_2 =1\\) for working days and \\(X_2=0\\) for non-working days. This results in two separate models for the two types of days:\n\\[\nY =\n\\begin{cases}\n605 + 1.72 X_1, & \\text{for non-working days} \\\\\n2935 + 1.72 X_1, & \\text{for working days}\n\\end{cases}\n\\]\nWe can plot the two models on the scatterplot to see how they work better than the original model.\n\n\n\n\n\n\n\n\n\nThe dashed line corresponds to the earlier model, from Example 9.7. With the new model, we have fitted separate intercepts to the two days, but the same slope. The benefit of fitting the model in this way, instead of breaking up the data into two portions and a different model on each one is that we use the entire dataset to estimate the variability. If we wish to fit separate intercepts and slopes, we need to include an interaction term, which is what the next subsection is about.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-regression.html#interaction-term",
    "href": "09-regression.html#interaction-term",
    "title": "9  Linear Regression",
    "section": "9.6 Interaction term",
    "text": "9.6 Interaction term\nA more complex model arises from an interaction between two terms. Here, we shall consider an interaction between a continuous variable and a categorical explanatory variable. Suppose that we have three predictors: height (\\(X_1\\)), weight (\\(X_2\\)) and gender (\\(X_3\\)). As spelt out in Section 9.5.1, we should use indicator variables to represent \\(X_3\\) in the model.\nIf we were to include an interaction between gender and weight, we would be allowing for a males and females to have separate coefficients for \\(X_2\\). Here is what the model would appear as: \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_2 X_3 + e\n\\] Remember that \\(X_3\\) will be 1 for males and 0 for females. The simplified equation for males would be:\n\\[\nY = (\\beta_0 + \\beta_3) + \\beta_1 X_1 + (\\beta_2 + \\beta_4) X_2 + e\n\\] For females, it would be: \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + e\n\\] Both the intercept and coefficient of \\(X_2\\) are different now. Recall that in Section 9.5.1, only the intercept term was different.\n\nExample 9.8 (Bike Data Working Day) \nFinally, we shall include an interaction in the model for bike rentals, resulting in separate intercepts and separate slopes.\n\nR codePython codeSAS output\n\n\n\n# R \nlm_reg_casual3 &lt;- lm(registered ~ casual * workingday, data=bike2)\nsummary(lm_reg_casual3)\n\n\nCall:\nlm(formula = registered ~ casual * workingday, data = bike2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4643.5  -733.0   -57.3   675.9  2532.1 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          1.363e+03  1.199e+02  11.361  &lt; 2e-16 ***\ncasual               1.164e+00  7.383e-02  15.769  &lt; 2e-16 ***\nworkingdayyes        8.063e+02  1.446e+02   5.578 3.44e-08 ***\ncasual:workingdayyes 1.819e+00  1.340e-01  13.575  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 977.6 on 727 degrees of freedom\nMultiple R-squared:  0.609, Adjusted R-squared:  0.6074 \nF-statistic: 377.5 on 3 and 727 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n# Python\nlm_reg_casual3 = ols('registered ~ casual * workingday', bike2).fit()\nprint(lm_reg_casual3.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             registered   R-squared:                       0.609\nModel:                            OLS   Adj. R-squared:                  0.607\nMethod:                 Least Squares   F-statistic:                     377.5\nDate:                Fri, 20 Dec 2024   Prob (F-statistic):          9.38e-148\nTime:                        15:52:55   Log-Likelihood:                -6068.3\nNo. Observations:                 731   AIC:                         1.214e+04\nDf Residuals:                     727   BIC:                         1.216e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n============================================================================================\n                               coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------------\nIntercept                 1362.6312    119.942     11.361      0.000    1127.158    1598.105\nworkingday[T.yes]          806.2675    144.551      5.578      0.000     522.480    1090.055\ncasual                       1.1643      0.074     15.769      0.000       1.019       1.309\ncasual:workingday[T.yes]     1.8186      0.134     13.575      0.000       1.556       2.082\n==============================================================================\nOmnibus:                        7.936   Durbin-Watson:                   0.528\nProb(Omnibus):                  0.019   Jarque-Bera (JB):               11.531\nSkew:                          -0.025   Prob(JB):                      0.00313\nKurtosis:                       3.613   Cond. No.                     5.72e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.72e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that \\(R^2_{adj}\\) has increased from 50.8% to 60.7%. The estimated models for each type of day are:\n\\[\nY =\n\\begin{cases}\n1362 + 1.16 X_1, & \\text{for non-working days} \\\\\n2168 + 2.97 X_1, & \\text{for working days}\n\\end{cases}\n\\]\nHere is visualisation of the lines that have been estimated for each sub-group of day. This is the image that we had earlier on Figure 9.2.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-regression.html#residual-diagnostics",
    "href": "09-regression.html#residual-diagnostics",
    "title": "9  Linear Regression",
    "section": "9.7 Residual Diagnostics",
    "text": "9.7 Residual Diagnostics\nRecall from Equation 9.4 that residuals are computed as \\[\nr_i = Y_i - \\hat{Y_i}\n\\] Residual analysis is a standard approach for identifying how we can improve a model. In the case of linear regression, we can use the residuals to assess if the distributional assumptions hold. We can also use residuals to identify influential points that are masking the general trend of other points. Finally, residuals can provided some direction on how to improve the model.\n\nStandardised Residuals\nIt can be shown that the variance of the residuals is in fact not constant! Let us define the hat-matrix as \\[\n\\textbf{H} = \\textbf{X}(\\textbf{X}'\\textbf{X} )^{-1} \\textbf{X}'\n\\] The diagonal values of \\(\\textbf{H}\\) will be denoted \\(h_{ii}\\), for \\(i = 1, \\ldots, n\\). It can then be shown that \\[\nVar(r_i) = \\sigma^2 (1 - h_{ii}), \\quad Cov(r_i, r_j) = -\\sigma^2 h_{ij}\n\\] As such, we use the standardised residuals when checking if the assumption of Normality has been met.\n\\[\nr_{i,std}  = \\frac{r_i}{\\hat{\\sigma}\\sqrt{1 - h_{ii}}}\n\\] If the model fits well, standardised residuals should look similar to a \\(N(0,1)\\) distribution. In addition, large values of the standardised residual indicate potential outlier points.\nBy the way, \\(h_{ii}\\) is also referred to as the leverage of a point. It is a measure of the potential influence of a point (on the parameters, and future predictions). \\(h_{ii}\\) is a value between 0 and 1. For a model with \\(p\\) parameters, the average \\(h_{ii}\\) should be should be \\(p/n\\). We consider points for whom \\(h_{ii} &gt; 2 \\times p/n\\) to be high leverage points.\nIn the literature and in textbooks, you will see mention of residuals, standardised residuals and studentised residuals. While they differ in definitions slightly, they typically yield the same information. Hence we shall stick to standardised residuals for our course.\n\n\nNormality\n\nExample 9.9 (Concrete Data Normality Check) \nLet us inspect the residuals from the concrete model for Normality.\n\nR codePython code\n\n\nr_s &lt;- rstandard(lm_flow_water_slag)\nhist(r_s)\nqqnorm(r_s)\nqqline(r_s)\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Python\nr_s = pd.Series(lm_flow_water_slag.resid_pearson)\nr_s.hist()\n\n\n\n\n\n\n\n\n\n\n\nWhile it does appear that we have slightly left-skewed data, the departure from Normality seems to arise mostly from a thinner tail on the right.\n\nshapiro.test(r_s)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  r_s\n## W = 0.97223, p-value = 0.02882\nks.test(r_s, \"pnorm\")\n## \n##  Asymptotic one-sample Kolmogorov-Smirnov test\n## \n## data:  r_s\n## D = 0.08211, p-value = 0.491\n## alternative hypothesis: two-sided\n\nAt 5% level, the two Normality tests do not agree on the result either. In any case, please do keep in mind where Normality is needed most: in the hypothesis tests. The estimated model is still valid - it is the best fitting line according to the least-squares criteria.\n\n\n\nScatterplots\nTo understand the model fit better, a set of scatterplots are typically made. These are plots of standardised residuals (on the \\(y\\)-axis) against\n\nfitted values\nexplanatory variables, one at a time.\npotential variables.\n\nResiduals are meant to contain only the information that our model cannot explain. Hence, if the model is good, the residuals should only contain random noise. There should be no apparent pattern to them. If we find such a pattern in one of the above plots, we would have some clue as to how we could improve the model.\nWe typically inspect the plots for the following patterns:\n\n\nA pattern like the one on the extreme left is ideal. Residuals are randomly distributed around zero; there is no pattern or trend in the plot.\nThe second plot is something rarely seen. It would probably appear if we were to plot residuals against a new variable that is not currently in the model. If we observe this plot, we should then include this variable in the model.\nThis plot indicates we should include a quadratic term in the model.\nThe wedge shape (or funnel shape) indicates that we do not have homoscedascity. The solution to this is either a transformation of the response, or weighted least squares. You will cover these in your linear models class.\n\n\nExample 9.10 (Concrete Data Residual Plots) \nThese are residual plots for the concrete data example.\n\nR codeSAS Plots\n\n\n\nopar &lt;- par(mfrow=c(1,3))\nplot(x=fitted(lm_flow_water_slag), r_s, main=\"Fitted\")\nplot(x=concrete$Water, r_s, main=\"X1\")\nplot(x=concrete$Slag, r_s, main=\"X2\")\n\n\n\n\n\n\n\npar(opar)\n\n\n\n\n\n\n\n\n\n\n\nWhile the plots of residuals versus explanatory variables look satisfactory, the plot of the residual versus fitted values appears to have funnel shape. Coupled with the observations about the deviations from Normality of the residuals in Example 9.6 (thin right-tail), we might want to try a square transform of the response.\n\n\n\nInfluential Points\nThe influence of a point on the inference can be judged by how much the inference changes with and without the point. For instance to assess if point \\(i\\) is influential on coefficient \\(j\\):\n\nEstimate the model coefficients with all the data points.\nLeave out the observations \\((Y_i , X_i)\\) one at a time and re-estimate the model coefficients.\nCompare the \\(\\beta\\)’s from step 2 with the original estimate from step 1.\n\nWhile the above method assesses influence on parameter estimates, Cook’s distance performs a similar iteration to assess the influence on the fitted values. Cook’s distance values greater than 1 indicate possibly influential points.\n\nExample 9.11 (Concrete Data Influential Points) \nTo inspect influential points for the concrete data, we can use this code.\n\nR codeSAS Output\n\n\n\ninfl &lt;- influence.measures(lm_flow_water_slag)\nsummary(infl)\n\nPotentially influential observations of\n     lm(formula = FLOW.cm. ~ Water + Slag, data = concrete) :\n\n   dfb.1_ dfb.Watr dfb.Slag dffit cov.r   cook.d hat  \n60  0.03  -0.03    -0.01    -0.03  1.09_*  0.00   0.06\n69  0.19  -0.19    -0.22    -0.40  0.85_*  0.05   0.02\n83  0.02  -0.02     0.01    -0.03  1.09_*  0.00   0.06\n88 -0.02   0.02    -0.01     0.03  1.09_*  0.00   0.06\n93  0.00   0.00     0.00     0.00  1.10_*  0.00   0.06\n98  0.01  -0.01     0.01    -0.02  1.10_*  0.00   0.06\n\n\nThe set of 6 points above appear to be influencing the covariance matrix of the parameter estimates greatly. To proceed, we would typically leave these observations out one-at-a-time to study the impact on our eventual decision.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-regression.html#further-reading",
    "href": "09-regression.html#further-reading",
    "title": "9  Linear Regression",
    "section": "9.8 Further Reading",
    "text": "9.8 Further Reading\nThe topic of linear regression is vast. It is an extremely well-established technique with numerous variations for a multitude of scenarios. Even a single course on it (ST3131) will not have sufficient time to cover all of it’s capabilities. Among topics that will be useful in your career are :\n\nGeneralised additive models, which allow the use of piecewise polynomials for flexible modeling of non-linear functions.\nGeneralised linear models, for modeling non-numeric response.\nGeneralised least squares, to handle correlated observations, and so on.\nPrincipal component regression, to handle the issue of multicollinearity (correlated predictors).\n\nThe textbooks Draper and Smith (1998) and James et al. (2013) are excellent reference books for this topic.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-regression.html#references",
    "href": "09-regression.html#references",
    "title": "9  Linear Regression",
    "section": "9.9 References",
    "text": "9.9 References\n\nWebsite References\n\nStats models documentation\nDiagnostics\nOn residual plots\n\n\n\n\n\nDraper, Norman R, and Harry Smith. 1998. Applied Regression Analysis. Vol. 326. John Wiley & Sons.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-regression.html#footnotes",
    "href": "09-regression.html#footnotes",
    "title": "9  Linear Regression",
    "section": "",
    "text": "This interpretation has to be taken very cautiously, especially when there are other explanatory variables in the model.↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "10-simulation.html",
    "href": "10-simulation.html",
    "title": "10  Simulation",
    "section": "",
    "text": "10.1 Introduction\nThe objective of any simulation study is to estimate an expectation \\(E(X)\\). Simulation studies involve the use of a computer to generate independent copies of the random variable of interest \\(X\\). Here are a couple of examples where simulation studies would be applicable.\nHere is a slightly more sophisticated example.\nThe two examples above are known as Discrete Event Simulations. In our course, we will not get to working with such scenarios. However, we will try to understand and familiarise ourselves with the basic building blocks of simulation studies. For more knowledge, do enrol yourself in ST3247!\nThe basic steps in a simulation study are:\nThis is sometimes referred to as Monte Carlo Simulation. Before proceeding, let us refresh our knowledge of the properties of the sample mean.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "10-simulation.html#sec-simulation",
    "href": "10-simulation.html#sec-simulation",
    "title": "10  Simulation",
    "section": "",
    "text": "Example 10.1 (Insurance Claims) Before the financial year begins, an insurance company has to decide how much cash to keep, in order to pay out the claims for that year. Suppose that claims are independent of each other and are distributed as \\(Exp(1/200)\\)1 dollars. Also suppose that the number of claims in a year is a Poisson random variable with mean 8.2.\nAn actuary has been asked to determine the size of the reserve fund that should be set up, and he recommends $12,000. We might consider answering the following question using simulation:\n\nWhat is the probability that the total claims will exceed the reserve fund?\n\nIf we let \\(Y\\) be the random variable representing the total sum of claims, we are interested in estimating \\(P(Y &gt; 12000)\\). Since probabilities are expectations, we can use simulation to estimate this value.\n\n\n\n\nExample 10.2 (Sandwich Shop Closing Time) Suppose that you run a sandwich shop, which is open from 9am till 5pm. Your philosophy has always been to serve every customer who has entered before 5pm, even if that requires you to stay back until they have been served. You would like to estimate the mean amount of overtime you have to work.\nIf you are willing to assume that the inter-arrival times of customers is \\(Exp(3)\\) hours, then it is possible to simulate this process to estimate the mean time that you would have to remain open, beyond 5pm.\n\n\n\n\nIdentify the random variable of interest and write a program to simulate it.\nGenerate an iid sample \\(X_1, X_2, \\ldots, X_n\\) using this program.\nEstimate \\(E(X)\\) using \\(\\bar{X}\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "10-simulation.html#theory",
    "href": "10-simulation.html#theory",
    "title": "10  Simulation",
    "section": "10.2 Theory",
    "text": "10.2 Theory\nThere are two important theorems that simulation studies rely on. The first is the Strong Law of Large Numbers (SLLN).\n\nTheorem 10.1 (Strong Law of Large Numbers) If \\(X_1, X_2, \\ldots, X_n\\) are independent and identically distributed with \\(E(X) &lt; \\infty\\), then \\[\n\\bar{X} =\\frac{1}{n} \\sum_{i=1}^n X_i\\rightarrow E(X) \\quad \\text{with probability 1.}\n\\]\n\nIn the simulation context, it means that as we generate more and more samples (i.e. increase \\(n\\)), our sample mean \\(\\bar{X}\\) converges to the desired value \\(E(X)\\), no matter what the distribution of \\(X\\) is.\nThe second theorem that aids us is the Central Limit Theorem (CLT).\n\nTheorem 10.2 (Central Limit Theorem) Let \\(X_1, X_2, \\ldots, X_n\\) be i.i.d., and suppose that\n\n\\(-\\infty &lt; E(X_1) = \\mu &lt; \\infty\\).\n\\(Var(X_1) = \\sigma^2 &lt; \\infty\\).\n\nThen \\[\n\\frac{\\sqrt{n} (\\bar{X} - \\mu)}{\\sigma} \\Rightarrow N(0,1)\n\\] where \\(\\Rightarrow\\) denotes convergence in distribution.\n\nThis is sometimes informally interpreted to mean that when \\(n\\) is large, \\(\\bar{X}\\) is approximately Normal with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\). In the simulation context, we can use this theorem to obtain a confidence interval for the expectation that we are estimating.\nAlso take note of the following properties of the sample mean and variance:\n\nTheorem 10.3 (Sample Estimates) It can be shown that both the sample mean and sample standard deviation are unbiased estimators. \\[\nE(\\bar{X}) = E(X), \\quad E(s^2) = \\sigma^2\n\\] where \\(s^2 = \\frac{\\sum (X_i - \\bar{X})^2}{n-1}\\).\n\nTo obtain a \\((1-\\alpha)100%\\) confidence interval for \\(\\mu\\), we use the following formula, from the CLT:\n\\[\n\\bar{X} \\pm z_{1-\\alpha/2} \\frac{s}{\\sqrt{n}}\n\\]\nWhen our goal is to estimate a probability \\(p\\), we have to introduce a corresponding indicator variable \\(X\\) such that\n\\[\nX =\n\\begin{cases}\n1 & \\text{with probability $p$} \\\\\n0 & \\text{with probability $1- p$}\n\\end{cases}\n\\]\nIn this case, the formula for the CI becomes \\[\n\\bar{X} \\pm z_{1-\\alpha/2} \\sqrt{\\frac{\\bar{X}(1-\\bar{X})}{n}}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "10-simulation.html#generating-random-variables-in-r-and-python",
    "href": "10-simulation.html#generating-random-variables-in-r-and-python",
    "title": "10  Simulation",
    "section": "10.3 Generating Random Variables in R and Python",
    "text": "10.3 Generating Random Variables in R and Python\nBoth R and Python contain built-in routines for generating random variables from common “named” distributions, e.g. Normal, Uniform, Gamma, etc. All software that can generate random variables utilise Pseudo-Random Number Generators (PRNG). These are routines that generate sequences of deterministic numbers with very very long cycles. However, since they pass several tests of randomness, they can be treated as truly random variables for all intents and purposes.\nIn both software, we can set the “seed”. This initialises the random number generator. When we reset the seed, we can reproduce the stream of random variables exactly. This feature exists:\n\nFor debugging purposes,\nTo allow us to study the sensitivity of our results to the seed.\n\n\nExample 10.3 (Random Variable Generation)  \n\nR codePython code\n\n\nIn R, all the functions for generating random variables begin with r (for random). Here are a few such functions:\n\n\n\nFunction name\nRandom Variable\n\n\n\n\nrnorm\nNormal\n\n\nrunif\nUniform\n\n\nrgamma\nGamma\n\n\nrpois\nPoisson\n\n\nrbinom\nBinomial\n\n\n\n\n#R \nset.seed(2137)\n\nopar &lt;- par(mfrow=c(1,3))\nY &lt;- rgamma(50, 2, 3)\nhist(Y)\n\nW &lt;- rpois(50, 1.3) # 50 obs from Pois(1.3)\nbarplot(table(W))\n\nZ &lt;- rbinom(50, size=2, 0.3) # 50 obs from Binom(2, 0.3)\nbarplot(table(Z))\n\n\n\n\n\n\n\npar(opar)\n\n\n\nIn Python, we can generate random variables using numpy and/or scipy. In our course, we shall use the scipy routines because its implementation is closer in spirit to R.\n\n\n\nFunction name\nRandom Variable\n\n\n\n\nnorm\nNormal\n\n\nuniform\nUniform\n\n\ngamma\nGamma\n\n\npoisson\nPoisson\n\n\nbinom\nBinomial\n\n\n\n\n#Python\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import binom, gamma, norm, poisson\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(2137)\nfig, ax = plt.subplots(1, 3, figsize=(8,4))\n\nax1 = plt.subplot(131)\nr = gamma.rvs(2, 3, size=50, random_state=rng)\nax1.hist(r);\n\nax1 = plt.subplot(132)\nr = poisson.rvs(1.3, size=50, random_state=rng)\nax1.hist(r);\n\nax1 = plt.subplot(133)\nr = binom.rvs(2, 0.3, size=50, random_state=rng)\nax1.hist(r);",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "10-simulation.html#monte-carlo-integration",
    "href": "10-simulation.html#monte-carlo-integration",
    "title": "10  Simulation",
    "section": "10.4 Monte-Carlo Integration",
    "text": "10.4 Monte-Carlo Integration\nSuppose we wish to evaluate\n\\[\n\\int_{-\\infty}^{\\infty} h(x) f(x) \\text{d}x\n\\] where \\(f(x)\\) is a pdf. The integral above is in fact equal to \\(E(h(X))\\), where \\(X \\sim f\\). Hence we can use everything we have discussed so far, to evaluate the expression using simulation! Everything depends on:\n\nBeing able to introduce a pdf to the integral\nBeing to able to simulate from that pdf.\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is critical that the support of the pdf is the same as the range of integration.\n\n\n\nExample 10.4 (Monte Carlo Integration over (0,1)) Suppose Suppose we wish to evaluate \\[\n\\theta = \\int_0^1 e^{2x} dx = \\int_0^1 e^{2x} \\cdot 1\\; dx\n\\]\nWe can identify that this is equal to \\(E(h(X))\\) where\n\n\\(X \\sim Unif(0,1)\\).\n\n\\(h(X) = e^{2x}\\)\n\nThus we can follow this pseudo-code:\n\nGenerate \\(X_1,X_2,\\ldots,X_n \\sim Unif(0,1)\\).\nEstimate the integral using \\[\n\\frac{1}{n} \\sum_{i=1}^n e^{2 X_i}\n\\]\n\nIn this simple case, we can in fact work out analytically that the integral is equal to \\[\n\\frac{1}{n}(e^2 - 1) = 3.195\n\\]\n\nR codePython code\n\n\n\nset.seed(2138)\nX &lt;- runif(50000, 0, 1)\nhX &lt;- exp(2*X)\n(mc_est &lt;- mean(hX))\n\n[1] 3.185626\n\n\n\n\n\nfrom scipy.stats import uniform\n\nX = uniform.rvs(0,1, size=50000, random_state=rng)\nhX = np.exp(2*X)\nhX.mean()\n\nnp.float64(3.2090955591090915)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "10-simulation.html#simulation-studies",
    "href": "10-simulation.html#simulation-studies",
    "title": "10  Simulation",
    "section": "10.5 Simulation Studies",
    "text": "10.5 Simulation Studies\nIn this section, we shall touch on how we can use simulation in some scenarios that are closer to the real world.\n\nConfidence Intervals\nThe usual 95% confidence interval for a mean is given by \\[\n\\bar{X} \\pm t_{0.025} s/\\sqrt{n}\n\\] where \\(t_{0.025}\\) is the 0.025 quantile of the t-distribution with \\(n-1\\) degrees. The resulting interval should contain the true mean in 95% of the experiments. However, it is derived under the assumption that the data is Normally distributed. Let us see if it still works if the data is from an asymmetric distribution: \\(Pois(0.5)\\).\n\nExample 10.5 (Coverage of Confidence Interval)  \n\nR codePython code\n\n\n\n# R \nset.seed(2139)\noutput_vec &lt;- rep(0, length=100)\nn &lt;- 20\nlambda &lt;- 0.5\nfor(i in 1:length(output_vec)) {\n  X &lt;- rpois(15, .5)\n  Xbar &lt;- mean(X)\n  s &lt;- sd(X)\n  t &lt;- qt(0.975, n-1)\n  CI &lt;- c(Xbar - t*s/sqrt(n), Xbar + t*s/sqrt(n))\n  if(CI[1] &lt; lambda & CI[2] &gt; lambda) {\n    output_vec[i] &lt;- 1\n  }\n}\nmean(output_vec)\n\n[1] 0.84\n\n\n\n\n\nrng = np.random.default_rng(2137)\noutput_vec = np.zeros(100)\nn = 20\nlambda_ = 0.5\nfor i in range(100):\n    X = poisson.rvs(0.5, size=15, random_state=rng)\n    Xbar = X.mean()\n    s = X.std()\n    t = norm.ppf(0.975)\n    CI = [Xbar - t*s/np.sqrt(n), Xbar + t*s/np.sqrt(n)]\n    if CI[0] &lt; lambda_ and CI[1] &gt; lambda_:\n        output_vec[i] = 1\noutput_vec.mean()\n\nnp.float64(0.86)\n\n\n\n\n\n\n\n\nType I Error\nConsider the independent two-sample \\(t\\)-test that we introduced in topic 7. The formal set-up also includes the assumption that our data arises from a Normal distribution. According to the theory of the \\(t\\)-test, if both groups have the same mean, we should falsely reject the null hypothesis 10% of the time if we perform it at 10% significance level. Let us assess if this is what actually happens.\n\nExample 10.6 (T-test Type I Error)  \n\nR codePython code\n\n\n\ngenerate_one_test &lt;- function(n=100) {\n  X &lt;- rnorm(n)\n  Y &lt;- rnorm(n)\n  t_test &lt;- t.test(X, Y,var.equal = TRUE)\n  # extract the p-value from the t_test\n  if(t_test$p.value &lt; 0.10) \n    return(1L) \n  else \n    return(0L)\n}\n\nset.seed(11)\noutput_vec &lt;- vapply(1:2000, \n                     function(x) generate_one_test(), \n                     1L)\nmean(output_vec)\n\n[1] 0.108\n\n\n\n\n\ndef generate_one_test(n=100):\n    X = norm.rvs(0, 1, size=n, random_state=rng)\n    Y = norm.rvs(0, 1, size=n, random_state=rng)\n    t_test = stats.ttest_ind(X, Y, equal_var=True)\n    if t_test.pvalue &lt; 0.10:\n        return 1\n    else:\n        return 0\noutput_vec = np.array([generate_one_test() for j in range(2000)])\noutput_vec.mean()\n\nnp.float64(0.102)\n\n\n\n\n\n\n\n\nNewspaper Inventory\n\nExample 10.7 Suppose that daily demand for newspaper is approximately gamma distributed, with mean 10,000 and variance 1,000,000. The newspaper prints and distributes 11,000 copies each day. The profit on each newspaper sold is $1, and the loss on each unsold newspaper is 0.25. Formally, the daily profit function h is\n\\[\nh(X) =\n\\begin{cases}\n11000 & \\text{if } X ≥ 11000 \\\\\n\\lfloor X \\rfloor + (11000 - \\lfloor X \\rfloor)(−0.25) & \\text{if } X &lt; 11000\n\\end{cases}\n\\]\nwhere \\(X\\) represents the daily demand. Let us estimate the expected daily profit using simulation.\n\nR codePython code\n\n\n\n# R code to estimate the expected daily profit\nset.seed(2141)\nn &lt;- 10000\nX &lt;- rgamma(n, 100, rate=1/100)\nhX &lt;- ifelse(X &gt;= 11000, 11000, floor(X) + (11000 - floor(X)) * (-0.25))\n#mean(hX)\n\n# 90% CI for the mean\ns &lt;- sd(hX)\nq1 &lt;- qnorm(0.95)\nCI &lt;- c(mean(hX) - q1*s/sqrt(n), mean(hX) + q1*s/sqrt(n))\ncat(\"The 90% CI for the mean is (\", format(CI[1], digits=2, nsmall=2), \", \", \n    format(CI[2], digits=2, nsmall=2), \").\\n\", sep=\"\")\n\nThe 90% CI for the mean is (9639.10, 9673.69).\n\n\n\n\n\nn = 10000\nX = gamma.rvs(100, scale=100, size=n, random_state=rng)\nhX = np.where(X &gt;= 11000, 11000, np.floor(X) + (11000 - np.floor(X)) * (-0.25))\n\n# 90% CI for the mean\nXbar = hX.mean()\ns = hX.std()\nt = norm.ppf(0.95)\nCI = [Xbar - t*s/np.sqrt(n), Xbar + t*s/np.sqrt(n)]\nprint(f\"The 90% CI for the mean is ({CI[0]: .3f}, {CI[1]: .3f}).\")\n\nThe 90% CI for the mean is ( 9623.529,  9658.387).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "10-simulation.html#resampling-methods",
    "href": "10-simulation.html#resampling-methods",
    "title": "10  Simulation",
    "section": "10.6 Resampling Methods",
    "text": "10.6 Resampling Methods\nThe next section introduces two techniques that are based on resampling the data.\n\nPermutation Test\nConsider the two-sample t-test that we introduced in topic 06. This parametric test requires us to check if the data from the two groups came from Normal distributions. The non-parametric version requires each group to have at least 10 observations. What if our data satisfies neither criteria?\nThe permutation test is applicable in such a situation. It makes no distributional assumptions whatsoever on the data. Here is pseudo-code for how it works:\n\nCompute the difference in group means. This observed difference is the test statistic.\nTreating the observed values as fixed, combine the two vectors of observations.\nPermute the observations, and then re-assign them to the two groups.\nCompute the difference between the group means.\nRepeat steps 2 - 4 multiple times (order of 1000).\n\nThe \\(p\\)-value for the null hypothesis can be computed by computing the proportion of simulated statistics that were larger in absolute value than the observed one.\n\nExample 10.8 (Abalone Data) \nIn the abalone dataset, this was the output from the t-test:\n\nabl &lt;- read.csv(\"data/abalone_sub.csv\")\nx &lt;- abl$viscera[abl$gender == \"M\"]\ny &lt;- abl$viscera[abl$gender == \"F\"]\n\nt.test(x, y, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  x and y\nt = 0.91008, df = 98, p-value = 0.365\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.02336287  0.06294287\nsample estimates:\nmean of x mean of y \n  0.30220   0.28241 \n\n\nThis would be the procedure for a permutation test:\n\nd1 &lt;- mean(x)  - mean(y)\nprint(d1)\n\n[1] 0.01979\n\ngenerate_one_perm &lt;- function(x, y) {\n  n1 &lt;- length(x)\n  n2 &lt;- length(y)\n  xy &lt;- c(x,y)\n  xy_sample &lt;- sample(xy)\n  d1 &lt;- mean(xy_sample[1:n1]) - mean(xy_sample[-(1:n1)])\n  d1\n}\nsampled_diff &lt;- replicate(2000, generate_one_perm(x,y))\nhist(sampled_diff)\n\n\n\n\n\n\n\n(p_val &lt;- 2*mean(sampled_diff &gt; d1))\n\n[1] 0.369\n\n\n\n\n\nBootstrapping\nThe video on Canvas provides a very brief introduction to bootstrapping. One of the greatest benefits of the bootstrap is the ability to provide confidence intervals for estimators for which we may not have the know-how to derive analytic or asymptotic results.\nConsider obtaining a confidence interval for the trimmed mean, that we encountered in Section 5.1.\n\nExample 10.9 \nThis is how we can use bootstrapping to obtain a confidence interval for a trimmed mean.\n\nlibrary(MASS)\n\nmean(chem)\n\n[1] 4.280417\n\nt.test(chem)\n\n\n    One Sample t-test\n\ndata:  chem\nt = 3.9585, df = 23, p-value = 0.0006236\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 2.043523 6.517311\nsample estimates:\nmean of x \n 4.280417 \n\n## [1] 4.280417\n\nNotice how the CI from the non-robust technique is very wide.\n\nlibrary(boot)\n\nstat_fn &lt;- function(d, i) {\n  b &lt;- mean(d[i], trim=0.1)\n  b\n}\nboot_out &lt;- boot(chem, stat_fn, R = 1999, stype=\"i\")\n# Returns two types of bootstrap intervals:\nboot.ci(boot.out = boot_out, type=c(\"perc\", \"bca\"))\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1999 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_out, type = c(\"perc\", \"bca\"))\n\nIntervals : \nLevel     Percentile            BCa          \n95%   ( 2.955,  4.700 )   ( 2.970,  4.773 )  \nCalculations and Intervals on Original Scale\n\n\nThe boot function requires a function (stat_fn) that computes the statistic from the bootstrapped sample. Note that the intervals returned from the trimmed mean are much narrower.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "10-simulation.html#summary",
    "href": "10-simulation.html#summary",
    "title": "10  Simulation",
    "section": "10.7 Summary",
    "text": "10.7 Summary\nIn this chapter, we have seen how simulation studies can be used to estimate expectations. Although we have restricted ourselves to very straightforward examples, the same principles can be applied to more complex scenarios.\nIn particular, there are three types of simulation models widely used to model complex systems:\n\nAgent-based models. These are regularly used to model the interactions of agents in a system, e.g. humans.\nDiscrete Event Simulations. These are used to model systems where events occur at discrete points in time. Typically, these are systems where there is an arrival of entities, a service, and a departure.\nProcess modeling. This approach is used to model the flow of entities through a system. They are sometimes also referred to as compartment models.\n\nPlease refer to the sections below for more information. For our course, please be familiar with the basic building blocks of simulation studies:\n\nGenerate iid samples, and then\nCompute the sample mean, and the CI for the true mean.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "10-simulation.html#references",
    "href": "10-simulation.html#references",
    "title": "10  Simulation",
    "section": "10.8 References",
    "text": "10.8 References\n\nWebsite References\n\nSome GUI-based software for simulation:\n\nAnylogic A very very powerful software for agent-based modeling.\nArena A discrete event simulator, with a free academic license.\nNetlogo An open source software for agent-based modeling.\n\nPython software:\n\nsimpy for discrete event simulations.\nmesa for agent-based modeling.\n\nR software:\n\nsimmer for discrete event simulation\n\nscipy documentation\nIntroduction to Bootstrapping",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "10-simulation.html#footnotes",
    "href": "10-simulation.html#footnotes",
    "title": "10  Simulation",
    "section": "",
    "text": "\\(f_X(x) = \\frac{1}{200} \\exp(-x/200),\\; x &gt; 0\\)↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Academic References",
    "section": "",
    "text": "Agresti, Alan. 2012. Categorical Data Analysis. Vol. 792. John\nWiley & Sons.\n\n\nCortez, Paulo, and Alice Maria Gonçalves Silva. 2008. “Using Data\nMining to Predict Secondary School Student Performance.”\n\n\nDraper, Norman R, and Harry Smith. 1998. Applied Regression\nAnalysis. Vol. 326. John Wiley & Sons.\n\n\nEkstrøm, Claus Thorn, and Helle Sørensen. 2015. “Statistical Data\nAnalysis for the Life Sciences.” CRC Press, London.\n\n\nFanaee-T, Hadi, and Joao Gama. 2013. “Event Labeling Combining\nEnsemble Detectors and Background Knowledge.” Progress in\nArtificial Intelligence, 1–15. https://doi.org/10.1007/s13748-013-0040-3.\n\n\nHuber, Peter J. 1992. “Robust Estimation of a Location\nParameter.” In Breakthroughs in Statistics: Methodology and\nDistribution, 492–518. Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al.\n2013. An Introduction to Statistical Learning. Vol. 112.\nSpringer.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nRosner, Bernard. 2015. Fundamentals of Biostatistics. Cengage\nlearning.\n\n\nVenables, William N, and Brian D Ripley. 2013. Modern Applied\nStatistics with s-PLUS. Springer Science & Business Media.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. O’Reilly Media, Inc.\n\n\nWilcox, R, and R. 2011. Introduction to Robust Estimation and\nHypothesis Testing. Academic press.\n\n\nYeh, I-Cheng. 2007. “Modeling Slump Flow of Concrete Using\nSecond-Order Regressions and Artificial Neural Networks.”\nCement and Concrete Composites 29 (6): 474–80.",
    "crumbs": [
      "Academic References"
    ]
  }
]